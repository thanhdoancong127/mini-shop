[
	{
		"_class": "assessment",
		"id": 18772968,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Upload the compressed file using multipart upload with S3 transfer acceleration\"</p>\n\n<p>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p>\n\n<p>Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and in any order. If transmission of any part fails, you can retransmit that part without affecting other parts. After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance. If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.</p>\n\n<p>Incorrect options:\n\"Upload the compressed file in a single operation\" - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation. Multipart upload provides improved throughput - you can upload parts in parallel to improve throughput. Therefore, this option is not correct.</p>\n\n<p>\"Upload the compressed file using multipart upload\" - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would further improve the transfer speed. Therefore just using multipart upload is not the correct option.</p>\n\n<p>\"FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from EC2 instance into the S3 bucket\" -  This is a roundabout process of getting the file into S3 and added as a distractor. Although it is technically feasible to follow this process, it would involve a lot of scripting and certainly would not be the fastest way to get the file into S3.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</p>\n",
			"question": "<p>One of the leading global measurement and data analytics companies measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB. Which of the following is the fastest way to upload the daily compressed file into S3?</p>\n",
			"answers": [
				"<p>Upload the compressed file in a single operation</p>",
				"<p>Upload the compressed file using multipart upload</p>",
				"<p>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from EC2 instance into the S3 bucket</p>",
				"<p>Upload the compressed file using multipart upload with S3 transfer acceleration</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "One of the leading global measurement and data analytics companies measures what the consumers watch and what advertising they’re exposed to. This real-time data is ingested into its on-premises data center and subsequently the daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB. Which of the following is the fastest way to upload the daily compressed file into S3?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773022,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Delete the existing standard queue and recreate it as a FIFO queue\"</p>\n\n<p>\"Make sure that the name of the FIFO queue ends with the .fifo suffix\"</p>\n\n<p>\"Make sure that the throughput for the target FIFO queue does not exceed 3,000 messages per second\"</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p>SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>By default, FIFO queues support up to 3,000 messages per second with batching, or up to 300 messages per second (300 send, receive, or delete operations per second) without batching. Therefore, using batching you can meet a throughput requirement of upto 3,000 messages per second.</p>\n\n<p>The name of a FIFO queue must end with the .fifo suffix. The suffix counts towards the 80-character queue name limit. To determine whether a queue is FIFO, you can check whether the queue name ends with the suffix.</p>\n\n<p>If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue.</p>\n\n<p>Incorrect options:\n\"Convert the existing standard queue into a FIFO queue\" - You can't convert an existing standard queue into a FIFO queue.</p>\n\n<p>\"Make sure that the name of the FIFO queue is the same as the standard queue\" - The name of a FIFO queue must end with the .fifo suffix.</p>\n\n<p>\"Make sure that the throughput for the target FIFO queue does not exceed 300 messages per second\" - By default, FIFO queues support up to 3,000 messages per second with batching.</p>\n\n<p>References:\nhttps://aws.amazon.com/sqs/faqs/\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html</p>\n",
			"question": "<p>A leading hotel reviews business uses Amazon Web Services to store and process images for their website. The team uses Amazon EC2 and Amazon SQS in an integrated workflow to generate the sizes they need for each photo. SQS communicates the photos that need to be processed and the status of the jobs. Because of changes in the business processes, the engineering team is now migrating from SQS Standard queues to FIFO queues with batching. As a solutions architect, which of the following steps would you add to the migration checklist for the engineering team? (Select three)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Delete the existing standard queue and recreate it as a FIFO queue</p>",
				"<p>Convert the existing standard queue into a FIFO queue</p>",
				"<p>Make sure that the name of the FIFO queue ends with the .fifo suffix</p>",
				"<p>Make sure that the name of the FIFO queue is the same as the standard queue</p>",
				"<p>Make sure that the throughput for the target FIFO queue does not exceed 3,000 messages per second</p>",
				"<p>Make sure that the throughput for the target FIFO queue does not exceed 300 messages per second</p>"
			]
		},
		"correct_response": [
			"a",
			"c",
			"e"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A leading hotel reviews business uses Amazon Web Services to store and process images for their website. The team uses Amazon EC2 and Amazon SQS in an integrated workflow to generate the sizes they need for each photo. SQS communicates the photos that need to be processed and the status of the jobs. Because of changes in the business processes, the engineering team is now migrating from SQS Standard queues to FIFO queues with batching. As a solutions architect, which of the following steps would you add to the migration checklist for the engineering team? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773008,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services\"</p>\n\n<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect.\nAWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.\nDataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single DataSync agent is capable of saturating a 10 Gbps network link.\nDataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the DataSync API and Console, and CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. DataSync performs data integrity verification both during the transfer and at the end of the transfer.</p>\n\n<p>How DataSync Works\n<img src=\"https://d1.awsstatic.com/cloud-storage/Storage/aws-datasync-how-it-works-diagram-s3-efs-fsx.c26c66393dc4e433369ee9947f39e9c54cd338bb.png\">\nvia - https://aws.amazon.com/datasync/</p>\n\n<p>Incorrect options:\n\"Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services\" - Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.\nAWS Snowball Edge is suitable for offline data transfers, for customers who are bandwidth constrained or transferring data from remote, disconnected, or austere environments. Therefore, it cannot support automated and accelerated online data transfers.</p>\n\n<p>\"Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services\" - The AWS Transfer Family provides fully managed support for file transfers directly into and out of Amazon S3. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (such as EFS and Amazon FSx for Windows File Server).</p>\n\n<p>\"Use File Gateway to automate and accelerate online data transfers to the given AWS storage services\" - AWS Storage Gateway's file interface, or file gateway, offers you a seamless way to connect to the cloud to store application data files and backup images as durable objects on Amazon S3 cloud storage. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching. It can be used for on-premises applications, and for Amazon EC2-based applications that need file protocol access to S3 object storage. Therefore, it cannot support migration into the other AWS storage services mentioned in the given use-case (such as EFS and Amazon FSx for Windows File Server).</p>\n\n<p>References:\nhttps://aws.amazon.com/datasync/faqs/\nhttps://aws.amazon.com/storagegateway/file/\nhttps://aws.amazon.com/aws-transfer-family/</p>\n",
			"question": "<p>A global pharmaceutical company is currently engaged in the discovery and development of potential COVID-19 vaccines. The company's labs generate hundreds of terabytes of research data daily. To further accelerate the innovation process, the engineering team at the company wants to move most of the on-premises data into Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. The team would like to automate and accelerate online data transfers to these AWS storage services. As a solutions architect, which of the following solutions would you recommend as the BEST fit?</p>\n",
			"answers": [
				"<p>Use AWS DataSync to automate and accelerate online data transfers to the given AWS storage services</p>",
				"<p>Use AWS Snowball Edge Storage Optimized device to automate and accelerate online data transfers to the given AWS storage services</p>",
				"<p>Use AWS Transfer Family to automate and accelerate online data transfers to the given AWS storage services</p>",
				"<p>Use File Gateway to automate and accelerate online data transfers to the given AWS storage services</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A global pharmaceutical company is currently engaged in the discovery and development of potential COVID-19 vaccines. The company's labs generate hundreds of terabytes of research data daily. To further accelerate the innovation process, the engineering team at the company wants to move most of the on-premises data into Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server easily, quickly, and cost-effectively. The team would like to automate and accelerate online data transfers to these AWS storage services. As a solutions architect, which of the following solutions would you recommend as the BEST fit?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772950,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"AWS Schema Conversion Tool\"\n\"AWS Database Migration Service\"</p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora.\nGiven the use-case where the CTO at the company wants to move away from license-based expensive, legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud, this is an example of heterogeneous database migrations.\nFor such a scenario, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two-step process. First use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database. All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located on your on-premises environment outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p>\n\n<p>Heterogeneous Database Migrations\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram_AWS-DMS_heterogeneous-database-migrations-2.3616bac30ab86d4310ddadfdec5d6e6ba4d8b81d.png\">\nvia - https://aws.amazon.com/dms/</p>\n\n<p>Incorrect options:\n\"AWS Snowball Edge\" - Snowball Edge Storage Optimized is the optimal choice if you need to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. It provides up to 80 TB of usable HDD storage, 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity to address large scale data transfer and pre-processing use cases. As each Snowball Edge Storage Optimized device can handle 80TB of data, you can order 10 such devices to take care of the data transfer for all applications. The original Snowball devices were transitioned out of service and Snowball Edge Storage Optimized are now the primary devices used for data transfer. You may see the Snowball device on the exam, just remember that the original Snowball device had 80TB of storage space.\nAWS Snowball Edge is suitable for offline data transfers, for customers who are bandwidth constrained or transferring data from remote, disconnected, or austere environments. Therefore, it cannot be used for database migrations.</p>\n\n<p>\"AWS Glue\" - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. Therefore, it cannot be used for database migrations.</p>\n\n<p>\"Basic Schema Copy\" - To quickly migrate a database schema to your target instance you can rely on the Basic Schema Copy feature of AWS Database Migration Service. Basic Schema Copy will automatically create tables and primary keys in the target instance if the target does not already contain tables with the same names. Basic Schema Copy is great for doing a test migration, or when you are migrating databases heterogeneously e.g. Oracle to MySQL or SQL Server to Oracle. Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. When you need to use a more customizable schema migration process (e.g. when you are migrating your production database and need to move your stored procedures and secondary database objects), you must use the AWS Schema Conversion Tool.</p>\n\n<p>References:\nhttps://aws.amazon.com/dms/\nhttps://aws.amazon.com/dms/faqs/\nhttps://aws.amazon.com/dms/schema-conversion-tool/</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>AWS Snowball Edge</p>",
				"<p>AWS Schema Conversion Tool</p>",
				"<p>AWS Database Migration Service</p>",
				"<p>AWS Glue</p>",
				"<p>Basic Schema Copy</p>"
			],
			"question": "<p>A global leader in communications and technology solutions is undergoing a major transformation in their database management approach. The CTO at the company wants to move away from license-based, expensive, and legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud. At the same time, the CTO is concerned about identifying areas for data-conversion that require special attention during migration since the on-premises databases have multiple active complex configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services would you recommend being used for this solution (Select two)?</p>\n"
		},
		"correct_response": [
			"b",
			"c"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A global leader in communications and technology solutions is undergoing a major transformation in their database management approach. The CTO at the company wants to move away from license-based, expensive, and legacy commercial database solutions deployed at the on-premises data center to more efficient, open-source, and cost-effective options on AWS Cloud. At the same time, the CTO is concerned about identifying areas for data-conversion that require special attention during migration since the on-premises databases have multiple active complex configurations such as secondary indexes, foreign keys, and stored procedures. As a solutions architect, which of the following AWS services would you recommend being used for this solution (Select two)?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773010,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift\"</p>\n\n<p>AWS Database Migration Service helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p>\n\n<p>Continuous Data Replication\n<img src=\"https://d1.awsstatic.com/product-marketing/DMS/product-page-diagram-AWS-DMS_continuous-data-replication.a0e3bd328d2a4bd9b40a83e767199dcc13cf678f.png\">\nvia - https://aws.amazon.com/dms/</p>\n\n<p>You can migrate data to Amazon Redshift databases using AWS Database Migration Service. Amazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. With an Amazon Redshift database as a target, you can migrate data from all of the other supported source databases.\nThe Amazon Redshift cluster must be in the same AWS account and the same AWS Region as the replication instance.\nDuring a database migration to Amazon Redshift, AWS DMS first moves data to an Amazon S3 bucket. When the files reside in an Amazon S3 bucket, AWS DMS then transfers them to the proper tables in the Amazon Redshift data warehouse. AWS DMS creates the S3 bucket in the same AWS Region as the Amazon Redshift database. The AWS DMS replication instance must be located in that same region.</p>\n\n<p>Incorrect options:\n\"Use AWS Glue to replicate the data from the databases into Amazon Redshift\" - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing.\nUsing AWS Glue involves significant development efforts to write custom migration scripts to copy the database data into Redshift.</p>\n\n<p>\"Use AWS EMR to replicate the data from the databases into Amazon Redshift\" - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.\nUsing EMR involves significant infrastructure management efforts to set up and maintain the EMR cluster. Additionally this option involves a major development effort to write custom migration jobs to copy the database data into Redshift.</p>\n\n<p>\"Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift\" - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\nHowever, the user is expected to manually provision an appropriate number of shards to process the expected volume of the incoming data stream. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream. Therefore Kinesis Data Streams is not the right fit for this use-case.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html\nhttps://aws.amazon.com/dms/</p>\n",
			"question": "<p>A global leader in digital advertising solutions has significant investments in running Oracle and PostgreSQL services on Amazon RDS. The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?</p>\n",
			"answers": [
				"<p>Use AWS Database Migration Service to replicate the data from the databases into Amazon Redshift</p>",
				"<p>Use AWS Glue to replicate the data from the databases into Amazon Redshift</p>",
				"<p>Use AWS EMR to replicate the data from the databases into Amazon Redshift</p>",
				"<p>Use Amazon Kinesis Data Streams to replicate the data from the databases into Amazon Redshift</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A global leader in digital advertising solutions has significant investments in running Oracle and PostgreSQL services on Amazon RDS. The business analytics team has been running ad-hoc queries on these databases to prepare daily reports for senior management. The engineering team has observed that the database performance takes a hit whenever these reports are run by the analytics team. To facilitate the business analytics reporting, the engineering team now wants to replicate this data with high availability and consolidate these databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift. As a solutions architect, which of the following would you recommend as the MOST resource-efficient solution that requires the LEAST amount of development time without the need to manage the underlying infrastructure?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773030,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"AWS Kinesis Data Streams\"</p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\nAmazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another application that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</p>\n\n<p>KDS provides the ability for multiple applications to consume the same stream concurrently\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q6-i1.jpg\">\nvia - https://aws.amazon.com/kinesis/data-streams/faqs/</p>\n\n<p>Incorrect options:\n\"AWS Kinesis Data Firehose\" - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores, therefore this option is incorrect.</p>\n\n<p>\"AWS Kinesis Data Analytics\" - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers at the same time, therefore this option is incorrect.</p>\n\n<p>Exam alert:\nPlease remember that Kinesis Data Firehose is used to load streaming data into data stores (Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk) whereas Kinesis Data Streams provides support for real-time processing of streaming data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple downstream Amazon Kinesis Applications.</p>\n\n<p>References:\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/\nhttps://aws.amazon.com/kinesis/data-analytics/faqs/</p>\n",
			"question": "<p>One of the largest media and information companies in the world needs to develop a platform that would analyze real-time clickstream events such as readership statistics, impressions, and page views for more than 200 global websites and apps. The company has hired you as a solutions architect to consult the engineering team and develop a solution using the AWS Cloud. The company wants to use clickstream data to perform data science, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these groups would work independently and would need real-time access to this clickstream data for their applications. Which of the following AWS services provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a simultaneous feed of the data stream to the downstream applications?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>AWS Kinesis Data Streams</p>",
				"<p>AWS Kinesis Data Firehose</p>",
				"<p>AWS Kinesis Data Analytics</p>",
				"<p>Amazon SQS</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "One of the largest media and information companies in the world needs to develop a platform that would analyze real-time clickstream events such as readership statistics, impressions, and page views for more than 200 global websites and apps. The company has hired you as a solutions architect to consult the engineering team and develop a solution using the AWS Cloud. The company wants to use clickstream data to perform data science, develop algorithms, and create visualizations and dashboards to support the business stakeholders. Each of these groups would work independently and would need real-time access to this clickstream data for their applications. Which of the following AWS services provides a highly available and fault-tolerant solution to capture the clickstream events from the source and then provide a simultaneous feed of the data stream to the downstream applications?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772940,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use VPC endpoint to access Amazon SQS\"</p>\n\n<p>AWS customers can access Amazon Simple Queue Service (Amazon SQS) from their Amazon Virtual Private Cloud (Amazon VPC) using VPC endpoints, without using public IPs, and without needing to traverse the public internet. VPC endpoints for Amazon SQS are powered by AWS PrivateLink, a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services.\nAmazon VPC endpoints are easy to configure. They also provide reliable connectivity to Amazon SQS without requiring an internet gateway, Network Address Translation (NAT) instance, VPN connection, or AWS Direct Connect connection. With VPC endpoints, the data between your Amazon VPC and Amazon SQS queue is transferred within the Amazon network, helping protect your instances from internet traffic.\nAWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network. AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly simplify the network architecture.</p>\n\n<p>Incorrect options:\n\"Use Internet Gateway to access Amazon SQS\" - An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. It, therefore, imposes no availability risks or bandwidth constraints on your network traffic. This option is ruled out as the team does not want to use the public internet to access Amazon SQS.</p>\n\n<p>\"Use VPN connection to access Amazon SQS\" - AWS Site-to-Site VPN (aka VPN Connection) enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your intranet and Amazon VPC over the Internet. VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. As the existing infrastructure is within AWS Cloud, therefore a VPN connection is not required.</p>\n\n<p>\"Use Network Address Translation (NAT) instance to access Amazon SQS\" - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. Amazon provides Amazon Linux AMIs that are configured to run as NAT instances. These AMIs include the string amzn-ami-vpc-nat in their names, so you can search for them in the Amazon EC2 console. This option is ruled out because NAT instances are used to provide internet access to any instances in a private subnet.</p>\n\n<p>References:\nhttps://aws.amazon.com/privatelink/\nhttps://aws.amazon.com/about-aws/whats-new/2018/12/amazon-sqs-vpc-endpoints-aws-privatelink/</p>\n",
			"question": "<p>A leading Software as a Service (SaaS) TeleHealth services company has deployed its IT infrastructure on the AWS Cloud. The flagship application is hosted on multiple EC2 instances within a single VPC in the us-east-1 region. The engineering team at the company wants to use Amazon SQS to decouple some of the underlying processes but the team is concerned about accessing SQS over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Use VPC endpoint to access Amazon SQS</p>",
				"<p>Use Internet Gateway to access Amazon SQS</p>",
				"<p>Use Network Address Translation (NAT) instance to access Amazon SQS</p>",
				"<p>Use VPN connection to access Amazon SQS</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A leading Software as a Service (SaaS) TeleHealth services company has deployed its IT infrastructure on the AWS Cloud. The flagship application is hosted on multiple EC2 instances within a single VPC in the us-east-1 region. The engineering team at the company wants to use Amazon SQS to decouple some of the underlying processes but the team is concerned about accessing SQS over the public internet. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772966,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use Global Accelerator to provide a low latency way to ingest live sports results\"</p>\n\n<p>AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. AWS Global Accelerator is easy to set up, configure, and manage. It provides static IP addresses that provide a fixed entry point to your applications and eliminate the complexity of managing specific IP addresses for different AWS Regions and Availability Zones. AWS Global Accelerator always routes user traffic to the optimal endpoint based on performance, reacting instantly to changes in application health, your user’s location, and policies that you configure. Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Therefore, this option is correct.</p>\n\n<p>How AWS Global Accelerator Works\n<img src=\"https://d1.awsstatic.com/r2018/b/ubiquity/global-accelerator-how-it-works.feb297eb78d8cc55205874a1691e0ea2bc8bdbf1.png\">\nvia - https://aws.amazon.com/global-accelerator/</p>\n\n<p>Incorrect options:\n\"Use CloudFront to provide a low latency way to ingest live sports results\" - Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.\nCloudFront points of presence (POPs) (edge locations) make sure that popular content can be served quickly to your viewers. CloudFront also has regional edge caches that bring more of your content closer to your viewers, even when the content is not popular enough to stay at a POP, to help improve performance for that content. Regional edge caches help with all types of content, particularly content that tends to become less popular over time. Examples include user-generated content, such as video, photos, or artwork; e-commerce assets such as product photos and videos; and news and event-related content that might suddenly find new popularity. CloudFront supports HTTP/RTMP protocol based requests, therefore this option is incorrect.</p>\n\n<p>\"Use Elastic Load Balancer to provide a low latency way to ingest live sports results\" - Elastic Load Balancer automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones. Elastic Load Balancer cannot help with decreasing latency of incoming traffic from the source.</p>\n\n<p>\"Use Auto Scaling group to provide a low latency way to ingest live sports results\" - Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. Auto Scaling group cannot help with decreasing latency of incoming traffic from the source.</p>\n\n<p>Exam Alert:\nPlease note the differences between the capabilities of Global Accelerator and CloudFront -\nAWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world. CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions.\nGlobal Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover. Both services integrate with AWS Shield for DDoS protection.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/global-accelerator/\nhttps://aws.amazon.com/cloudfront/faqs/</p>\n",
			"question": "<p>A sports data company is delivering real-time data to media companies, sports federations, and the betting industry using its IT infrastructure on AWS Cloud. The company wants an urgent solution that provides a low-latency way to ingest live sports results regardless of where in the world their sports journalists are sitting. The live sports results are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for ingesting live sports results?</p>\n",
			"answers": [
				"<p>Use Elastic Load Balancer to provide a low latency way to ingest live sports results</p>",
				"<p>Use CloudFront to provide a low latency way to ingest live sports results</p>",
				"<p>Use Global Accelerator to provide a low latency way to ingest live sports results</p>",
				"<p>Use Auto Scaling group to provide a low latency way to ingest live sports results</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A sports data company is delivering real-time data to media companies, sports federations, and the betting industry using its IT infrastructure on AWS Cloud. The company wants an urgent solution that provides a low-latency way to ingest live sports results regardless of where in the world their sports journalists are sitting. The live sports results are delivered via a proprietary application using UDP protocol. As a solutions architect, which of the following solutions would you recommend such that it offers the BEST performance for ingesting live sports results?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772912,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Internet Gateway (I1)\"</p>\n\n<p>An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.\nAn Internet Gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. Therefore, for instance E1, the  Network Address Translation is done by Internet Gateway I1.</p>\n\n<p>Additionally, an Internet Gateway supports IPv4 and IPv6 traffic. It does not cause availability risks or bandwidth constraints on your network traffic.\nTo enable access to or from the internet for instances in a subnet in a VPC, you must do the following:\nAttach an Internet gateway to your VPC.\nAdd a route to your subnet's route table that directs internet-bound traffic to the internet gateway. If a subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet. If a subnet is associated with a route table that does not have a route to an internet gateway, it's known as a private subnet.\nEnsure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).\nEnsure that your network access control lists and security group rules allow the relevant traffic to flow to and from your instance.</p>\n\n<p>Internet Gateway Overview\n<img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/internet-gateway-overview-diagram.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</p>\n\n<p>Incorrect options:\n\"NAT instance (N1)\" - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet. As the instance E1 is in a public subnet, therefore this option is not correct.</p>\n\n<p>\"Subnet (S1)\" and \"Route Table (R1)\" - A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. A subnet is a range of IP addresses in your VPC. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Therefore neither Subnet nor Route Table can be used for Network Address Translation.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</p>\n",
			"question": "<p>The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a NAT instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the EC2 instance E1?</p>\n",
			"answers": [
				"<p>NAT instance (N1)</p>",
				"<p>Internet Gateway (I1)</p>",
				"<p>Subnet (S1)</p>",
				"<p>Route Table (R1)</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The DevOps team at an IT company has created a custom VPC (V1) and attached an Internet Gateway (I1) to the VPC. The team has also created a subnet (S1) in this custom VPC and added a route to this subnet's route table (R1) that directs internet-bound traffic to the Internet Gateway. Now the team launches an EC2 instance (E1) in the subnet S1 and assigns a public IPv4 address to this instance. Next the team also launches a NAT instance (N1) in the subnet S1. Under the given infrastructure setup, which of the following entities is doing the Network Address Translation for the EC2 instance E1?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772958,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"VPC with a public subnet only and AWS Site-to-Site VPN access\"</p>\n\n<p>The Amazon VPC console wizard provides the following four configurations:</p>\n\n<ol>\n<li>VPC with a single public subnet - The configuration for this scenario includes a virtual private cloud (VPC) with a single public subnet, and an internet gateway to enable communication over the internet. We recommend this configuration if you need to run a single-tier, public-facing web application, such as a blog or a simple website.</li>\n</ol>\n\n<p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/case-1.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario1.html</p>\n\n<ol>\n<li>VPC with public and private subnets (NAT) - The configuration for this scenario includes a virtual private cloud (VPC) with a public subnet and a private subnet. We recommend this scenario if you want to run a public-facing web application while maintaining back-end servers that aren't publicly accessible. A common example is a multi-tier website, with the web servers in a public subnet and the database servers in a private subnet. You can set up security and routing so that the web servers can communicate with the database servers.</li>\n</ol>\n\n<p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/nat-gateway-diagram.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</p>\n\n<ol>\n<li>VPC with public and private subnets and AWS Site-to-Site VPN access - The configuration for this scenario includes a virtual private cloud (VPC) with a public subnet and a private subnet, and a virtual private gateway to enable communication with your network over an IPsec VPN tunnel. We recommend this scenario if you want to extend your network into the cloud and also directly access the Internet from your VPC. This scenario enables you to run a multi-tiered application with a scalable web front end in a public subnet and to house your data in a private subnet that is connected to your network by an IPsec AWS Site-to-Site VPN connection.</li>\n</ol>\n\n<p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/case-3.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario3.html</p>\n\n<ol>\n<li>VPC with a private subnet only and AWS Site-to-Site VPN access - The configuration for this scenario includes a virtual private cloud (VPC) with a single private subnet, and a virtual private gateway to enable communication with your network over an IPsec VPN tunnel. There is no Internet gateway to enable communication over the Internet. We recommend this scenario if you want to extend your network into the cloud using Amazon's infrastructure without exposing your network to the Internet.</li>\n</ol>\n\n<p><img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/case-4.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario4.html</p>\n\n<p>Therefore, the option \"VPC with a public subnet only and AWS Site-to-Site VPN access\" is NOT supported by the Amazon VPC console wizard.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_wizard.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html</p>\n",
			"question": "<p>An e-commerce company is planning to migrate their two-tier application from on-premises infrastructure to AWS Cloud. The company will continue to have some of their data assets on the on-premises infrastructure. As the engineering team is new to the AWS Cloud, they are planning to use the Amazon VPC console wizard to set up the networking configuration for the two-tier application having public web servers and private database servers. As part of the due diligence, the team has prepared a report with all configuration options available for Amazon VPC console wizard, however, there is a mistake in one of the configuration options listed in the report. Can you spot the configuration that is NOT supported by the Amazon VPC console wizard?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>VPC with a single public subnet</p>",
				"<p>VPC with public and private subnets (NAT)</p>",
				"<p>VPC with public and private subnets and AWS Site-to-Site VPN access</p>",
				"<p>VPC with a public subnet only and AWS Site-to-Site VPN access</p>"
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An e-commerce company is planning to migrate their two-tier application from on-premises infrastructure to AWS Cloud. The company will continue to have some of their data assets on the on-premises infrastructure. As the engineering team is new to the AWS Cloud, they are planning to use the Amazon VPC console wizard to set up the networking configuration for the two-tier application having public web servers and private database servers. As part of the due diligence, the team has prepared a report with all configuration options available for Amazon VPC console wizard, however, there is a mistake in one of the configuration options listed in the report. Can you spot the configuration that is NOT supported by the Amazon VPC console wizard?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772930,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use EC2 dedicated hosts\"</p>\n\n<p>You can use Dedicated Hosts to launch Amazon EC2 instances on physical servers that are dedicated for your use. Dedicated Hosts give you additional visibility and control over how instances are placed on a physical server, and you can reliably use the same physical server over time. As a result, Dedicated Hosts enable you to use your existing server-bound software licenses like Windows Server and address corporate compliance and regulatory requirements.</p>\n\n<p>Incorrect options:\n\"Use EC2 dedicated instances\" - Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. Dedicated instances cannot be used for existing server-bound software licenses.</p>\n\n<p>\"Use EC2 on-demand instances\" and \"Use EC2 reserved instances\" - Amazon EC2 presents a virtual computing environment, allowing you to use web service interfaces to launch instances with a variety of operating systems, load them with your custom application environment, manage your network’s access permissions, and run your image using as many or few systems as you desire.\nAmazon EC2 provides the following purchasing options to enable you to optimize your costs based on your needs:\nOn-Demand Instances – Pay, by the second, for the instances that you launch.\nReserved Instances – Reduce your Amazon EC2 costs by making a commitment to a consistent instance configuration, including instance type and Region, for a term of 1 or 3 years.\nNeither on-demand instances nor reserved instances can be used for existing server-bound software licenses.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/ec2/dedicated-hosts/\nhttps://aws.amazon.com/ec2/dedicated-hosts/faqs/\nhttps://aws.amazon.com/ec2/pricing/dedicated-instances/</p>\n",
			"question": "<p>A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?</p>\n",
			"answers": [
				"<p>Use EC2 on-demand instances</p>",
				"<p>Use EC2 reserved instances</p>",
				"<p>Use EC2 dedicated instances</p>",
				"<p>Use EC2 dedicated hosts</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A financial services company is looking to move its on-premises IT infrastructure to AWS Cloud. The company has multiple long-term server bound licenses across the application stack and the CTO wants to continue to utilize those licenses while moving to AWS. As a solutions architect, which of the following would you recommend as the MOST cost-effective solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772926,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"NAT instance can be used as a bastion server\"\n\"Security Groups can be associated with a NAT instance\"\n\"NAT instance supports port forwarding\"</p>\n\n<p>A NAT instance or a NAT Gateway can be used in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet.</p>\n\n<p>How NAT Gateway works:\n<img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/nat-gateway-diagram.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</p>\n\n<p>How NAT Instance works:\n<img src=\"https://docs.aws.amazon.com/vpc/latest/userguide/images/nat-instance-diagram.png\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</p>\n\n<p>Please see this high-level summary of the differences between NAT instances and NAT gateways relevant to the options described in the question:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q12-i1.jpg\">\nvia - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</p>\n",
			"question": "<p>The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a NAT instance or a NAT gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the NAT instance and the NAT gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)</p>\n",
			"answers": [
				"<p>NAT gateway supports port forwarding</p>",
				"<p>Security Groups can be associated with a NAT gateway</p>",
				"<p>NAT gateway can be used as a bastion server</p>",
				"<p>NAT instance can be used as a bastion server</p>",
				"<p>Security Groups can be associated with a NAT instance</p>",
				"<p>NAT instance supports port forwarding</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d",
			"e",
			"f"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The DevOps team at an IT company is provisioning a two-tier application in a VPC with a public subnet and a private subnet. The team wants to use either a NAT instance or a NAT gateway in the public subnet to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet but needs some technical assistance in terms of the configuration options available for the NAT instance and the NAT gateway. As a solutions architect, which of the following options would you identify as CORRECT? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772928,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance\"</p>\n\n<p>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives a connection request, it selects a target from the target group for the default rule. It attempts to open a TCP connection to the selected target on the port specified in the listener configuration.\nRequest Routing and IP Addresses -\nIf you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. The load balancer rewrites the destination IP address from the data packet before forwarding it to the target instance.\nIf you specify targets using IP addresses, you can route traffic to an instance using any private IP address from one or more network interfaces. This enables multiple applications on an instance to use the same port. Note that each network interface can have its security group. The load balancer rewrites the destination IP address before forwarding it to the target.</p>\n\n<p>Incorrect options:\n\"Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance\" - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So public IP address cannot be used to route the traffic to the instance.</p>\n\n<p>\"Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance\" - If you specify targets using an instance ID, traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance. So elastic IP address cannot be used to route the traffic to the instance.</p>\n\n<p>\"Traffic is routed to instances using the instance ID specified in the primary network interface for the instance\" - You cannot use instance ID to route traffic to the instance. This option is just added as a distractor.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html</p>\n",
			"question": "<p>A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer (NLB) to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Traffic is routed to instances using the primary private IP address specified in the primary network interface for the instance</p>",
				"<p>Traffic is routed to instances using the primary public IP address specified in the primary network interface for the instance</p>",
				"<p>Traffic is routed to instances using the primary elastic IP address specified in the primary network interface for the instance</p>",
				"<p>Traffic is routed to instances using the instance ID specified in the primary network interface for the instance</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A leading online gaming company is migrating its flagship application to AWS Cloud for delivering its online games to users across the world. The company would like to use a Network Load Balancer (NLB) to handle millions of requests per second. The engineering team has provisioned multiple instances in a public subnet and specified these instance IDs as the targets for the NLB. As a solutions architect, can you help the engineering team understand the correct routing mechanism for these target instances?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772976,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each\"</p>\n\n<p>The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Therefore, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. When cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone. Therefore, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each.</p>\n\n<p>Consider the following diagrams to understand the effect of cross-zone load balancing.</p>\n\n<p>If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route its 50% of the client traffic to all 10 targets.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_disabled.png\">\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</p>\n\n<p>If cross-zone load balancing is disabled:\nEach of the two targets in Availability Zone A receives 25% of the traffic.\nEach of the eight targets in Availability Zone B receives 6.25% of the traffic.\nThis is because each load balancer node can route its 50% of the client traffic only to targets in its Availability Zone</p>\n\n<p><img src=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/images/cross_zone_load_balancing_disabled.png\">\nvia - https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</p>\n\n<p>The traffic distribution pattern for other options is incorrect.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</p>\n",
			"question": "<p>An e-commerce company is using an Elastic Load Balancer for its fleet of EC2 instances spread across two Availability Zones, with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?</p>\n",
			"answers": [
				"<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each</p>",
				"<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each</p>",
				"<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives 50% traffic and four instances in Availability Zone B receive 12.5% traffic each</p>",
				"<p>With cross-zone load balancing enabled, one instance in Availability Zone A receives 20% traffic and four instances in Availability Zone B receive 20% traffic each. With cross-zone load balancing disabled, one instance in Availability Zone A receives no traffic and four instances in Availability Zone B receive 25% traffic each</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An e-commerce company is using an Elastic Load Balancer for its fleet of EC2 instances spread across two Availability Zones, with one instance as a target in Availability Zone A and four instances as targets in Availability Zone B. The company is doing benchmarking for server performance when cross-zone load balancing is enabled compared to the case when cross-zone load balancing is disabled. As a solutions architect, which of the following traffic distribution outcomes would you identify as correct?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773038,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"By default, basic monitoring is enabled when you use the AWS management console to create a launch configuration. Detailed monitoring is enabled by default when you create a launch configuration using the AWS CLI\"</p>\n\n<p>A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information to launch the instance.\nYou configure monitoring for EC2 instances using a launch configuration or template. Monitoring is enabled whenever an instance is launched, either basic monitoring (5-minute granularity) or detailed monitoring (1-minute granularity). For detailed monitoring, additional charges apply.\nBy default, basic monitoring is enabled when you create a launch template or when you use the AWS Management Console to create a launch configuration. Detailed monitoring is enabled by default when you create a launch configuration using the AWS CLI or an SDK.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q15-i1.jpg\">\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</p>\n\n<p>Incorrect options:\n\"By default, basic monitoring is enabled when you use the AWS management console or AWS CLI to create a launch configuration. For the given use-case, the detailed monitoring option for the instances must have been explicitly set to enabled while creating the Launch Configuration from the AWS CLI\" - When we use AWS CLI, then detailed monitoring is enabled by default, hence this option is incorrect.</p>\n\n<p>\"By default, detailed monitoring is enabled when you use the AWS management console or AWS CLI to create a launch configuration. For the given use-case, the basic monitoring option for the instances must have been explicitly set to enabled while creating the Launch Configuration from the AWS management console\" - When we use AWS management console, then basic monitoring is enabled by default, hence this option is incorrect.</p>\n\n<p>\"This seems to be an issue with the us-east-1 region. The issue can be resolved by moving the instances to us-west-1 region\" - AWS region has no impact on the monitoring type for instances. This option has been added as a distractor.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</p>\n",
			"question": "<p>A social media application is hosted on a fleet of EC2 instances in the us-east-1 region and the instances are part of an Auto Scaling group. The Auto Scaling group and the Launch Configuration for the instances are managed via a shell script using the AWS CLI. The billing department at the company has noticed that all instances created via this Launch Configuration have detailed monitoring enabled and have been incurring additional costs. To verify this behavior, the lead engineer logs into the AWS management console and creates another Launch Configuration with the same settings but finds that the instances created via this new Launch Configuration have only basic monitoring enabled. As a solutions architect, how would you explain this issue described in the use-case?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>By default, basic monitoring is enabled when you use the AWS management console to create a launch configuration. Detailed monitoring is enabled by default when you create a launch configuration using the AWS CLI</p>",
				"<p>By default, basic monitoring is enabled when you use the AWS management console or AWS CLI to create a launch configuration. For the given use-case, the detailed monitoring option for the instances must have been explicitly set to enabled while creating the Launch Configuration from the AWS CLI</p>",
				"<p>By default, detailed monitoring is enabled when you use the AWS management console or AWS CLI to create a launch configuration. For the given use-case, the basic monitoring option for the instances must have been explicitly set to enabled while creating the Launch Configuration from the AWS management console</p>",
				"<p>This seems to be an issue with the us-east-1 region. The issue can be resolved by moving the instances to us-west-1 region</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A social media application is hosted on a fleet of EC2 instances in the us-east-1 region and the instances are part of an Auto Scaling group. The Auto Scaling group and the Launch Configuration for the instances are managed via a shell script using the AWS CLI. The billing department at the company has noticed that all instances created via this Launch Configuration have detailed monitoring enabled and have been incurring additional costs. To verify this behavior, the lead engineer logs into the AWS management console and creates another Launch Configuration with the same settings but finds that the instances created via this new Launch Configuration have only basic monitoring enabled. As a solutions architect, how would you explain this issue described in the use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772934,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"You can change the tenancy of an instance from dedicated to host\"\n\"You can change the tenancy of an instance from host to dedicated\"</p>\n\n<p>Each EC2 instance that you launch into a VPC has a tenancy attribute. This attribute has the following values.\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q16-i1.jpg\">\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</p>\n\n<p>By default, EC2 instances run on a shared-tenancy basis.\nDedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at the hardware level. However, Dedicated Instances may share hardware with other instances from the same AWS account that is not Dedicated Instances.\nA Dedicated Host is also a physical server that's dedicated to your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Incorrect options:\n\"You can change the tenancy of an instance from default to dedicated\" - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect.</p>\n\n<p>\"You can change the tenancy of an instance from dedicated to default\" - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect.</p>\n\n<p>\"You can change the tenancy of an instance from default to host\" - You can only change the tenancy of an instance from dedicated to host, or from host to dedicated after you've launched it. Therefore, this option is incorrect.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</p>\n",
			"question": "<p>An IT company is looking to move its on-premises infrastructure to AWS Cloud. The company has a portfolio of applications with a few of them using server bound licenses that are valid for the next year. To utilize the licenses, the CTO wants to use dedicated hosts for a one year term and then migrate the given instances to default tenancy thereafter. As a solutions architect, which of the following options would you identify as CORRECT for changing the tenancy of an instance after you have launched it? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>You can change the tenancy of an instance from dedicated to host</p>",
				"<p>You can change the tenancy of an instance from host to dedicated</p>",
				"<p>You can change the tenancy of an instance from default to dedicated</p>",
				"<p>You can change the tenancy of an instance from dedicated to default</p>",
				"<p>You can change the tenancy of an instance from default to host</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a",
			"b"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "An IT company is looking to move its on-premises infrastructure to AWS Cloud. The company has a portfolio of applications with a few of them using server bound licenses that are valid for the next year. To utilize the licenses, the CTO wants to use dedicated hosts for a one year term and then migrate the given instances to default tenancy thereafter. As a solutions architect, which of the following options would you identify as CORRECT for changing the tenancy of an instance after you have launched it? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772984,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\"</p>\n\n<p>VPC sharing (part of Resource Access Manager) allows multiple AWS accounts to create their application resources such as EC2 instances, RDS databases, Redshift clusters, and Lambda functions, into shared and centrally-managed Amazon Virtual Private Clouds (VPCs). To set this up, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.\nYou can share Amazon VPCs to leverage the implicit routing within a VPC for applications that require a high degree of interconnectivity and are within the same trust boundaries. This reduces the number of VPCs that you create and manage while using separate accounts for billing and access control.</p>\n\n<p>Incorrect options:\n\"Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations\" - Using VPC sharing, an account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. The owner account cannot share the VPC itself. Therefore this option is incorrect.</p>\n\n<p>\"Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations\" - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Therefore this option is incorrect.</p>\n\n<p>\"Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations\" - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. VPC peering does not facilitate centrally managed VPCs. Moreover, an AWS owner account cannot share the VPC itself with another AWS account. Therefore this option is incorrect.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html</p>\n",
			"question": "<p>A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up \"AWS Organizations\" to manage several departments running their AWS accounts and using resources such as EC2 instances and RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use VPC sharing to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
				"<p>Use VPC sharing to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
				"<p>Use VPC peering to share one or more subnets with other AWS accounts belonging to the same parent organization from AWS Organizations</p>",
				"<p>Use VPC peering to share a VPC with other AWS accounts belonging to the same parent organization from AWS Organizations</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail company uses AWS Cloud to manage its IT infrastructure. The company has set up \"AWS Organizations\" to manage several departments running their AWS accounts and using resources such as EC2 instances and RDS databases. The company wants to provide shared and centrally-managed VPCs to all departments using applications that need a high degree of interconnectivity. As a solutions architect, which of the following options would you choose to facilitate this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772996,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN\"</p>\n\n<p>Amazon VPC provides the facility to create an IPsec VPN connection (also known as site-to-site VPN) between remote customer networks and their Amazon VPC over the internet. The following are the key concepts for a site-to-site VPN:\nVirtual private gateway: A Virtual Private Gateway (also known as a VPN Gateway) is the endpoint on the AWS VPC side of your VPN connection.\nVPN connection: A secure connection between your on-premises equipment and your VPCs.\nVPN tunnel: An encrypted link where data can pass from the customer network to or from AWS.\nCustomer Gateway: An AWS resource that provides information to AWS about your Customer Gateway device.\nCustomer Gateway device: A physical device or software application on the customer side of the Site-to-Site VPN connection.</p>\n\n<p>AWS Managed IPSec VPN\n<img src=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/images/aws-managed-vpn.png\">\nvia - https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html</p>\n\n<p>Incorrect options:\n\"Create a Virtual Private Gateway on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN\" - You need to create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.</p>\n\n<p>\"Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN\" - You need to create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.</p>\n\n<p>\"Create a Virtual Private Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN\" - You need to create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN. Therefore, this option is wrong.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn-network-to-amazon.html\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html</p>\n",
			"question": "<p>A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Create a Virtual Private Gateway on the on-premises side of the VPN and a Customer Gateway on the AWS side of the VPN</p>",
				"<p>Create a Customer Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN</p>",
				"<p>Create a Virtual Private Gateway on the AWS side of the VPN and a Customer Gateway on the on-premises side of the VPN</p>",
				"<p>Create a Virtual Private Gateway on both the AWS side of the VPN as well as the on-premises side of the VPN</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail organization is moving some of its on-premises data to AWS Cloud. The DevOps team at the organization has set up an AWS Managed IPSec VPN Connection between their remote on-premises network and their Amazon VPC over the internet. Which of the following represents the correct configuration for the IPSec VPN Connection?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772974,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com\"</p>\n\n<p>Alias records provide a Route 53–specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets.\nYou can create an alias record at the top node of a DNS namespace, also known as the zone apex, however, you cannot create a CNAME record for the top node of the DNS namespace. So, if you register the DNS name covid19survey.com, the zone apex is covid19survey.com. You can't create a CNAME record for covid19survey.com, but you can create an alias record for covid19survey.com that routes traffic to www.example.com.</p>\n\n<p>Exam Alert:\nYou should also note that Route 53 doesn't charge for alias queries to AWS resources but Route 53 does charge for CNAME queries. Additionally, an alias record can only redirect queries to selected AWS resources such as S3 buckets, CloudFront distributions, and another record in the same Route 53 hosted zone; however a CNAME record can redirect DNS queries to any DNS record. So, you can create a CNAME record that redirects queries from app.covid19survey.com to app.covid19survey.net.</p>\n\n<p>Incorrect options:\n\"Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com\" - You cannot create a CNAME record for the top node of the DNS namespace, so this option is incorrect.</p>\n\n<p>\"Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com\" - An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. It cannot be used to create a Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect.</p>\n\n<p>\"Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com\" - An NS record identifies the name servers for the hosted zone. It cannot be used to create a Route 53 record to route traffic for the top node of the DNS namespace, so this option is incorrect.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html</p>\n",
			"question": "<p>A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Route 53. The web development team would like to create a Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?</p>\n",
			"answers": [
				"<p>Create a CNAME record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
				"<p>Create an alias record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
				"<p>Create an MX record for covid19survey.com that routes traffic to www.covid19survey.com</p>",
				"<p>Create an NS record for covid19survey.com that routes traffic to www.covid19survey.com</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A startup has created a new web application for users to complete a risk assessment survey for COVID-19 symptoms via a self-administered questionnaire. The startup has purchased the domain covid19survey.com using Route 53. The web development team would like to create a Route 53 record so that all traffic for covid19survey.com is routed to www.covid19survey.com. As a solutions architect, which of the following is the MOST cost-effective solution that you would recommend to the web development team?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772982,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"AWS Kinesis Data Streams\"</p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\nAmazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).\nAmazon Kinesis Data Streams is recommended when you need the ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.</p>\n\n<p>KDS provides the ability to consume records in the same order a few hours later\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q20-i1.jpg\">\nvia - https://aws.amazon.com/kinesis/data-streams/faqs/</p>\n\n<p>Incorrect options:\n\"AWS Kinesis Data Firehose\" - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. As Kinesis Data Firehose is used to load streaming data into data stores , therefore this option is incorrect.</p>\n\n<p>\"AWS Kinesis Data Analytics\" - Amazon Kinesis Data Analytics is the easiest way to analyze streaming data in real-time.  You can quickly build SQL queries and sophisticated Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale. Kinesis Data Analytics enables you to easily and quickly build queries and sophisticated streaming applications in three simple steps: setup your streaming data sources, write your queries or streaming applications and set up your destination for processed data. As Kinesis Data Analytics is used to build SQL queries and sophisticated Java applications, therefore this option is incorrect.</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. For SQS, you cannot have the same message being consumed by multiple consumers in the same order a few hours later, therefore this option is incorrect.</p>\n\n<p>References:\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nhttps://aws.amazon.com/kinesis/data-firehose/faqs/\nhttps://aws.amazon.com/kinesis/data-analytics/faqs/</p>\n",
			"question": "<p>A leading news aggregation company offers hundreds of digital products and services for customers ranging from law firms to banks to consumers. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days. As a solutions architect, which of the following AWS services provides the ability to run the billing process and auditing process on the given clickstream data in the same order?</p>\n",
			"answers": [
				"<p>Amazon SQS</p>",
				"<p>AWS Kinesis Data Firehose</p>",
				"<p>AWS Kinesis Data Analytics</p>",
				"<p>AWS Kinesis Data Streams</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A leading news aggregation company offers hundreds of digital products and services for customers ranging from law firms to banks to consumers. The company bills its clients based on per unit of clickstream data provided to the clients. As the company operates in a regulated industry, it needs to have the same ordered clickstream data available for auditing within a window of 7 days. As a solutions architect, which of the following AWS services provides the ability to run the billing process and auditing process on the given clickstream data in the same order?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772978,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have dedicated instance tenancy\"</p>\n\n<p>A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information to launch the instance.</p>\n\n<p>When you create a launch configuration, the default value for the instance placement tenancy is null and the instance tenancy is controlled by the tenancy attribute of the VPC.  If you set the Launch Configuration Tenancy to default and the VPC Tenancy is set to dedicated, then the instances have dedicated tenancy. If you set the Launch Configuration Tenancy to dedicated and the VPC Tenancy is set to default, then again the instances have dedicated tenancy.</p>\n\n<p>Launch Configuration Tenancy vs VPC Tenancy\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q21-i1.jpg\">\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-in-vpc.html#as-vpc-tenancy</p>\n\n<p>Incorrect options:\n\"The instances launched by Launch Configuration LC1 will have dedicated instance tenancy while the instances launched by the Launch Configuration LC2 will have default instance tenancy\" - If either Launch Configuration Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.</p>\n\n<p>\"The instances launched by Launch Configuration LC1 will have default instance tenancy while the instances launched by the Launch Configuration LC2 will have dedicated instance tenancy\" - If either Launch Configuration Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.</p>\n\n<p>\"The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have default instance tenancy\" - If either Launch Configuration Tenancy or VPC Tenancy is set to dedicated, then the instance tenancy is also dedicated. Therefore, this option is incorrect.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-in-vpc.html#as-vpc-tenancy</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>The instances launched by Launch Configuration LC1 will have dedicated instance tenancy while the instances launched by the Launch Configuration LC2 will have default instance tenancy</p>",
				"<p>The instances launched by Launch Configuration LC1 will have default instance tenancy while the instances launched by the Launch Configuration LC2 will have dedicated instance tenancy</p>",
				"<p>The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have default instance tenancy</p>",
				"<p>The instances launched by both Launch Configuration LC1 and Launch Configuration LC2 will have dedicated instance tenancy</p>"
			],
			"question": "<p>A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group (ASG). The ASG uses a Launch Configuration (LC1) with \"dedicated\" instance placement tenancy but the VPC (V1) used by the Launch Configuration LC1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Configuration (LC2) with \"default\" instance placement tenancy but the VPC (V2) used by the Launch Configuration LC2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Configuration LC1 and Launch Configuration LC2?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A video conferencing application is hosted on a fleet of EC2 instances which are part of an Auto Scaling group (ASG). The ASG uses a Launch Configuration (LC1) with \"dedicated\" instance placement tenancy but the VPC (V1) used by the Launch Configuration LC1 has the instance tenancy set to default. Later the DevOps team creates a new Launch Configuration (LC2) with \"default\" instance placement tenancy but the VPC (V2) used by the Launch Configuration LC2 has the instance tenancy set to dedicated. Which of the following is correct regarding the instances launched via Launch Configuration LC1 and Launch Configuration LC2?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773024,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use AZ ID to uniquely identify the Availability Zones across the two AWS Accounts\"</p>\n\n<p>An Availability Zone is represented by a region code followed by a letter identifier; for example, us-east-1a. To ensure that resources are distributed across the Availability Zones for a region, AWS maps Availability Zones to names for each AWS account. For example, the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account.</p>\n\n<p>To coordinate Availability Zones across accounts, you must use the AZ ID, which is a unique and consistent identifier for an Availability Zone. For example, usw2-az2 is an AZ ID for the us-west-2 region and it has the same location in every AWS account.</p>\n\n<p>Viewing AZ IDs enables you to determine the location of resources in one account relative to the resources in another account. For example, if you share a subnet in the Availability Zone with the AZ ID usw2-az2 with another account, this subnet is available to that account in the Availability Zone whose AZ ID is also usw2-az2.</p>\n\n<p>You can view the AZ IDs by going to the service health section of the EC2 Dashboard via your AWS Management Console.</p>\n\n<p>AZ IDs for Availability Zones\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q22-i1.jpg\"></p>\n\n<p>Incorrect options:\n\"Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts\" - A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. Since a VPC spans an AWS region, it cannot be used to uniquely identify an Availability Zone. Therefore, this option is incorrect.</p>\n\n<p>\"Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts\" - A subnet is a range of IP addresses in your VPC. A subnet spans an Availability Zone of an AWS region. The default subnet representing the Availability Zone us-west-2a for one AWS account might not be the same location as us-west-2a for another AWS account. Therefore, this option is incorrect.</p>\n\n<p>\"Reach out to AWS Support for creating the EC2 instances in the same Availability Zone across the two AWS accounts\" - Since the AZ ID is a unique and consistent identifier for an Availability Zone, there is no need to contact AWS Support. Therefore, this option is incorrect.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html</p>\n",
			"question": "<p>An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in the us-west-2 region. The IT consultant is trying to launch an EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone of the us-west-2 region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones are still different. As a solutions architect, which of the following would you suggest resolving this issue?</p>\n",
			"answers": [
				"<p>Reach out to AWS Support for creating the EC2 instances in the same Availability Zone across the two AWS accounts</p>",
				"<p>Use AZ ID to uniquely identify the Availability Zones across the two AWS Accounts</p>",
				"<p>Use the default subnet to uniquely identify the Availability Zones across the two AWS Accounts</p>",
				"<p>Use the default VPC to uniquely identify the Availability Zones across the two AWS Accounts</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An IT consultant is helping a small business revamp their technology infrastructure on the AWS Cloud. The business has two AWS accounts and all resources are provisioned in the us-west-2 region. The IT consultant is trying to launch an EC2 instance in each of the two AWS accounts such that the instances are in the same Availability Zone of the us-west-2 region. Even after selecting the same default subnet (us-west-2a) while launching the instances in each of the AWS accounts, the IT consultant notices that the Availability Zones are still different. As a solutions architect, which of the following would you suggest resolving this issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772988,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"VPN CloudHub\"</p>\n\n<p>If you have multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub. This enables your remote sites to communicate with each other, and not just with the VPC. Sites that use AWS Direct Connect connections to the virtual private gateway can also be part of the AWS VPN CloudHub. The VPN CloudHub operates on a simple hub-and-spoke model that you can use with or without a VPC. This design is suitable if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.\nPer the given use-case, the corporate headquarters has an AWS Direct Connect connection to the VPC and the branch offices have Site-to-Site VPN connections to the VPC. Therefore using the AWS VPN CloudHub, branch offices can send and receive data with each other as well as with their corporate headquarters.</p>\n\n<p>VPN CloudHub\n<img src=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/images/AWS_VPN_CloudHub-diagram.png\">\nvia - https://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html</p>\n\n<p>Incorrect options:\n\"VPC Endpoint\" - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet.\nWhen you use VPC endpoint, the traffic between your VPC and the other AWS service does not leave the Amazon network, therefore this option cannot be used to send and receive data between the remote branch offices of the company.</p>\n\n<p>\"VPC Peering\" - A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.\nVPC peering facilitates a connection between two VPCs within the AWS network, therefore this option cannot be used to send and receive data between the remote branch offices of the company.</p>\n\n<p>\"Software VPN\" - Amazon VPC offers you the flexibility to fully manage both sides of your Amazon VPC connectivity by creating a VPN connection between your remote network and a software VPN appliance running in your Amazon VPC network. Since Software VPN just handles connectivity between the remote network and Amazon VPC, therefore it cannot be used to send and receive data between the remote branch offices of the company.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub-network-to-amazon.html\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html</p>\n",
			"question": "<p>A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>VPC Peering</p>",
				"<p>VPN CloudHub</p>",
				"<p>Software VPN</p>",
				"<p>VPC Endpoint</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A media company has its corporate headquarters in Los Angeles with an on-premises data center using an AWS Direct Connect connection to the AWS VPC. The branch offices in San Francisco and Miami use Site-to-Site VPN connections to connect to the AWS VPC. The company is looking for a solution to have the branch offices send and receive data with each other as well as with their corporate headquarters. As a solutions architect, which of the following AWS services would you recommend addressing this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773012,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Elastic Fabric Adapter\"</p>\n\n<p>An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High Performance Computing (HPC) and machine learning applications. It enhances the performance of inter-instance communication that is critical for scaling HPC and machine learning applications. EFA devices provide all Elastic Network Adapter (ENA) devices functionalities plus a new OS bypass hardware interface that allows user-space applications to communicate directly with the hardware-provided reliable transport functionality.</p>\n\n<p>How Elastic Fabric Adapter Works\n<img src=\"https://d1.awsstatic.com/Product-Page-Diagram_Elastic-Fabric-Adapter_How-it-Works_updated.2a51303e17a203eb094ab098ebc31a61dab66365.png\">\nvia - https://aws.amazon.com/hpc/efa/</p>\n\n<p>Incorrect options:\n\"Elastic Network Interface\" - An Elastic Network Interface (ENI) is a logical networking component in a VPC that represents a virtual network card. You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The ENI is the simplest networking component available on AWS and is insufficient for HPC workflows.</p>\n\n<p>\"Elastic Network Adapter\" - Elastic Network Adapter (ENA) devices support enhanced networking via single root I/O virtualization (SR-IOV) to provide high-performance networking capabilities. Although enhanced networking provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies, still EFA is a better fit for the given use-case because the EFA device provides all the functionality of an ENA device, plus hardware support for applications to communicate directly with the EFA device without involving the instance kernel (OS-bypass communication) using an extended programming interface.</p>\n\n<p>\"Elastic IP Address\" - An Elastic IP address is a static IPv4 address associated with your AWS account. An Elastic IP address is a public IPv4 address, which is reachable from the internet. It is not a networking device that can be used to facilitate HPC workflows.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\nhttps://aws.amazon.com/hpc/efa/</p>\n",
			"question": "<p>A software genomics company provides end-to-end genomic data analysis and reporting solutions to hospitals and labs. The company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze whole genomes and gene panels for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the EC2 instances running these HPC workflows?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Elastic Fabric Adapter</p>",
				"<p>Elastic Network Interface</p>",
				"<p>Elastic Network Adapter</p>",
				"<p>Elastic IP Address</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A software genomics company provides end-to-end genomic data analysis and reporting solutions to hospitals and labs. The company has multiple High Performance Computing (HPC) workflows that quickly and accurately process and analyze whole genomes and gene panels for hereditary diseases. The company is looking to migrate these workflows from their on-premises infrastructure to AWS Cloud. As a solutions architect, which of the following networking components would you recommend on the EC2 instances running these HPC workflows?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773028,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Scheduled Reserved Instances\"</p>\n\n<p>EC2 Scheduled Reserved Instances provide a capacity reservation so that you can have confidence in your ability to launch the number of instances you have reserved when you need them. Scheduled Reserved Instances run on a part-time basis. Scheduled Reserved Instances option allows you to reserve capacity on a recurring daily, weekly, and monthly schedule. For the given use-case, you can purchase a daily reservation such as 1 am to 2 am every day so that the daily job to backup media assets is guaranteed to run during the defined time window. Scheduled Reserved Instances are available for one-year terms at 5-10% below On-Demand rates.</p>\n\n<p>Incorrect options:\n\"On-Demand Instances\" - With an On-Demand instance, you pay for compute capacity by the hour or the second depending on which instances you run. It is a good fit for applications with short-term, spiky, or unpredictable workloads that cannot be interrupted. For the given use-case, we don't need the instance to be always running, as that would incur unnecessary costs. We could use a script to run the on-demand instance only during the defined job window, still, it would be expensive compared to Scheduled Reserved instances (remember that Scheduled Reserved Instances are available for one-year terms at 5-10% below On-Demand rates)</p>\n\n<p>Spot Instances – A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price).\nAs the Spot Instance runs whenever capacity is available, there is no guarantee that the daily job will be executed during the defined time window. Therefore this option is incorrect.</p>\n\n<p>Dedicated instances are Amazon EC2 instances that run in a VPC on hardware that's dedicated to a single customer. Your dedicated instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. Dedicated instances may share hardware with other instances from the same AWS account that are not dedicated instances. The given use-case does not require the instances to run on dedicated hardware, so it's not the correct choice.</p>\n\n<p>References:\nhttps://aws.amazon.com/about-aws/whats-new/2016/01/announcing-amazon-ec2-reserved-instances-for-recurring-instances/\nhttps://aws.amazon.com/ec2/pricing/</p>\n",
			"question": "<p>A social media company runs its IT infrastructure on the AWS Cloud. The company has a batch job running at 1 am daily that takes a backup of the media assets uploaded for the past day and the entire job takes about 30 minutes to run. The company wants to use the MOST cost-effective long-term purchasing option for EC2 instances such that the job is guaranteed to run during the defined time window. As a solutions architect, which of the following EC2 purchase options would you recommend?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>On-Demand Instances</p>",
				"<p>Spot Instances</p>",
				"<p>Dedicated Instances</p>",
				"<p>Scheduled Reserved Instances</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A social media company runs its IT infrastructure on the AWS Cloud. The company has a batch job running at 1 am daily that takes a backup of the media assets uploaded for the past day and the entire job takes about 30 minutes to run. The company wants to use the MOST cost-effective long-term purchasing option for EC2 instances such that the job is guaranteed to run during the defined time window. As a solutions architect, which of the following EC2 purchase options would you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773002,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use delay queues to postpone the delivery of new messages to the queue for a few seconds\"</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\nDelay queues let you postpone the delivery of new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-delay-queues-diagram.png\">\nvia - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</p>\n\n<p>Incorrect options:\n\"Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds\" - SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. You cannot use FIFO queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>\"Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds\" - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>\"Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds\" - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of new messages to the queue for a few seconds.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html</p>\n",
			"question": "<p>An IT company is using SQS queues for decoupling the various components of application architecture. As the consuming components need additional time to process SQS messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?</p>\n",
			"answers": [
				"<p>Use FIFO queues to postpone the delivery of new messages to the queue for a few seconds</p>",
				"<p>Use dead-letter queues to postpone the delivery of new messages to the queue for a few seconds</p>",
				"<p>Use visibility timeout to postpone the delivery of new messages to the queue for a few seconds</p>",
				"<p>Use delay queues to postpone the delivery of new messages to the queue for a few seconds</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An IT company is using SQS queues for decoupling the various components of application architecture. As the consuming components need additional time to process SQS messages, the company wants to postpone the delivery of new messages to the queue for a few seconds. As a solutions architect, which of the following solutions would you suggest to the company?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773042,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Provisioned IOPS SSD (io1)\"</p>\n\n<p>Provisioned IOPS SSD (io1) is backed by solid-state drives (SSDs) and is a high-performance EBS storage option designed for critical, I/O intensive database and application workloads, as well as throughput-intensive database workloads. io1 is designed to deliver a consistent baseline performance of up to 50 IOPS/GB to a maximum of 64,000 IOPS and provide up to 1,000 MB/s of throughput per volume. Therefore, the io1 volume type would be able to meet the requirement of 25,000 IOPS per volume for the given use-case.</p>\n\n<p>Incorrect options:\n\"General Purpose SSD (gp2)\" - gp2 is backed by solid-state drives (SSDs) and is suitable for a broad range of transactional workloads, including dev/test environments, low-latency interactive applications, and boot volumes. It supports max IOPS/Volume of 16,000.</p>\n\n<p>\"Cold HDD (sc1)\" - sc1 is backed by hard disk drives (HDDs). It is ideal for less frequently accessed workloads with large, cold datasets. It supports max IOPS/Volume of 250.</p>\n\n<p>\"Throughput Optimized HDD (st1)\" - st1 is backed by hard disk drives (HDDs) and is ideal for frequently accessed, throughput-intensive workloads with large datasets and large I/O sizes, such as MapReduce, Kafka, log processing, data warehouse, and ETL workloads. It supports max IOPS/Volume of 500.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/ebs/volume-types/</p>\n",
			"question": "<p>A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following EBS volume types would you recommend for this use-case?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>General Purpose SSD (gp2)</p>",
				"<p>Cold HDD (sc1)</p>",
				"<p>Provisioned IOPS SSD (io1)</p>",
				"<p>Throughput Optimized HDD (st1)</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A big data analytics company is working on a real-time vehicle tracking solution. The data processing workflow involves both I/O intensive and throughput intensive database workloads. The development team needs to store this real-time data in a NoSQL database hosted on an EC2 instance and needs to support up to 25,000 IOPS per volume. As a solutions architect, which of the following EBS volume types would you recommend for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773048,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"AWS Managed Microsoft AD\"</p>\n\n<p>AWS Directory Service provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory (AD) with other AWS services.\nAWS Directory Service for Microsoft Active Directory (aka AWS Managed Microsoft AD) is powered by an actual Microsoft Windows Server Active Directory (AD), managed by AWS. With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud such as SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).</p>\n\n<p>Incorrect options:\n\"AD Connector\" - Use AD Connector if you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. AD Connector simply connects your existing on-premises Active Directory to AWS. You cannot use it to run directory-aware workloads on AWS, hence this option is not correct.</p>\n\n<p>\"Simple AD\" - Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. Simple AD is a standalone managed directory that is powered by a Samba 4 Active Directory Compatible Server.  Simple AD does not support features such as trust relationships with other domains. Therefore, this option is not correct.</p>\n\n<p>\"Amazon Cloud Directory\" - Amazon Cloud Directory is a cloud-native directory that can store hundreds of millions of application-specific objects with multiple relationships and schemas. Use Amazon Cloud Directory if you need a highly scalable directory store for your application’s hierarchical data. You cannot use it to\nestablish trust relationships with other domains on the on-premises infrastructure. Therefore, this option is not correct.</p>\n\n<p>Exam Alert:\nYou may see questions on choosing \"AWS Managed Microsoft AD\" vs \"AD Connector\" vs \"Simple AD\" on the exam. Just remember that you should use AD Connector if you only need to allow your on-premises users to log in to AWS applications with their Active Directory credentials. AWS Managed Microsoft AD would also allow you to run directory-aware workloads in the AWS Cloud.  AWS Managed Microsoft AD is your best choice if you have more than 5,000 users and need a trust relationship set up between an AWS hosted directory and your on-premises directories. Simple AD is the least expensive option and your best choice if you have 5,000 or fewer users and don’t need the more advanced Microsoft Active Directory features such as trust relationships with other domains.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html</p>\n",
			"question": "<p>An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>AD Connector</p>",
				"<p>AWS Managed Microsoft AD</p>",
				"<p>Simple AD</p>",
				"<p>Amazon Cloud Directory</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An e-commerce company uses Microsoft Active Directory to provide users and groups with access to resources on the on-premises infrastructure. The company has extended its IT infrastructure to AWS in the form of a hybrid cloud. The engineering team at the company wants to run directory-aware workloads on AWS for a SQL Server-based application. The team also wants to configure a trust relationship to enable single sign-on (SSO) for its users to access resources in either domain. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772936,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\"</p>\n\n<p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”.</p>\n\n<p>Incorrect options:\n\"Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\" - AWS CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. You cannot use CloudWatch to maintain a history of resource configuration changes.</p>\n\n<p>\"Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\" - With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides an event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to maintain a history of resource configuration changes.</p>\n\n<p>\"Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes\" - Using AWS Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. You cannot use Systems Manager to maintain a history of resource configuration changes.</p>\n\n<p>Exam Alert:\nYou may see scenario-based questions asking you to select one of CloudWatch vs CloudTrail vs Config. Just remember this thumb rule -\nThink resource performance monitoring, events, and alerts; think CloudWatch.\nThink account-specific activity and audit; think CloudTrail.\nThink resource-specific history, audit, and compliance; think Config.</p>\n\n<p>References:\nhttps://aws.amazon.com/config/\nhttps://aws.amazon.com/cloudwatch/\nhttps://aws.amazon.com/cloudtrail/\nhttps://aws.amazon.com/systems-manager/</p>\n",
			"question": "<p>A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?</p>\n",
			"answers": [
				"<p>Use AWS Config to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
				"<p>Use Amazon CloudWatch to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
				"<p>Use AWS CloudTrail to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>",
				"<p>Use AWS Systems Manager to review resource configurations to meet compliance guidelines and maintain a history of resource configuration changes</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A financial services company has recently migrated from on-premises infrastructure to AWS Cloud. The DevOps team wants to implement a solution that allows all resource configurations to be reviewed and make sure that they meet compliance guidelines. Also, the solution should be able to offer the capability to look into the resource configuration history across the application stack. As a solutions architect, which of the following solutions would you recommend to the team?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772972,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Schedule a weekly CloudWatch event cron expression to invoke a Lambda function that runs the database rollover job\"</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. AWS Lambda supports standard rate and cron expressions for frequencies of up to once per minute.</p>\n\n<p>Schedule expressions using rate or cron:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q30-i1.jpg\"></p>\n\n<p>Incorrect options:\n\"Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script\" - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing and it's not the right fit for running a database rollover script. Although AWS Glue is also serverless, Lambda is a more cost-effective option compared to AWS Glue.</p>\n\n<p>\"Provision an EC2 spot instance to run the database rollover job triggered via an OS-based weekly cron expression\" - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly (up to 90% off the On-Demand price).\nAs the Spot Instance runs whenever capacity is available, there is no guarantee that the weekly job will be executed during the defined time window. Additionally, the given use-case requires a serverless solution, therefore this option is incorrect.</p>\n\n<p>\"Provision an EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression\" - Scheduled Reserved Instances run on a part-time basis. Scheduled Reserved Instances option allows you to use reserve capacity on a recurring daily, weekly, and monthly schedules. Scheduled Reserved Instances are available for one-year terms at 5-10% below On-Demand rates. As the given use-case requires a serverless solution, therefore this option is incorrect.</p>\n\n<p>References:\nhttps://aws.amazon.com/lambda/\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents-expressions.html</p>\n",
			"question": "<p>A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Create a time-based schedule option within an AWS Glue job to invoke itself every week and run the database rollover script</p>",
				"<p>Schedule a weekly CloudWatch event cron expression to invoke a Lambda function that runs the database rollover job</p>",
				"<p>Provision an EC2 spot instance to run the database rollover script to be run via an OS-based weekly cron expression</p>",
				"<p>Provision an EC2 scheduled reserved instance to run the database rollover script to be run via an OS-based weekly cron expression</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A social media startup uses AWS Cloud to manage its IT infrastructure. The engineering team at the startup wants to perform weekly database rollovers for a MySQL database server using a serverless cron job that typically takes about 5 minutes to execute the database rollover script written in Python. The database rollover will archive the past week’s data from the production database to keep the database small while still keeping its data accessible. As a solutions architect, which of the following would you recommend as the MOST cost-efficient and reliable solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772998,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\"</p>\n\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access.\nWith cached volumes, the AWS Volume Gateway stores the full volume in its Amazon S3 service bucket, and just the recently accessed data is retained in the gateway’s local cache for low-latency access.</p>\n\n<p>Incorrect options:\n\"Use AWS direct connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket\" - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Direct connect cannot be used to store the most frequently accessed logs locally for low-latency access.</p>\n\n<p>\"Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket\" - With stored volumes, your entire data volume is available locally in the gateway, for fast read access. Volume Gateway also maintains an asynchronous copy of your stored volume in the service’s Amazon S3 bucket. This does not fit the requirements per the given use-case, hence this option is not correct.</p>\n\n<p>\"Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket\" - You can use Snowball Edge Storage Optimized device to securely and quickly transfer dozens of terabytes to petabytes of data to AWS. Snowball Edge Storage Optimized device cannot be used to store the most frequently accessed logs locally for low-latency access.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/storagegateway/volume/</p>\n",
			"question": "<p>A leading interactive entertainment company for the mobile world has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use AWS direct connect to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket</p>",
				"<p>Use AWS Volume Gateway - Stored Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket</p>",
				"<p>Use AWS Snowball Edge Storage Optimized device to store the most frequently accessed logs locally for low-latency access while storing the full backup of logs in an Amazon S3 bucket</p>",
				"<p>Use AWS Volume Gateway - Cached Volume - to store the most frequently accessed logs locally for low-latency access while storing the full volume with all logs in its Amazon S3 service bucket</p>"
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A leading interactive entertainment company for the mobile world has a hybrid cloud structure for its on-premises data center and AWS Cloud infrastructure. The company wants to build a web log archival solution such that only the most frequently accessed logs are available as cached data locally while backing up all logs on Amazon S3. As a solutions architect, which of the following solutions would you recommend for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772992,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"DynamoDB\"</p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. Companies use caching through DynamoDB Accelerator (DAX) when they have high read volumes or need submillisecond read latency.</p>\n\n<p>Incorrect options:\n\"DocumentDB\" - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. Although DocumentDB is fully managed, it does not have an in-memory caching layer.</p>\n\n<p>\"ElastiCache\" - Amazon ElastiCache allows you to set up popular open-Source compatible in-memory data stores in the cloud. You can build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores such as Redis and Memcached. Elasticache is used as a caching layer. It's not a fully managed NoSQL database.</p>\n\n<p>\"RDS\" - RDS makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. It's not a NoSQL database.</p>\n\n<p>References:\nhttps://aws.amazon.com/dynamodb/</p>\n",
			"question": "<p>A media streaming company is looking to migrate its on-premises infrastructure into the AWS Cloud. The engineering team is looking for a fully managed NoSQL database solution with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team expects the number of concurrent users to touch up to a million so the database should be able to scale elastically. As a solutions architect, which of the following AWS services would you recommend for this use-case?</p>\n",
			"answers": [
				"<p>DynamoDB</p>",
				"<p>DocumentDB</p>",
				"<p>ElastiCache</p>",
				"<p>RDS</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A media streaming company is looking to migrate its on-premises infrastructure into the AWS Cloud. The engineering team is looking for a fully managed NoSQL database solution with in-memory caching to maintain low latency that is critical for real-time scenarios such as video streaming and interactive content. The team expects the number of concurrent users to touch up to a million so the database should be able to scale elastically. As a solutions architect, which of the following AWS services would you recommend for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773020,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"AWS Step Functions\"</p>\n\n<p>AWS Step Functions lets you coordinate and orchestrate multiple AWS services such as AWS Lambda and AWS Glue into serverless workflows. Workflows are made up of a series of steps, with the output of one step acting as input into the next. A Step Function automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected. The Step Function can ensure that the Glue ETL job and the lambda functions execute in order and complete successfully as per the workflow defined in the given use-case. Therefore, Step Function is the best solution.</p>\n\n<p>How Step Functions Work\n<img src=\"https://d1.awsstatic.com/product-marketing/Step%20Functions/sfn_how-it-works.f795601e8338db32506b9abb01e71704f483fc81.png\">\nvia - https://aws.amazon.com/step-functions/</p>\n\n<p>Incorrect options:\n\"AWS Batch\" - AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. AWS Batch cannot be used to orchestrate a workflow, hence it is an incorrect option.</p>\n\n<p>\"Amazon Simple Workflow Service (SWF)\" - Amazon SWF helps developers build, run, and scale background jobs that have parallel or sequential steps. In Amazon SWF, tasks represent invocations of logical steps in applications. Tasks are processed by workers which are programs that interact with Amazon SWF to get tasks, process them, and return their results. To coordinate the application execution across workers, you write a program called the decider in your choice of programming language. Although Amazon SWF provides you complete control over your orchestration logic, it increases the complexity of developing applications. Hence this option is not correct.</p>\n\n<p>Exam Alert:\nPlease understand the differences between Amazon SWF vs. AWS Step Functions</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q33-i1.jpg\">\nvia - https://aws.amazon.com/swf/faqs/</p>\n\n<p>\"Amazon EMR\" - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. EMR cannot be used to orchestrate a workflow, hence it is an incorrect option.</p>\n\n<p>References:\nhttps://aws.amazon.com/step-functions/\nhttps://aws.amazon.com/swf/faqs/</p>\n",
			"question": "<p>A big data consulting firm needs to set up a data lake on Amazon S3 for a retail client. The data lake is split in raw and refined zones. The source data arrives in the raw zone and is then processed via an AWS Glue based ETL job into the refined zone. The workflow also involves multiple lambda functions that perform data hygiene and data completeness checks in both the raw and refined zones. The firm is looking for an orchestration solution to manage this end to end workflow. As a solutions architect, which of the following AWS services involves the LEAST development effort for this use-case?</p>\n",
			"answers": [
				"<p>AWS Batch</p>",
				"<p>Amazon Simple Workflow Service (SWF)</p>",
				"<p>AWS Step Functions</p>",
				"<p>Amazon EMR</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A big data consulting firm needs to set up a data lake on Amazon S3 for a retail client. The data lake is split in raw and refined zones. The source data arrives in the raw zone and is then processed via an AWS Glue based ETL job into the refined zone. The workflow also involves multiple lambda functions that perform data hygiene and data completeness checks in both the raw and refined zones. The firm is looking for an orchestration solution to manage this end to end workflow. As a solutions architect, which of the following AWS services involves the LEAST development effort for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773036,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>\"Use SQS long polling to retrieve messages from your Amazon SQS queues\"</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\nAmazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nLong polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. Using long polling can reduce the cost of using SQS because you can reduce the number of empty receives.</p>\n\n<p>Short Polling vs Long Polling\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q34-i1.jpg\"></p>\n\n<p>Incorrect options:\n\"Use SQS short polling to retrieve messages from your Amazon SQS queues\" - With short polling, Amazon SQS sends the response right away, even if the query found no messages. You end up paying more because of the increased number of empty receives.</p>\n\n<p>\"Use SQS visibility timeout to retrieve messages from your Amazon SQS queues\" - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p>\"Use SQS message timer to retrieve messages from your Amazon SQS queues\" - You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. You cannot use message timer to retrieve messages from your Amazon SQS queues. This option has been added as a distractor.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html\nhttps://aws.amazon.com/sqs/faqs/</p>\n",
			"question": "<p>A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS. As a solutions architect, which of the following options would you recommend for the given use-case?</p>\n",
			"answers": [
				"<p>Use SQS short polling to retrieve messages from your Amazon SQS queues</p>",
				"<p>Use SQS visibility timeout to retrieve messages from your Amazon SQS queues</p>",
				"<p>Use SQS long polling to retrieve messages from your Amazon SQS queues</p>",
				"<p>Use SQS message timer to retrieve messages from your Amazon SQS queues</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A financial services company is migrating their messaging queues from self-managed message-oriented middleware systems to Amazon SQS. The development team at the company wants to minimize the costs of using SQS. As a solutions architect, which of the following options would you recommend for the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772946,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint\"\n\"Create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\"</p>\n\n<p>Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. Amazon Route 53 effectively connects user requests to infrastructure running in AWS – such as Amazon EC2 instances – and can also be used to route users to infrastructure outside of AWS. By default, Route 53 Resolver automatically answers DNS queries for local VPC domain names for EC2 instances. You can integrate DNS resolution between Resolver and DNS resolvers on your on-premises network by configuring forwarding rules.</p>\n\n<p>To resolve any DNS queries for resources in the AWS VPC from the on-premises network, you can create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint.</p>\n\n<p>Resolver Inbound Endpoint\n<img src=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/images/Resolver-inbound-endpoint.png\">\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html</p>\n\n<p>To resolve DNS queries for any resources in the on-premises network from the AWS VPC, you can create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. To conditionally forward queries, you need to create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com) and the IP addresses of the DNS resolvers on the on-premises network that you want to forward the queries to.</p>\n\n<p>Resolver Outbound Endpoint\n<img src=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/images/Resolver-outbound-endpoint.png\">\nvia - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html</p>\n\n<p>Incorrect options:\n\"Create an outbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint\" - DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via an inbound endpoint. Hence, this option is incorrect.</p>\n\n<p>\"Create an inbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint\" - Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via an outbound endpoint. Hence, this option is incorrect.</p>\n\n<p>\"Create a universal endpoint on Route 53 Resolver and then Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint\" - There is no such thing as a universal endpoint on Route 53 Resolver. This option has been added as a distractor.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-getting-started.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver.html</p>\n",
			"question": "<p>A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve DNS queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions would you recommend for this use-case? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Create an outbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint</p>",
				"<p>Create an inbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint</p>",
				"<p>Create a universal endpoint on Route 53 Resolver and then Route 53 Resolver can receive and forward queries to resolvers on the on-premises network via this endpoint</p>",
				"<p>Create an inbound endpoint on Route 53 Resolver and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint</p>",
				"<p>Create an outbound endpoint on Route 53 Resolver and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint</p>"
			]
		},
		"correct_response": [
			"d",
			"e"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail company has connected its on-premises data center to the AWS Cloud via AWS Direct Connect. The company wants to be able to resolve DNS queries for any resources in the on-premises network from the AWS VPC and also resolve any DNS queries for resources in the AWS VPC from the on-premises network. As a solutions architect, which of the following solutions would you recommend for this use-case? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772922,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions\"</p>\n\n<p>AWS CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. A stack set lets you create stacks in AWS accounts across regions by using a single AWS CloudFormation template. Using an administrator account of an \"AWS Organization\", you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts of an \"AWS Organization\" across specified regions.</p>\n\n<p>AWS CloudFormation StackSets\n<img src=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/images/stack_set_conceptual_sv.png\"></p>\n\n<p>Incorrect options:\n\"Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions\" - Cloudformation template is a JSON or YAML-format, text-based file that describes all the AWS resources you need to deploy to run your application. A template acts as a blueprint for a stack. CloudFormation templates cannot be used to deploy the same template across AWS accounts and regions.\"</p>\n\n<p>\"Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions\" - CloudFormation stack is a set of AWS resources that are created and managed as a single unit when AWS CloudFormation instantiates a template. A stack cannot be used to deploy the same template across AWS accounts and regions.</p>\n\n<p>\"Use AWS Resource Access Manager (RAM) to deploy the same template across AWS accounts and regions\" -  AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization. Resource Access Manager cannot be used to deploy the same template across AWS accounts and regions.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-whatis-howdoesitwork.html</p>\n",
			"question": "<p>A multi-national company uses AWS Cloud to manage its IT infrastructure. The company has set up \"AWS Organizations\" to manage several departments running their AWS accounts. The departments operate from different countries and are spread across various AWS regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of EC2 instances, specific IAM roles for Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?</p>\n",
			"answers": [
				"<p>Use AWS CloudFormation StackSets to deploy the same template across AWS accounts and regions</p>",
				"<p>Use AWS CloudFormation templates to deploy the same template across AWS accounts and regions</p>",
				"<p>Use AWS CloudFormation stacks to deploy the same template across AWS accounts and regions</p>",
				"<p>Use AWS Resource Access Manager (RAM) to deploy the same template across AWS accounts and regions</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A multi-national company uses AWS Cloud to manage its IT infrastructure. The company has set up \"AWS Organizations\" to manage several departments running their AWS accounts. The departments operate from different countries and are spread across various AWS regions. The company wants to set up a consistent resource provisioning process across departments so that each resource follows pre-defined configurations such as using a specific type of EC2 instances, specific IAM roles for Lambda functions, etc. As a solutions architect, which of the following options would you recommend for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772994,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use Elasticache for distributed cache-based session management\"</p>\n\n<p>Amazon ElastiCache can be used as a distributed in-memory cache for session management. Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Session stores can be set up using both Memcached or Redis for ElastiCache.\nAmazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store\nAmazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Session stores are easy to create with Amazon ElastiCache for Memcached.</p>\n\n<p>Incorrect options:\n\"Use RDS for distributed in-memory cache-based session management\" - Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It cannot be used as a distributed in-memory cache for session management, hence this option is incorrect.</p>\n\n<p>\"Use DynamoDB for distributed in-memory cache-based session management\" - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB is a NoSQL database and is not the right fit for a distributed in-memory cache-based session management solution.</p>\n\n<p>\"Use Application Load Balancer sticky sessions\" - Although sticky sessions enable each user to interact with one server and one server only, however, in case of an unhealthy server, all the session data is gone as well. Therefore Elasticache powered distributed in-memory cache-based session management is a better solution.</p>\n\n<p>References:\nhttps://aws.amazon.com/getting-started/hands-on/building-fast-session-caching-with-amazon-elasticache-for-redis/\nhttps://aws.amazon.com/elasticache/</p>\n",
			"question": "<p>A CRM application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use RDS for distributed in-memory cache based session management</p>",
				"<p>Use Elasticache for distributed in-memory cache based session management</p>",
				"<p>Use Application Load Balancer sticky sessions</p>",
				"<p>Use DynamoDB for distributed in-memory cache based session management</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A CRM application is facing user experience issues with users reporting frequent sign-in requests from the application. The application is currently hosted on multiple EC2 instances behind an Application Load Balancer. The engineering team has identified the root cause as unhealthy servers causing session data to be lost. The team would like to implement a distributed in-memory cache-based session management solution. As a solutions architect, which of the following solutions would you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772942,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use Enhanced Fanout feature of Kinesis Data Streams\"</p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. You should use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel. With enhanced fan-out developers can register stream consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard, and this throughput automatically scales with the number of shards in a stream.</p>\n\n<p>Kinesis Data Streams Fanout\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q38-i1.jpg\">\nvia - https://aws.amazon.com/blogs/aws/kds-enhanced-fanout/</p>\n\n<p>Incorrect options:\n\"Swap out Kinesis Data Streams with Kinesis Data Firehose\" -  Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics tools. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Kinesis Data Firehose can only write to S3, Redshift, Elasticsearch or Splunk. You can't have applications consuming data streams from Kinesis Data Firehose, that's the job of Kinesis Data Streams. Therefore this option is not correct.</p>\n\n<p>\"Swap out Kinesis Data Streams with SQS Standard queues\"\n\"Swap out Kinesis Data Streams with SQS FIFO queues\"\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent. As multiple applications are consuming the same stream concurrently, both SQS Standard and SQS FIFO are not the right fit for the given use-case.</p>\n\n<p>Exam Alert:\nPlease understand the differences between the capabilities of Kinesis Data Streams vs SQS, as you may be asked scenario-based questions on this topic in the exam.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q38-i2.jpg\">\nvia - https://aws.amazon.com/kinesis/data-streams/faqs/</p>\n\n<p>References:\nhttps://aws.amazon.com/blogs/aws/kds-enhanced-fanout/\nhttps://aws.amazon.com/kinesis/data-streams/faqs/</p>\n",
			"question": "<p>A big data analytics company is using Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend improving the performance for the given use-case?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Swap out Kinesis Data Streams with SQS Standard queues</p>",
				"<p>Swap out Kinesis Data Streams with SQS FIFO queues</p>",
				"<p>Use Enhanced Fanout feature of Kinesis Data Streams</p>",
				"<p>Swap out Kinesis Data Streams with Kinesis Data Firehose</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A big data analytics company is using Kinesis Data Streams (KDS) to process IoT data from the field devices of an agricultural sciences company. Multiple consumer applications are using the incoming data streams and the engineers have noticed a performance lag for the data delivery speed between producers and consumers of the data streams. As a solutions architect, which of the following would you recommend improving the performance for the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772962,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls\"</p>\n\n<p>Amazon S3 Glacier is a secure, durable, and extremely low-cost Amazon S3 cloud storage class for data archiving and long-term backup. It is designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>An S3 Glacier vault is a container for storing archives. When you create a vault, you specify a vault name and the AWS Region in which you want to create the vault. S3 Glacier Vault Lock allows you to easily deploy and enforce compliance controls for individual S3 Glacier vaults with a vault lock policy. You can specify controls such as “write once read many” (WORM) in a vault lock policy and lock the policy from future edits. Therefore, this is the correct option.</p>\n\n<p>Incorrect options:\n\"Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls\" - You can use lifecycle policy to define actions you want Amazon S3 to take during an object's lifetime. For example, use a lifecycle policy to transition objects to another storage class, archive them, or delete them after a specified period. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>\"Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls\" - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>\"Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls\" - Amazon S3 access control lists (ACLs) enable you to manage access to buckets and objects. It cannot be used to enforce compliance controls. Therefore, this option is incorrect.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/working-with-vaults.html\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/vault-lock.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html</p>\n",
			"question": "<p>A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon S3. The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use S3 Glacier vault to store the sensitive archived data and then use a vault lock policy to enforce compliance controls</p>",
				"<p>Use S3 Glacier to store the sensitive archived data and then use an S3 lifecycle policy to enforce compliance controls</p>",
				"<p>Use S3 Glacier vault to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>",
				"<p>Use S3 Glacier to store the sensitive archived data and then use an S3 Access Control List to enforce compliance controls</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A silicon valley based healthcare startup uses AWS Cloud for its IT infrastructure. The startup stores patient health records on Amazon S3. The engineering team needs to implement an archival solution based on Amazon S3 Glacier to enforce regulatory and compliance controls on data access. As a solutions architect, which of the following solutions would you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772920,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Data at rest inside the volume is encrypted\"\n\"Any snapshot created from the volume is encrypted\"\n\"Data moving between the volume and the instance is encrypted\"</p>\n\n<p>Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. When you create an encrypted EBS volume and attach it to a supported instance type, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume and volumes created from those snapshots are all encrypted. It uses AWS Key Management Service (AWS KMS) customer master keys (CMK) when creating encrypted volumes and snapshots. Encryption operations occur on the servers that host EC2 instances, ensuring the security of both data-at-rest and data-in-transit between an instance and its attached EBS storage.</p>\n\n<p>Therefore, the incorrect options are:\n\"Data moving between the volume and the instance is NOT encrypted\"\n\"Any snapshot created from the volume is NOT encrypted\"\n\"Data at rest inside the volume is NOT encrypted\"</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Data at rest inside the volume is encrypted</p>",
				"<p>Data moving between the volume and the instance is NOT encrypted</p>",
				"<p>Any snapshot created from the volume is encrypted</p>",
				"<p>Any snapshot created from the volume is NOT encrypted</p>",
				"<p>Data moving between the volume and the instance is encrypted</p>",
				"<p>Data at rest inside the volume is NOT encrypted</p>"
			],
			"question": "<p>A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on EC2 instances with storage on EBS volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on EBS. Which of the following options outline the correct capabilities of an encrypted EBS volume? (Select three)</p>\n"
		},
		"correct_response": [
			"a",
			"c",
			"e"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A pharmaceutical company is considering moving to AWS Cloud to accelerate the research and development process. Most of the daily workflows would be centered around running batch jobs on EC2 instances with storage on EBS volumes. The CTO is concerned about meeting HIPAA compliance norms for sensitive data stored on EBS. Which of the following options outline the correct capabilities of an encrypted EBS volume? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773044,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Use Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud\"</p>\n\n<p>AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office site to your Amazon Virtual Private Cloud (Amazon VPC). You can securely extend your data center or branch office network to the cloud with an AWS Site-to-Site VPN connection. A VPC VPN Connection utilizes IPSec to establish encrypted network connectivity between your on-premises network and Amazon VPC over the Internet. IPsec is a protocol suite for securing IP communications by authenticating and encrypting each IP packet in a data stream.</p>\n\n<p>Incorrect options:\n\"Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud\" - AWS Direct Connect lets you establish a dedicated network connection between your network and one of the AWS Direct Connect locations. Using industry-standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. AWS Direct Connect does not encrypt your traffic that is in transit. To encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service. As AWS Direct Connect does not support encrypted network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p>\"Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud\" - AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS. DataSync eliminates or automatically handles many of these tasks, including scripting copy jobs, scheduling, and monitoring transfers, validating data, and optimizing network utilization. As AWS Data Sync cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p>\"Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud\" - AWS Secrets Manager helps you protect secrets needed to access your applications, services, and IT resources. The service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. As AWS Secrets Manager cannot be used to establish network connectivity between an on-premises data center and AWS Cloud, therefore this option is incorrect.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/internetwork-traffic-privacy.html\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html</p>\n",
			"question": "<p>A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use AWS Direct Connect to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
				"<p>Use AWS Data Sync to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
				"<p>Use AWS Secrets Manager to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>",
				"<p>Use Site-to-Site VPN to establish encrypted network connectivity between the on-premises data center and AWS Cloud</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail company wants to establish encrypted network connectivity between its on-premises data center and AWS Cloud. The company wants to get the solution up and running in the fastest possible time and it should also support encryption in transit. As a solutions architect, which of the following solutions would you suggest to the company?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772964,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Amazon EMR\"</p>\n\n<p>You can use Amazon Kinesis Data Firehose to load streaming data into data lakes, data stores, and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk.</p>\n\n<p>How Kinesis Data Firehose works\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Firehose.9340b812ab86518341c47b24316995b3792bf6e1.png\">\nvia - https://aws.amazon.com/kinesis/data-firehose/</p>\n\n<p>Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. Firehose does not support Amazon EMR as a target for delivering the streaming data.</p>\n\n<p>Incorrect options:\n\"Amazon S3\" - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3.</p>\n\n<p>\"Amazon RedShift\" - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis.</p>\n\n<p>\"Amazon Elasticsearch\" - Amazon Elasticsearch Service is a fully managed service that makes it easy for you to deploy, secure, and run Elasticsearch cost-effectively at scale. Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.</p>\n\n<p>As mentioned, Firehose can deliver streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/kinesis/data-firehose/</p>\n",
			"question": "<p>The data science team at a mobility company wants to analyze real-time location data of rides. The company is using Kinesis Data Firehose for delivering the location-specific streaming data into targets for downstream analytics. Which of the following targets are NOT supported by Kinesis Data Firehose?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Amazon EMR</p>",
				"<p>Amazon S3</p>",
				"<p>Amazon RedShift</p>",
				"<p>Amazon Elasticsearch</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The data science team at a mobility company wants to analyze real-time location data of rides. The company is using Kinesis Data Firehose for delivering the location-specific streaming data into targets for downstream analytics. Which of the following targets are NOT supported by Kinesis Data Firehose?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773018,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use EC2 instances with Instance Store as the storage type\"</p>\n\n<p>An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers. Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.\nAs Instance Store delivers high random I/O performance, it can act as a temporary storage space, and these volumes are included as part of the instance's usage cost, therefore this is the correct option.</p>\n\n<p>Amazon EC2 Instance Store\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/instance_storage.png\">\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</p>\n\n<p>Incorrect options:\n\"Use EC2 instances with EBS General Purpose SSD (gp2) as the storage option\" - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver its provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.\nEBS gp2 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the EC2 instance), therefore this option is not correct.</p>\n\n<p>\"Use EC2 instances with EBS Provisioned IOPS SSD (io1) as the storage option\" - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.\nEBS io1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the EC2 instance), therefore this option is not correct.</p>\n\n<p>\"Use EC2 instances with EBS Throughput Optimized HDD (st1) as the storage option\" - Throughput Optimized HDD (st1) are low-cost HDD volumes designed for frequently accessed, throughput-intensive workloads such as Big data and Data warehouses. EBS st1 is persistent storage and costlier than Instance Stores (the cost of the storage volume is in addition to that of the EC2 instance), therefore this option is not correct.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</p>\n",
			"question": "<p>Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?</p>\n",
			"answers": [
				"<p>Use EC2 instances with Instance Store as the storage option</p>",
				"<p>Use EC2 instances with EBS General Purpose SSD (gp2) as the storage option</p>",
				"<p>Use EC2 instances with EBS Provisioned IOPS SSD (io1) as the storage option</p>",
				"<p>Use EC2 instances with EBS Throughput Optimized HDD (st1) as the storage option</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "Computer vision researchers at a university are trying to optimize the I/O bound processes for a proprietary algorithm running on EC2 instances. The ideal storage would facilitate high-performance IOPS when doing file processing in a temporary storage space before uploading the results back into Amazon S3. As a solutions architect, which of the following AWS storage options would you recommend as the MOST performant as well as cost-optimal?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773000,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use a dead-letter queue to handle message processing failures\"</p>\n\n<p>Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed.\nSometimes, messages can’t be processed because of a variety of possible issues, such as when a user comments on a story but it remains unprocessed because the original story itself is deleted by the author while the comments were being posted. In such a case, the dead-letter queue can be used to handle message processing failures.</p>\n\n<p>How do dead-letter queues work?\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q44-i1.jpg\">\nvia - https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</p>\n\n<p>Incorrect options:\n\"Use a temporary queue to handle message processing failures\" - The most common use case for temporary queues is the request-response messaging pattern (for example, processing a login request), where a requester creates a temporary queue for receiving each response message. To avoid creating an Amazon SQS queue for each response message, the Temporary Queue Client lets you create and delete multiple temporary queues without making any Amazon SQS API calls. Temporary queues cannot be used to handle message processing failures.</p>\n\n<p>\"Use short polling to handle message processing failures\"\n\"Use long polling to handle message processing failures\"\nAmazon SQS provides short polling and long polling to receive messages from a queue. By default, queues use short polling. With short polling, Amazon SQS sends the response right away, even if the query found no messages. With long polling, Amazon SQS sends a response after it collects at least one available message, up to the maximum number of messages specified in the request. Amazon SQS sends an empty response only if the polling wait time expires.\nNeither short polling nor long polling can be used to handle message processing failures.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use a temporary queue to handle message processing failures</p>",
				"<p>Use a dead-letter queue to handle message processing failures</p>",
				"<p>Use short polling to handle message processing failures</p>",
				"<p>Use long polling to handle message processing failures</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A social media company uses Amazon SQS queues to decouple their application architecture. The engineering team has observed message processing failures for an edge case scenario where comments posted to a story remain unprocessed if the original story itself is deleted by the author while the comments were being posted. As a solutions architect, which of the following solutions would you recommend for handling such message failures?</p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A social media company uses Amazon SQS queues to decouple their application architecture. The engineering team has observed message processing failures for an edge case scenario where comments posted to a story remain unprocessed if the original story itself is deleted by the author while the comments were being posted. As a solutions architect, which of the following solutions would you recommend for handling such message failures?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773014,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Use WAF geo match statement listing the countries that you want to block\"\n\"Use WAF IP set statement that specifies the IP addresses that you want to allow through\"</p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns and rules that filter out specific traffic patterns you define.\nYou can deploy AWS WAF on Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts your web servers or origin servers running on EC2, or Amazon API Gateway for your APIs.</p>\n\n<p>AWS WAF - How it Works\n<img src=\"https://d1.awsstatic.com/products/WAF/product-page-diagram_AWS-WAF_How-it-Works@2x.452efa12b06cb5c87f07550286a771e20ca430b9.png\">\nvia - https://aws.amazon.com/waf/</p>\n\n<p>To block specific countries, you can create a WAF geo match statement listing the countries that you want to block, and to allow traffic from IPs of the remote development team, you can create a WAF IP set statement that specifies the IP addresses that you want to allow through. You can combine the two rules as shown below:</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q45-i1.jpg\">\nvia - https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html</p>\n\n<p>Incorrect options:\n\"Create a deny rule for the blocked countries in the NACL associated to each of the EC2 instances\" - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. NACL does not have the capability to block traffic based on geographic match conditions.</p>\n\n<p>\"Use ALB geo match statement listing the countries that you want to block\"\n\"Use ALB IP set statement that specifies the IP addresses that you want to allow through\"\nAn Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.\nAn ALB cannot block or allow traffic based on geographic match conditions or IP based conditions. Both these options have been added as distractors.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-geo-match.html\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-waf-to-filter-incoming-traffic-from-embargoed-countries/</p>\n",
			"question": "<p>An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF. As a solutions architect, which of the following solutions would you suggest as the BEST fit for the given use-case? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Create a deny rule for the blocked countries in the NACL associated with each of the EC2 instances</p>",
				"<p>Use ALB geo match statement listing the countries that you want to block</p>",
				"<p>Use ALB IP set statement that specifies the IP addresses that you want to allow through</p>",
				"<p>Use WAF geo match statement listing the countries that you want to block</p>",
				"<p>Use WAF IP set statement that specifies the IP addresses that you want to allow through</p>"
			]
		},
		"correct_response": [
			"d",
			"e"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An online gaming company wants to block access to its application from specific countries; however, the company wants to allow its remote development team (from one of the blocked countries) to have access to the application. The application is deployed on EC2 instances running under an Application Load Balancer (ALB) with AWS WAF. As a solutions architect, which of the following solutions would you suggest as the BEST fit for the given use-case? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772954,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"S3 can encrypt object metadata by using Server-Side Encryption\"</p>\n\n<p>Amazon S3 is a simple key-value store designed to store as many objects as you want. You store these objects in one or more buckets, and each object can be up to 5 TB in size.\nAn object consists of the following:\nKey – The name that you assign to an object. You use the object key to retrieve the object.\nVersion ID – Within a bucket, a key and version ID uniquely identify an object.\nValue – The content that you are storing.\nMetadata – A set of name-value pairs with which you can store information regarding the object.\nSubresources – Amazon S3 uses the subresource mechanism to store object-specific additional information.\nAccess Control Information – You can control access to the objects you store in Amazon S3.</p>\n\n<p>Metadata, which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata.</p>\n\n<p>Incorrect options:\n\"S3 can protect data at rest using Server-Side Encryption\"  - This is possible and AWS provides three different ways of doing this - Server-side encryption with Amazon S3‐managed keys (SSE-S3), Server-side encryption with customer master keys stored in AWS Key Management Service (SSE-KMS), Server-side encryption with customer-provided keys (SSE-C).</p>\n\n<p>\"S3 can protect data at rest using Client-Side Encryption\" - This is a possible scenario too. You can encrypt data on the client-side and upload the encrypted data to Amazon S3. In this case, the client manages the encryption process, the encryption keys, and related tools.</p>\n\n<p>\"S3 can encrypt data in transit using HTTPS (TLS)\" - This is also possible and you can use HTTPS (TLS) to help prevent potential attackers from eavesdropping on or manipulating network traffic using person-in-the-middle or similar attacks.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/security-best-practices.html#server-side\nhttps://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf?did=wp_card&amp;trk=wp_card</p>\n",
			"question": "<p>A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon S3 to meet compliance guidelines. The engineering team has hired you as a solutions architect to help them understand the encryption capabilities on S3. Can you help the team identify the INCORRECT option from the choices below?</p>\n",
			"answers": [
				"<p>S3 can protect data at rest using Server-Side Encryption</p>",
				"<p>S3 can protect data at rest using Client-Side Encryption</p>",
				"<p>S3 can encrypt data in transit using HTTPS (TLS)</p>",
				"<p>S3 can encrypt object metadata by using Server-Side Encryption</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A financial services company is moving its IT infrastructure to AWS Cloud and wants to enforce adequate data protection mechanisms on Amazon S3 to meet compliance guidelines. The engineering team has hired you as a solutions architect to help them understand the encryption capabilities on S3. Can you help the team identify the INCORRECT option from the choices below?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772980,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"AWS Trusted Advisor\"</p>\n\n<p>AWS Trusted Advisor is an online tool that draws upon best practices learned from AWS’s aggregated operational history of serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment and makes recommendations for saving money, improving system performance, or closing security gaps. It scans your AWS infrastructure and compares it to AWS Best practices in five categories (Cost Optimization, Performance, Security, Fault Tolerance, Service limits) and then provides recommendations.</p>\n\n<p>How Trusted Advisor Works\n<img src=\"https://d1.awsstatic.com/product-marketing/AWS%20Support/AWS-trusted-advisor.5b9909d5f29f680eeb12ccff536e8d88d8701304.png\">\nvia - https://aws.amazon.com/premiumsupport/technology/trusted-advisor/</p>\n\n<p>Incorrect options:\n\"AWS Config\" - AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. You can use Config to answer questions such as - “What did my AWS resource look like at xyz point in time?”. It does not offer any feedback about architectural best practices.</p>\n\n<p>\"AWS Management Console\" - The AWS Management Console is a web application that comprises and refers to a broad collection of service consoles for managing Amazon Web Services. You log into your AWS account using the AWS Management console. It does not offer any feedback about architectural best practices.</p>\n\n<p>\"AWS Systems Manager\" - AWS Systems Manager is an AWS service that you can use to view and control your infrastructure on AWS. Using the Systems Manager console, you can view operational data from multiple AWS services and automate operational tasks across your AWS resources. With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. It does not offer any feedback about architectural best practices.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/</p>\n",
			"question": "<p>A multi-national company is looking at optimizing their AWS resources across various countries and regions. They want to understand the best practices on cost optimization, performance, and security for their system architecture spanning across multiple business units. Which AWS service is the best fit for their requirements? </p>\n",
			"answers": [
				"<p>AWS Config</p>",
				"<p>AWS Trusted Advisor</p>",
				"<p>AWS Management Console</p>",
				"<p>AWS Systems Manager</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A multi-national company is looking at optimizing their AWS resources across various countries and regions. They want to understand the best practices on cost optimization, performance, and security for their system architecture spanning across multiple business units. Which AWS service is the best fit for their requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773040,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications\"</p>\n\n<p>AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data to and from AWS storage services over the internet or AWS Direct Connect.\nAWS DataSync fully automates and accelerates moving large active datasets to AWS, up to 10 times faster than command-line tools. It is natively integrated with Amazon S3, Amazon EFS, Amazon FSx for Windows File Server, Amazon CloudWatch, and AWS CloudTrail, which provides seamless and secure access to your storage services, as well as detailed monitoring of the transfer.\nDataSync uses a purpose-built network protocol and scale-out architecture to transfer data. A single DataSync agent is capable of saturating a 10 Gbps network link.\nDataSync fully automates the data transfer. It comes with retry and network resiliency mechanisms, network optimizations, built-in task scheduling, monitoring via the DataSync API and Console, and CloudWatch metrics, events, and logs that provide granular visibility into the transfer process. DataSync performs data integrity verification both during the transfer and at the end of the transfer.</p>\n\n<p>How DataSync Works\n<img src=\"https://d1.awsstatic.com/cloud-storage/Storage/aws-datasync-how-it-works-diagram-s3-efs-fsx.c26c66393dc4e433369ee9947f39e9c54cd338bb.png\">\nvia - https://aws.amazon.com/datasync/</p>\n\n<p>AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. The service provides three different types of gateways – Tape Gateway, File Gateway, and Volume Gateway – that seamlessly connect on-premises applications to cloud storage, caching data locally for low-latency access. File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.</p>\n\n<p>The combination of DataSync and File Gateway is the correct solution. AWS DataSync enables you to automate and accelerate online data transfers to AWS storage services. File Gateway then provides your on-premises applications with low latency access to the migrated data.</p>\n\n<p>Incorrect options:\n\"Use AWS DataSync to migrate existing data to Amazon S3 as well as access the S3 data for ongoing updates\" - AWS DataSync is used to easily transfer data to and from AWS with up to 10x faster speeds. It is used to transfer data and cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications.</p>\n\n<p>\"Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use S3 Transfer Acceleration for ongoing updates from the on-premises applications\" - File Gateway can be used to move on-premises data to AWS Cloud, but it not an optimal solution for high volumes. Migration services such as DataSync are best suited for this purpose. S3 Transfer Acceleration cannot facilitate ongoing updates to the migrated files from the on-premises applications.</p>\n\n<p>\"Use S3 Transfer Acceleration to migrate existing data to Amazon S3 and then use DataSync for ongoing updates from the on-premises applications\" -  If your application is already integrated with the Amazon S3 API, and you want higher throughput for transferring large files to S3, S3 Transfer Acceleration can be used. However DataSync cannot be used to facilitate ongoing updates to the migrated files from the on-premises applications.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/datasync/features/</p>\n",
			"question": "<p>A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?</p>\n",
			"answers": [
				"<p>Use AWS DataSync to migrate existing data to Amazon S3 as well as access the S3 data for ongoing updates</p>",
				"<p>Use File Gateway configuration of AWS Storage Gateway to migrate data to Amazon S3 and then use S3 Transfer Acceleration for ongoing updates from the on-premises applications</p>",
				"<p>Use S3 Transfer Acceleration to migrate existing data to Amazon S3 and then use DataSync for ongoing updates from the on-premises applications</p>",
				"<p>Use AWS DataSync to migrate existing data to Amazon S3 and then use File Gateway to retain access to the migrated data for ongoing updates from the on-premises applications</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A leading media company wants to do an accelerated online migration of hundreds of terabytes of files from their on-premises data center to Amazon S3 and then establish a mechanism to access the migrated data for ongoing updates from the on-premises applications. As a solutions architect, which of the following would you select as the MOST performant solution for the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773034,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Lightsail\"</p>\n\n<p>Lightsail is an easy-to-use cloud platform that offers you everything needed to build an application or website, plus a cost-effective, monthly plan. Lightsail offers several preconfigured, one-click-to-launch operating systems, development stacks, and web applications, including Linux, Windows OS, and WordPress.\nTherefore, this is the best fit for the given use-case.</p>\n\n<p>Incorrect options:\n\"Lambda\" - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. You cannot set up a Wordpress blog on a Lambda function.</p>\n\n<p>\"EC2 instance\" - An EC2 instance is a virtual server in Amazon's Elastic Compute Cloud (EC2) for running applications on the Amazon Web Services (AWS) infrastructure. EC2 instances come with variable pricing. Additionally, the blog would need to be installed manually on the underlying virtual server. Therefore, this option is not correct.</p>\n\n<p>\"Elastic Beanstalk\" - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nYou can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. As the blogger does not want to deal with code, this option is not correct.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/lightsail/</p>\n",
			"question": "<p>A fashion blogger is looking at starting a Wordpress blog on AWS Cloud. He estimates the blog traffic to be a few thousand users a month, wants the infrastructure fee to be a fixed price per month and the Wordpress blog should be offered as a preconfigured deployment option as he does not want to deal with code. Which AWS service is the BEST fit for the given use-case?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Lightsail</p>",
				"<p>Lambda</p>",
				"<p>EC2 instance</p>",
				"<p>Elastic Beanstalk</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A fashion blogger is looking at starting a Wordpress blog on AWS Cloud. He estimates the blog traffic to be a few thousand users a month, wants the infrastructure fee to be a fixed price per month and the Wordpress blog should be offered as a preconfigured deployment option as he does not want to deal with code. Which AWS service is the BEST fit for the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772952,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use message timers to postpone the delivery of certain messages to the queue by one minute\"</p>\n\n<p>You can use message timers to set an initial invisibility period for a message added to a queue. So, if you send a message with a 60-second timer, the message isn't visible to consumers for its first 60 seconds in the queue. The default (minimum) delay for a message is 0 seconds. The maximum is 15 minutes. Therefore, you should use message timers to postpone the delivery of certain messages to the queue by one minute.</p>\n\n<p>Incorrect option:\n\"Use dead-letter queues to postpone the delivery of certain messages to the queue by one minute\" - Dead-letter queues can be used by other queues (source queues) as a target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. You cannot use dead-letter queues to postpone the delivery of certain messages to the queue by one minute.</p>\n\n<p>\"Use visibility timeout to postpone the delivery of certain messages to the queue by one minute\" - Visibility timeout is a period during which Amazon SQS prevents other consumers from receiving and processing a given message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. You cannot use visibility timeout to postpone the delivery of certain messages to the queue by one minute.</p>\n\n<p>\"Use delay queues to postpone the delivery of certain messages to the queue by one minute\" - Delay queues let you postpone the delivery of all new messages to a queue for several seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes. You cannot use delay queues to postpone the delivery of only certain messages to the queue by one minute.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-timers.html</p>\n",
			"question": "<p>A data analytics company is using SQS queues for decoupling the various processes of an application workflow. The company wants to postpone the delivery of certain messages to the queue by one minute while all other messages need to be delivered immediately to the queue. As a solutions architect, which of the following solutions would you suggest to the company?</p>\n",
			"answers": [
				"<p>Use message timers to postpone the delivery of certain messages to the queue by one minute</p>",
				"<p>Use dead-letter queues to postpone the delivery of certain messages to the queue by one minute</p>",
				"<p>Use visibility timeout to postpone the delivery of certain messages to the queue by one minute</p>",
				"<p>Use delay queues to postpone the delivery of certain messages to the queue by one minute</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A data analytics company is using SQS queues for decoupling the various processes of an application workflow. The company wants to postpone the delivery of certain messages to the queue by one minute while all other messages need to be delivered immediately to the queue. As a solutions architect, which of the following solutions would you suggest to the company?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772916,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"If a spot request is persistent, then it is opened again after your Spot Instance is interrupted\"\n\"Spot blocks are designed not to be interrupted\"\n\"When you cancel an active spot request, it does not terminate the associated instance\"</p>\n\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances.</p>\n\n<p>A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. Therefore the option - \"If a spot request is persistent, then it is opened again after your Spot Instance is interrupted\" - is correct.</p>\n\n<p>How Spot requests work\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/spot_lifecycle.png\">\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</p>\n\n<p>Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously for the duration you select. You can use a duration of 1, 2, 3, 4, 5, or 6 hours. In rare situations, Spot blocks may be interrupted due to Amazon EC2 capacity needs. Therefore, the option - \"Spot blocks are designed not to be interrupted\" - is correct.</p>\n\n<p>If your Spot Instance request is active and has an associated running Spot Instance, or your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. Moreover, to cancel a persistent Spot request and terminate its Spot Instances, you must cancel the Spot request first and then terminate the Spot Instances. Therefore, the option - \"When you cancel an active spot request, it does not terminate the associated instance\" - is correct.</p>\n\n<p>Incorrect options:\n\"When you cancel an active spot request, it terminates the associated instance as well\" - If your Spot Instance request is active and has an associated running Spot Instance, then canceling the request does not terminate the instance; you must terminate the running Spot Instance manually. So, this option is incorrect.</p>\n\n<p>\"If a spot request is persistent, then it is opened again after you stop the Spot Instance\" - If the request is persistent and you stop your Spot Instance, the request only opens after you start your Spot Instance. So, this option is incorrect.</p>\n\n<p>\"Spot blocks are designed to be interrupted, just like a spot instance\" - Spot blocks are designed not to be interrupted. Only in rare situations, spot blocks may be interrupted due to Amazon EC2 capacity needs. So, this option is incorrect.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>When you cancel an active spot request, it terminates the associated instance as well</p>",
				"<p>If a spot request is persistent, then it is opened again after your Spot Instance is interrupted</p>",
				"<p>If a spot request is persistent, then it is opened again after you stop the Spot Instance</p>",
				"<p>Spot blocks are designed not to be interrupted</p>",
				"<p>When you cancel an active spot request, it does not terminate the associated instance</p>",
				"<p>Spot blocks are designed to be interrupted, just like a spot instance</p>"
			],
			"question": "<p>The CTO at a retail company wants to optimize the cost of EC2 instances. He wants to move certain nightly batch jobs to spot instances. He has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)</p>\n"
		},
		"correct_response": [
			"b",
			"d",
			"e"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "The CTO at a retail company wants to optimize the cost of EC2 instances. He wants to move certain nightly batch jobs to spot instances. He has hired you as a solutions architect to provide the initial guidance. Which of the following would you identify as CORRECT regarding the capabilities of spot instances? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773032,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"This is a scale-up example of vertical scalability\"</p>\n\n<p>Vertical scalability means increasing the size of the instance. For example, your application runs on a t2.micro. Scaling up that application vertically means running it on a larger instance such as t2.large. Scaling down that application vertically means running it on a smaller instance such as t2.nano. Scalability is very common for non-distributed systems, such as a database. There’s usually a limit to how much you can vertically scale (hardware limit).\nIn this case, as the instance type was upgraded from t2.nano to u-12tb1.metal, this is a scale-up example of vertical scalability.</p>\n\n<p>Incorrect options:\n\"This is a scale-up example of horizontal scalability\" - Horizontal Scalability means increasing the number of instances/systems for your application. When you increase the number of instances, it's called scale-out whereas if you decrease the number of instances, it's called scale-in. Scale-up is used in conjunction with vertical scaling and not with horizontal scaling. Hence this is incorrect.</p>\n\n<p>\"This is a scale-out example of vertical scalability\" - Scale-out is used in conjunction with horizontal scaling and not with vertical scaling. Hence this is incorrect.</p>\n\n<p>\"This is an example of high availability\" - High availability means running your application/system in at least 2 data centers (== Availability Zones). The goal of high availability is to survive a data center loss. An example of High Availability is when you run instances for the same application across multi AZ. This option has been added as a distractor.</p>\n",
			"question": "<p>A DevOps engineer at an IT company just upgraded an EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade?</p>\n",
			"answers": [
				"<p>This is a scale-up example of vertical scalability</p>",
				"<p>This is a scale-out example of vertical scalability</p>",
				"<p>This is a scale-up example of horizontal scalability</p>",
				"<p>This is an example of high availability</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A DevOps engineer at an IT company just upgraded an EC2 instance type from t2.nano (0.5G of RAM, 1 vCPU) to u-12tb1.metal (12.3 TB of RAM, 448 vCPUs). How would you categorize this upgrade?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772986,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Connection Draining\"</p>\n\n<p>To ensure that an Elastic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open, use connection draining. This enables the load balancer to complete in-flight requests made to instances that are de-registering or unhealthy. The maximum timeout value can be set between 1 and 3,600 seconds (the default is 300 seconds). When the maximum time limit is reached, the load balancer forcibly closes connections to the de-registering instance.</p>\n\n<p>Incorrect options:\n\"Cross Zone Load Balancing\" - The nodes for your load balancer distribute requests from clients to registered targets. When cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. Cross Zone load balancing cannot be used to complete in-flight requests made to instances that are de-registering or unhealthy.</p>\n\n<p>\"Sticky Sessions\" - You can use the sticky session feature (also known as session affinity) to enable the load balancer to bind a user's session to a specific instance. This ensures that all requests from the user during the session are sent to the same instance. Sticky sessions cannot be used to complete in-flight requests made to instances that are de-registering or unhealthy.</p>\n\n<p>\"Idle Timeout\" - For each request that a client makes through an Elastic Load Balancer, the load balancer maintains two connections. The front-end connection is between the client and the load balancer. The back-end connection is between the load balancer and a registered EC2 instance. The load balancer has a configured \"idle timeout\" period that applies to its connections. If no data has been sent or received by the time that the \"idle timeout\" period elapses, the load balancer closes the connection. \"Idle timeout\" can not be used to complete in-flight requests made to instances that are de-registering or unhealthy.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/config-conn-drain.html</p>\n",
			"question": "<p>A retail company has its flagship application running on a fleet of EC2 instances behind an Elastic Load Balancer (ELB). The engineering team has been seeing recurrent issues wherein the in-flight requests from the ELB to the EC2 instances are getting dropped when an instance becomes unhealthy. Which of the following features can be used to address this issue?</p>\n",
			"answers": [
				"<p>Cross Zone load balancing</p>",
				"<p>Connection Draining</p>",
				"<p>Sticky Sessions</p>",
				"<p>Idle Timeout</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail company has its flagship application running on a fleet of EC2 instances behind an Elastic Load Balancer (ELB). The engineering team has been seeing recurrent issues wherein the in-flight requests from the ELB to the EC2 instances are getting dropped when an instance becomes unhealthy. Which of the following features can be used to address this issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773046,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use a target tracking scaling policy based on a custom Amazon SQS queue metric\"</p>\n\n<p>If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively. You may use an existing CloudWatch Amazon SQS metric like ApproximateNumberOfMessagesVisible for target tracking but you could still face an issue so that the number of messages in the queue might not change proportionally to the size of the Auto Scaling group that processes messages from the queue. The solution is to use a backlog per instance metric with the target value being the acceptable backlog per instance to maintain. To calculate your backlog per instance, divide the ApproximateNumberOfMessages queue attribute by the number of instances in the InService state for the Auto Scaling group. Then set a target value for the Acceptable backlog per instance.\nTo illustrate with an example, let's say that the current ApproximateNumberOfMessages is 1500 and the fleet's running capacity is 10. If the average processing time is 0.1 seconds for each message and the longest acceptable latency is 10 seconds, then the acceptable backlog per instance is 10 / 0.1, which equals 100. This means that 100 is the target value for your target tracking policy. If the backlog per instance is currently at 150 (1500 / 10), your fleet scales out, and it scales out by five instances to maintain proportion to the target value.</p>\n\n<p>Scaling Based on Amazon SQS:\n<img src=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/images/sqs-as-custom-metric-diagram.png\">\nvia - https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</p>\n\n<p>Incorrect options:\n\"Use a simple scaling policy based on a custom Amazon SQS queue metric\" - With simple scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. The main issue with simple scaling is that after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. This implies that the application would not be able to react quickly to sudden spikes in orders.</p>\n\n<p>\"Use a step scaling policy based on a custom Amazon SQS queue metric\" - With step scaling, you choose scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. When step adjustments are applied, they increase or decrease the current capacity of your Auto Scaling group, and the adjustments vary based on the size of the alarm breach. For the given use-case, step scaling would try to approximate the correct number of instances by increasing/decreasing the steps as per the policy. This is not as efficient as the target tracking policy where you can calculate the exact number of instances required to handle the spike in orders.</p>\n\n<p>\"Use a scheduled scaling policy based on a custom Amazon SQS queue metric\" - Scheduled scaling allows you to set your scaling schedule. For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can plan your scaling actions based on the predictable traffic patterns of your web application. Scaling actions are performed automatically as a function of time and date. You cannot use scheduled scaling policies to address the sudden spike in orders.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use a target tracking scaling policy based on a custom Amazon SQS queue metric</p>",
				"<p>Use a simple scaling policy based on a custom Amazon SQS queue metric</p>",
				"<p>Use a step scaling policy based on a custom Amazon SQS queue metric</p>",
				"<p>Use a scheduled scaling policy based on a custom Amazon SQS queue metric</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>An e-commerce company runs its web application on EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an SQS queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An e-commerce company runs its web application on EC2 instances in an Auto Scaling group and it's configured to handle consumer orders in an SQS queue for downstream processing. The DevOps team has observed that the performance of the application goes down in case of a sudden spike in orders received. As a solutions architect, which of the following solutions would you recommend to address this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772990,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"You can copy an AMI across AWS Regions\"\n\"You can share an AMI with another AWS account\"\n\"Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot\"</p>\n\n<p>An Amazon Machine Image (AMI) provides the information required to launch an instance. An AMI includes the following:\nOne or more EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance.\nLaunch permissions that control which AWS accounts can use the AMI to launch instances.\nA block device mapping that specifies the volumes to attach to the instance when it's launched.</p>\n\n<p>You can copy an AMI within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process. Therefore, the option - \"You can copy an AMI across AWS Regions\" - is correct.</p>\n\n<p>Copying AMIs across regions:\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/ami_copy.png\">\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</p>\n\n<p>The following table shows encryption support for various AMI-copying scenarios. While it is possible to copy an unencrypted snapshot to yield an encrypted snapshot, you cannot copy an encrypted snapshot to yield an unencrypted one. Therefore, the option - \"Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot\" is correct.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q55-i1.jpg\">\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</p>\n\n<p>You can share an AMI with another AWS account. To copy an AMI that was shared with you from another account, the owner of the source AMI must grant you read permissions for the storage that backs the AMI, either the associated EBS snapshot (for an Amazon EBS-backed AMI) or an associated S3 bucket (for an instance store-backed AMI). Therefore, the option - \"You can share an AMI with another AWS account\" - is correct.</p>\n\n<p>Incorrect options:\n\"You cannot copy an AMI across AWS Regions\"\n\"You cannot share an AMI with another AWS account\"\n\"Copying an AMI backed by an encrypted snapshot results in an unencrypted target snapshot\"</p>\n\n<p>These options are just the inverse of the correct options described above.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html</p>\n",
			"question": "<p>The DevOps team at a multi-national company is helping its subsidiaries standardize EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of AMIs? (Select three)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>You cannot copy an AMI across AWS Regions</p>",
				"<p>You cannot share an AMI with another AWS account</p>",
				"<p>Copying an AMI backed by an encrypted snapshot results in an unencrypted target snapshot</p>",
				"<p>You can copy an AMI across AWS Regions</p>",
				"<p>You can share an AMI with another AWS account</p>",
				"<p>Copying an AMI backed by an encrypted snapshot cannot result in an unencrypted target snapshot</p>"
			]
		},
		"correct_response": [
			"d",
			"e",
			"f"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "The DevOps team at a multi-national company is helping its subsidiaries standardize EC2 instances by using the same Amazon Machine Image (AMI). Some of these subsidiaries are in the same AWS region but use different AWS accounts whereas others are in different AWS regions but use the same AWS account as the parent company. The DevOps team has hired you as a solutions architect for this project. Which of the following would you identify as CORRECT regarding the capabilities of AMIs? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773006,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Amazon FSx for Windows File Server\" - Amazon FSx for Windows File Server is a fully managed, highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration.</p>\n\n<p>\"File Gateway Configuration of AWS Storage Gateway\" - Depending on the use case, Storage Gateway provides 3 types of storage interfaces for on-premises applications: File, Volume, and Tape. The File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as Network File System (NFS) and Server Message Block (SMB).</p>\n\n<p>Incorrect options:\n\"Elastic File System\" - Amazon EFS is a file storage service for use with Amazon EC2. Amazon EFS provides a file system interface, file system access semantics, and concurrently-accessible storage for up to thousands of Amazon EC2 instances. Amazon EFS uses the Network File System protocol. EFS does not support SMB protocol.</p>\n\n<p>\"Elastic Block Storage\" - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that require the lowest latency access to data from a single EC2 instance. EBS does not support SMB protocol.</p>\n\n<p>Simple Storage Service (Amazon S3) - Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Amazon S3 provides a simple, standards-based REST web services interface that is designed to work with any Internet-development toolkit. S3 does not support SMB protocol.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/fsx/windows/\nhttps://aws.amazon.com/storagegateway/file/</p>\n",
			"question": "<p>A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)</p>\n",
			"answers": [
				"<p>Elastic File System</p>",
				"<p>Elastic Block Storage</p>",
				"<p>Amazon FSx for Windows File Server</p>",
				"<p>Simple Storage Service (Amazon S3)</p>",
				"<p>File Gateway Configuration of AWS Storage Gateway</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c",
			"e"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A financial services company wants to move the Windows file server clusters out of their datacenters. They are looking for cloud file storage offerings that provide full Windows compatibility. Can you identify the AWS storage services that provide highly reliable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol compatible with Windows systems? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772944,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata.\"\n\"If your instance has a public IPv4 address, it retains the public IPv4 address after recovery\"</p>\n\n<p>You can create an Amazon CloudWatch alarm to automatically recover the Amazon EC2 instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group. If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.</p>\n\n<p>Incorrect options:</p>\n\n<p>\"Terminated EC2 instances can be recovered if they are configured at the launch of instance\" - This is incorrect as terminated instances cannot be recovered.</p>\n\n<p>\"During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained\" - As mentioned above, during instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost.</p>\n\n<p>\"If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery\" - As mentioned above, if your instance has a public IPv4 address, it retains the public IPv4 address after recovery.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</p>\n",
			"question": "<p>The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)</p>\n",
			"answers": [
				"<p>Terminated EC2 instances can be recovered if they are configured at the launch of instance</p>",
				"<p>A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata</p>",
				"<p>If your instance has a public IPv4 address, it retains the public IPv4 address after recovery</p>",
				"<p>During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is retained</p>",
				"<p>If your instance has a public IPv4 address, it does not retain the public IPv4 address after recovery</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b",
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The engineering team at a social media company wants to use Amazon CloudWatch alarms to automatically recover EC2 instances if they become impaired. The team has hired you as a solutions architect to provide subject matter expertise. As a solutions architect, which of the following statements would you identify as CORRECT regarding this automatic recovery process? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773004,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Amazon Aurora Serverless\"</p>\n\n<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start-up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. You pay on a per-second basis for the database capacity you use when the database is active and migrate between standard and serverless configurations with a few clicks in the Amazon RDS Management Console.</p>\n\n<p>Incorrect options:\n\"Amazon DynamoDB\" - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second. But, it is a NoSQL database service and hence not a fit for the given use-case.</p>\n\n<p>\"Amazon Relational Database Service (Amazon RDS)\" -  Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups. Amazon RDS does not offer auto scaling capability and hence is not the right fit for the given use-case.</p>\n\n<p>\"Amazon Aurora \" - Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. But, its not a complete auto scaling solution and neither is it fully managed like Aurora serverless. Hence is not the right fit for the given use-case.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/rds/aurora/serverless/</p>\n",
			"question": "<p>A leading bank has moved its IT infrastructure to AWS Cloud and they have been using Amazon EC2 Auto Scaling for their web servers. This has helped them deal with traffic spikes effectively. But, their SQL database has now become a bottleneck and they urgently need a fully managed auto scaling solution for their SQL database to address any unpredictable changes in the traffic. Can you identify the AWS service that is best suited for this use-case?</p>\n",
			"answers": [
				"<p>Amazon DynamoDB</p>",
				"<p>Amazon Relational Database Service (Amazon RDS)</p>",
				"<p>Amazon Aurora Serverless</p>",
				"<p>Amazon Aurora</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A leading bank has moved its IT infrastructure to AWS Cloud and they have been using Amazon EC2 Auto Scaling for their web servers. This has helped them deal with traffic spikes effectively. But, their SQL database has now become a bottleneck and they urgently need a fully managed auto scaling solution for their SQL database to address any unpredictable changes in the traffic. Can you identify the AWS service that is best suited for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18773016,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:\n\"If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action\"\n\"SCPs affect all users and roles in attached accounts, including the root user\"\n\"SCPs do not affect service-linked role\"</p>\n\n<p>Service control policies (SCPs) are one type of policy that can be used to manage your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization’s access control guidelines.\nIn SCPs, you can restrict which AWS services, resources, and individual API actions the users and roles in each member account can access. You can also define conditions for when to restrict access to AWS services, resources, and API actions. These restrictions even override the administrators of member accounts in the organization.\nPlease note the following effects on permissions vis-a-vis the SCPs:\nIf a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action.\nSCPs affect all users and roles in the attached accounts, including the root user.\nSCPs do not affect any service-linked role.</p>\n\n<p>Incorrect options:\n\"If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action\"\n\"SCPs affect all users and roles in attached accounts, excluding the root user\"\n\"SCPs affect service-linked roles\"</p>\n\n<p>These options are just the inverse of the correct options described above.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</p>\n",
			"question": "<p>An AWS Organization is using Service Control Policies (SCP) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can't perform that action</p>",
				"<p>If a user or role has an IAM permission policy that grants access to an action that is either not allowed or explicitly denied by the applicable SCPs, the user or role can still perform that action</p>",
				"<p>SCPs affect all users and roles in attached accounts, including the root user</p>",
				"<p>SCPs affect all users and roles in attached accounts, excluding the root user</p>",
				"<p>SCPs affect service-linked roles</p>",
				"<p>SCPs do not affect service-linked role</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a",
			"c",
			"f"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An AWS Organization is using Service Control Policies (SCP) for central control over the maximum available permissions for all accounts in their organization. This allows the organization to ensure that all accounts stay within the organization’s access control guidelines. Which of the given scenarios are correct regarding the permissions described below? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772914,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct options:\n\"Use EC2 hibernate\"</p>\n\n<p>When you hibernate an instance, AWS signals the operating system to perform hibernation (suspend-to-disk). Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. AWS then persists the instance's Amazon EBS root volume and any attached Amazon EBS data volumes.\nWhen you start your instance:\nThe Amazon EBS root volume is restored to its previous state\nThe RAM contents are reloaded\nThe processes that were previously running on the instance are resumed\nPreviously attached data volumes are reattached and the instance retains its instance ID</p>\n\n<p>Sicnce EC2 hibernate allows RAM contents to be reloaded and previously running processes to be resumed, hence this option is correct.</p>\n\n<p>Overview of EC2 hibernation\n<img src=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/images/hibernation-flow.png\">\nvia - https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</p>\n\n<p>Incorrect options:\n\"Use a custom AMI with auxiliary software libraries pre-installed\" - Custom AMI will not help as the required software libraries would still need to be loaded in the RAM, causing a delay in processing. Custom AMIs cannot be used to pre-warm the instances</p>\n\n<p>\"Use Spot Instances\" - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. You cannot use spot instances types to pre-warm the instances.</p>\n\n<p>\"Create an Auto Scaling Group (ASG) with capacity 0\" - Using Auto Scaling Group (ASG) with capacity 0 implies that you will not have any running instances when there is no traffic. You cannot use this to pre-warm the instances.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html</p>\n",
			"question": "<p>An AI research group at a university uses a proprietary speech synthesis application hosted on an EC2 instance. When rebooted, the instance takes a long time to build a memory footprint for all the auxiliary software libraries required for the application to function. The research group would like to pre-warm the instances so they can launch the analysis right away when needed. Which of the following solutions would you recommend?</p>\n",
			"answers": [
				"<p>Use a custom AMI with auxiliary software libraries pre-installed</p>",
				"<p>Use EC2 hibernate</p>",
				"<p>Create an Auto Scaling Group (ASG) with capacity 0</p>",
				"<p>Use Spot Instances</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An AI research group at a university uses a proprietary speech synthesis application hosted on an EC2 instance. When rebooted, the instance takes a long time to build a memory footprint for all the auxiliary software libraries required for the application to function. The research group would like to pre-warm the instances so they can launch the analysis right away when needed. Which of the following solutions would you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772924,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Amazon S3 Glacier\"</p>\n\n<p>Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving.\nAmazon S3 Glacier automatically encrypts data at rest using Advanced Encryption Standard (AES) 256-bit symmetric keys and supports secure transfer of your data over Secure Sockets Layer (SSL).</p>\n\n<p>Incorrect options:\n\"Amazon S3 Standard-Infrequent Access\" - Amazon S3 Standard-Infrequent Access is for data that is accessed less frequently, but requires rapid access when needed. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. It does not support encryption by default for both data at rest as well as in-transit.</p>\n\n<p>\"Amazon S3 Intelligent Tiering\" - The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. It does not support encryption by default for both data at rest as well as in-transit.</p>\n\n<p>\"Amazon S3 One Zone-IA\" - S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ. It does not support encryption by default for both data at rest as well as in-transit.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/glacier/features/</p>\n",
			"question": "<p>Which of the following S3 storage classes supports encryption by default for both data at rest as well as in-transit?</p>\n",
			"answers": [
				"<p>Amazon S3 Glacier</p>",
				"<p>Amazon S3 One Zone-IA</p>",
				"<p>Amazon S3 Standard-Infrequent Access</p>",
				"<p>Amazon S3 Intelligent Tiering</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "Which of the following S3 storage classes supports encryption by default for both data at rest as well as in-transit?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772956,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"Use RAID 0 when I/O performance is more important than fault tolerance\"</p>\n\n<p>With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level.</p>\n\n<p>RAID configuration options for I/O performance v/s fault tolerance:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q62-i1.jpg\"></p>\n\n<p>Incorrect options:\n\"Use RAID 1 when I/O performance is more important than fault tolerance\" - This is incorrect because you should use RAID 1 when fault tolerance is more important than I/O performance.</p>\n\n<p>\"Both RAID 0 and RAID 1 provide equally good I/O performance\" -  This is incorrect because RAID 0 provides better I/O performance.</p>\n\n<p>\"Amazon EBS does not support the standard RAID configurations\" - This is incorrect because EBS supports the standard RAID configurations.</p>\n\n<p>Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</p>\n",
			"question": "<p>A data analytics company is running a proprietary database on an EC2 instance using EBS volumes. The database is heavily I/O bound. As a solutions architect, which of the following RAID configurations would you recommend improving the I/O performance?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use RAID 0 when I/O performance is more important than fault tolerance</p>",
				"<p>Use RAID 1 when I/O performance is more important than fault tolerance</p>",
				"<p>Both RAID 0 and RAID 1 provide equally good I/O performance</p>",
				"<p>Amazon EBS does not support the standard RAID configurations</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A data analytics company is running a proprietary database on an EC2 instance using EBS volumes. The database is heavily I/O bound. As a solutions architect, which of the following RAID configurations would you recommend improving the I/O performance?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772948,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct options:\n\"AWS Snowmobile\"</p>\n\n<p>AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.  AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.</p>\n\n<p>Incorrect options:</p>\n\n<p>\"AWS Snowball\" - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.</p>\n\n<p>\"AWS Storage Gateway\" - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.  Used for key hybrid storage solutions that include moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low latency access to data in AWS for on-premises applications, as well as various migration, archiving, processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers are in remote locations where internet speed may not optimal, thereby increasing both cost and time for migrating 20TB of data.</p>\n\n<p>\"AWS Direct Connect\" - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customer’s datacenter or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the studio wants the data transfer to be done in the shortest possible time.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/snowmobile/</p>\n",
			"question": "<p>A Hollywood production studio is looking at transferring their existing digital media assets of around 20PB to AWS Cloud in the shortest possible timeframe. Which of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>AWS Snowball</p>",
				"<p>AWS Storage Gateway</p>",
				"<p>AWS Direct Connect</p>",
				"<p>AWS Snowmobile</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A Hollywood production studio is looking at transferring their existing digital media assets of around 20PB to AWS Cloud in the shortest possible timeframe. Which of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772960,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"EFS Infrequent Access\"</p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs.</p>\n\n<p>How EFS Infrequent Access Works:\n<img src=\"https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png\">\nvia - https://aws.amazon.com/efs/features/infrequent-access/</p>\n\n<p>Incorrect options\n\"EFS Standard\" - EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect.</p>\n\n<p>\"S3 Standard\", \"S3 Standard-IA\" - Both these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/efs/features/infrequent-access/</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>EFS Infrequent Access</p>",
				"<p>EFS Standard</p>",
				"<p>S3 Standard</p>",
				"<p>S3 Standard-IA</p>"
			],
			"relatedLectureIds": "",
			"question": "<p>A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year. As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 18772970,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n\"You can use an Internet Gateway ID as the custom source for the inbound rule\"</p>\n\n<p>A security group acts as a virtual firewall that controls the traffic for one or more instances. When you launch an instance, you can specify one or more security groups; otherwise, you can use the default security group. You can add rules to each security group that allows traffic to or from its associated instances. You can modify the rules for a security group at any time; the new rules are automatically applied to all instances that are associated with the security group.</p>\n\n<p>Please see this list of allowed source or destination for security group rules:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt4-q65-i1.jpg\"></p>\n\n<p>Therefore, you cannot use an Internet Gateway ID as the custom source for the inbound rule.</p>\n\n<p>Incorrect options:\n\"You can use a security group as the custom source for the inbound rule\"\n\"You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule\"\n\"You can use an IP address as the custom source for the inbound rule\"</p>\n\n<p>As described in the list of allowed sources or destinations for security group rules, the above options are supported.</p>\n\n<p>References:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html</p>\n",
			"question": "<p>The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?</p>\n",
			"answers": [
				"<p>You can use a security group as the custom source for the inbound rule</p>",
				"<p>You can use a range of IP addresses in CIDR block notation as the custom source for the inbound rule</p>",
				"<p>You can use an IP address as the custom source for the inbound rule</p>",
				"<p>You can use an Internet Gateway ID as the custom source for the inbound rule</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "The DevOps team at an IT company has recently migrated to AWS and they are configuring security groups for their two-tier application with public web servers and private database servers. The team wants to understand the allowed configuration options for an inbound rule for a security group. As a solutions architect, which of the following would you identify as an INVALID option for setting up such a configuration?",
		"related_lectures": []
	}
]