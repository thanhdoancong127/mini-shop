[
	{
		"_class": "assessment",
		"id": 25815722,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</p>",
				"<p>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</p>",
				"<p>Enable DAX for DynamoDB and ElastiCache Memcached for S3</p>",
				"<p>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Enable DynamoDB Accelerator (DAX) for DynamoDB and CloudFront for S3</strong></p>\n\n<p>DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement—from milliseconds to microseconds—even at millions of requests per second.</p>\n\n<p>DAX is tightly integrated with DynamoDB—you simply provision a DAX cluster, use the DAX client SDK to point your existing DynamoDB API calls at the DAX cluster, and let DAX handle the rest. Because DAX is API-compatible with DynamoDB, you don't have to make any functional application code changes. DAX is used to natively cache DynamoDB reads.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content.</p>\n\n<p>So, you can use CloudFront to improve application performance to serve static content from S3.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and CloudFront for S3</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store.</p>\n\n<p>ElastiCache for Redis Overview:\n<img src=\"https://d1.awsstatic.com/elasticache/EC_Use_Cases/product-page-diagram_ElastiCache_redis_how-it-works.eac60b60768c2a2041cc0b2673e7f5d2a0fef6a1.png\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p>Although you can integrate Redis with DynamoDB, it's much more involved than using DAX which is a much better fit.</p>\n\n<p><strong>Enable DAX for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p><strong>Enable ElastiCache Redis for DynamoDB and ElastiCache Memcached for S3</strong></p>\n\n<p>Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.</p>\n\n<p>ElastiCache cannot be used as a cache to serve static content from S3, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/dynamodb/dax/\">https://aws.amazon.com/dynamodb/dax/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n",
			"question": "<p>The engineering team at a retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. On analyzing the usage trends, it is found that 90% of the read requests are shared across all users.</p>\n\n<p>As a Solutions Architect, which of the following is the MOST efficient solution to improve the application performance?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "The engineering team at a retail company has developed a REST API which is deployed in an Auto Scaling group behind an Application Load Balancer. The API stores the data payload in DynamoDB and the static content is served through S3. On analyzing the usage trends, it is found that 90% of the read requests are shared across all users.\n\nAs a Solutions Architect, which of the following is the MOST efficient solution to improve the application performance?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815724,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p>The Application Load Balancer (ALB) is best suited for load balancing HTTP and HTTPS traffic and provides advanced request routing targeted at the delivery of modern application architectures, including microservices and containers. Operating at the individual request level (Layer 7), the Application Load Balancer routes traffic to targets within Amazon Virtual Private Cloud (Amazon VPC) based on the content of the request.</p>\n\n<p>An Elastic IP address is a static, public, IPv4 address allocated to your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. Elastic IPs do not change and remain allocated to your account until you delete them.</p>\n\n<p><strong>Use an Application Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure an Elastic IP address to mask any failure of an instance</strong> - This is the correct option since the question has a specific requirement for content-based routing which can be configured via the Application Load Balancer. Different AZs provide high availability to the overall architecture and Elastic IP address will help mask any instance failures.</p>\n\n<p>More info on Application Load Balancer:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q2-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\">https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/</a></p>\n\n<p>More info on Elastic Load Balancer:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q2-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\">https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use a Network Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Private IP address to mask any failure of an instance</strong> - Network Load Balancer cannot facilitate content-based routing so this option is incorrect.</p>\n\n<p><strong>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure an Elastic IP address to mask any failure of an instance</strong></p>\n\n<p><strong>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Public IP address to mask any failure of an instance</strong></p>\n\n<p>Both these options are incorrect as you cannot use the Auto Scaling group to distribute traffic to the EC2 instances. You can span your Auto Scaling group across multiple Availability Zones within a Region and then attaching a load balancer to distribute incoming traffic across those zones.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q2-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/\">https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf\">https://docs.aws.amazon.com/whitepapers/latest/fault-tolerant-components/fault-tolerant-components.pdf</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-add-availability-zone.html</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>An e-commerce company is looking for a highly available architecture to migrate their flagship application which is planned to be hosted on a fleet of Amazon EC2 instances. The company is also looking at facilitating content-based routing in its architecture.</p>\n\n<p>As a Solutions Architect, which of the following will you suggest for the company?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use an Application Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure an Elastic IP address to mask any failure of an instance</p>",
				"<p>Use a Network Load Balancer for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Private IP address to mask any failure of an instance</p>",
				"<p>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure an Elastic IP address to mask any failure of an instance</p>",
				"<p>Use an Auto Scaling group for distributing traffic to the EC2 instances spread across different Availability Zones. Configure a Public IP address to mask any failure of an instance</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An e-commerce company is looking for a highly available architecture to migrate their flagship application which is planned to be hosted on a fleet of Amazon EC2 instances. The company is also looking at facilitating content-based routing in its architecture.\n\nAs a Solutions Architect, which of the following will you suggest for the company?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815726,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"question": "<p>The health check for a system monitors 100,000 servers on a distributed network. If a server is found to be unhealthy, an SNS notification is sent, along with a priority message on registered phone numbers. A major event had stalled 10,000 of these servers and the company realized that their design couldn't stand the load of firing thousands of notifications and updates simultaneously. They also understood that, even if half the servers go unhealthy, it will choke the network and the company will not be able to update the clients on-time about the status of their servers.</p>\n\n<p>As a Solutions Architect, which of the following options do you suggest to address this scenario?</p>\n",
			"answers": [
				"<p>Use AWS Lambda with SNS to speed up the processing of records</p>",
				"<p>The SNS service is not meant for heavy workloads of this order. Opting for SQS would have kept the system stable during server fails</p>",
				"<p>The health check system should send the full snapshot of the current state of all the servers each time, denoting them as bits of data to reduce workload and keep spikes at bay</p>",
				"<p>The health check system should send the current state of only the failed servers, denoting them as bits of data to reduce workload</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p>Systems can fail when there are large, rapid changes in load. For example, a health check system that monitors the health of thousands of servers should send the same size payload (a full snapshot of the current state) each time. Whether no servers are failing, or all of them, the health check system is doing constant work with no large, rapid changes.</p>\n\n<p><strong>The health check system should send the full snapshot of the current state of all the servers each time, denoting them as bits of data to reduce workload and keep spikes at bay</strong>- The health check system should send the full snapshot of the current state each time. 100,000 server health states, each represented by a bit, would only be a 12.5-KB payload. Whether no servers are failing, or all of them are, the health check system is doing constant work, and large, rapid changes are not a threat to the system stability. This is actually how the control plane is designed for Amazon Route 53 health checks.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The SNS service is not meant for heavy workloads of this order. Opting for SQS would have kept the system stable during server fails</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. An SQS before an SNS will help scale the architecture better. But, AWS in its Reliability pillar states that the load on systems shouldn't drastically change from a normal working day to a heavy load day. So this option is ruled out.</p>\n\n<p><strong>The health check system should send the current state of only the failed servers, denoting them as bits of data to reduce workload</strong> - The design principle will still be the same as the existing one. By sending only failed servers data, spikes in workload are possible and it will result in the same network spikes as the current situation.</p>\n\n<p><strong>Use AWS Lambda with SNS to speed up the processing of records</strong> - AWS Lambda is a serverless compute service of Amazon. But, the given scenario requires a change in design to allow the free flow of data without affecting the network. Lambda cannot help address this use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/architecture/AWS-Reliability-Pillar.pdf\">https://d1.awsstatic.com/whitepapers/architecture/AWS-Reliability-Pillar.pdf</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The health check for a system monitors 100,000 servers on a distributed network. If a server is found to be unhealthy, an SNS notification is sent, along with a priority message on registered phone numbers. A major event had stalled 10,000 of these servers and the company realized that their design couldn't stand the load of firing thousands of notifications and updates simultaneously. They also understood that, even if half the servers go unhealthy, it will choke the network and the company will not be able to update the clients on-time about the status of their servers.\n\nAs a Solutions Architect, which of the following options do you suggest to address this scenario?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815728,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>An Internet-of-Things(IoT) technology company has leveraged a distributed architecture to build its AWS Cloud based solution. This distributed system relies on communications networks to interconnect its three different service components - service A, service B and Service C. Service A invokes service B which in turn invokes service C for sending a response back to service A. During the testing phase, it has been noticed that the failure of service C results in the failure of service A too.</p>\n\n<p>As a Solutions Architect, which of the following will you suggest to fix this problem?</p>\n",
			"answers": [
				"<p>Service B should re-compute the response using different means (or service) to replace the failure of service C</p>",
				"<p>Since service C is critical for the entire architecture, in the event of a failure, service C should have a standby to fall back on</p>",
				"<p>Service B should return an error when service C fails</p>",
				"<p>Service B should return a static response, a simple alternative to returning an error when service C fails to respond</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Service B should return a static response, a simple alternative to returning an error when service C fails to respond</strong></p>\n\n<p>Distributed systems rely on communications networks to interconnect components (such as servers or services). Your workload must operate reliably despite data loss or latency over these networks. Components of the distributed system must operate in a\nway that does not negatively impact other components or the workload. AWS suggests implementing graceful degradation to transform applicable hard dependencies into soft dependencies.</p>\n\n<p>Check out the complete solution in this deep-dive:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q4-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/architecture/AWS-Reliability-Pillar.pdf\">https://d1.awsstatic.com/whitepapers/architecture/AWS-Reliability-Pillar.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Service B should re-compute the response using different means (or service) to replace the failure of service C</strong></p>\n\n<p><strong>Since service C is critical for the entire architecture, in the event of a failure, service C should have a standby to fall back on</strong></p>\n\n<p>Note that the static response, discussed in the correct option, is a simple alternative to returning an error and is not an attempt to re-compute the response using different means. Such attempts at a completely different mechanism to try to achieve the same result are called fallback behavior, and are an anti-pattern to be avoided. Therefore the two options given above are incorrect options as these are anti-patterns that AWS suggests should be avoided while designing the architecture.</p>\n\n<p><strong>Service B should return an error when service C fails</strong> - Returning an error will not solve the problem of keeping the system available, so this is an incorrect answer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/architecture/AWS-Reliability-Pillar.pdf\">https://d1.awsstatic.com/whitepapers/architecture/AWS-Reliability-Pillar.pdf</a></p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An Internet-of-Things(IoT) technology company has leveraged a distributed architecture to build its AWS Cloud based solution. This distributed system relies on communications networks to interconnect its three different service components - service A, service B and Service C. Service A invokes service B which in turn invokes service C for sending a response back to service A. During the testing phase, it has been noticed that the failure of service C results in the failure of service A too.\n\nAs a Solutions Architect, which of the following will you suggest to fix this problem?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815730,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. To augment its custom technology stack, the company is looking for AWS services that can be used for buffering or throttling to handle traffic variations.</p>\n\n<p>Which of the following services can be used to support this requirement?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Amazon SQS, Amazon SNS and AWS Lambda</p>",
				"<p>Amazon API Gateway, Amazon SQS and Amazon Kinesis</p>",
				"<p>Amazon Gateway Endpoints, Amazon SQS and Amazon Kinesis</p>",
				"<p>Elastic Load Balancer, Amazon SQS, AWS Lambda</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p>Throttling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount of time.</p>\n\n<p><strong>Amazon API Gateway, Amazon SQS and Amazon Kinesis</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size.</p>\n\n<p>Amazon SQS - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Amazon SQS offers buffer capabilities to smooth out temporary volume spikes without losing messages or increasing latency.</p>\n\n<p>Amazon Kinesis - Amazon Kinesis is a fully managed, scalable service that can ingest, buffer, and process streaming data in real-time.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon SQS, Amazon SNS and AWS Lambda</strong> - Amazon SQS has the ability to buffer its messages. Amazon Simple Notification Service (SNS) cannot buffer messages and is generally used with SQS to provide the buffering facility. AWS Lambda is a compute service and does not provide any buffering capability. So, this combination of services is incorrect.</p>\n\n<p><strong>Amazon Gateway Endpoints, Amazon SQS and Amazon Kinesis</strong> - A Gateway Endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. This cannot help in throttling or buffering of requests. Amazon SQS and Kinesis can buffer incoming data. Since Gateway Endpoint is an incorrect service for throttling or buffering, this option is incorrect.</p>\n\n<p><strong>Elastic Load Balancer, Amazon SQS, AWS Lambda</strong> - Elastic Load Balancer cannot throttle requests. Amazon SQS can be used to buffer messages. AWS Lambda cannot be used for buffering. So, this combination is also incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A Big Data analytics company wants to set up an AWS cloud architecture that throttles requests in case of sudden traffic spikes. To augment its custom technology stack, the company is looking for AWS services that can be used for buffering or throttling to handle traffic variations.\n\nWhich of the following services can be used to support this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815732,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p>AWS WAF is a web application firewall service that lets you monitor web requests and protect your web applications from malicious requests. Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. You can also use AWS WAF preconfigured protections to block common attacks like SQL injection or cross-site scripting.</p>\n\n<p><strong>Configure AWS WAF on the Application Load Balancer in a VPC</strong></p>\n\n<p>You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). Geographic (Geo) Match Conditions in AWS WAF allows you to use AWS WAF to restrict application access based on the geographic location of your viewers. With geo match conditions you can choose the countries from which AWS WAF should allow access.</p>\n\n<p>Geo match conditions are important for many customers. For example, legal and licensing requirements restrict some customers from delivering their applications outside certain countries. These customers can configure a whitelist that allows only viewers in those countries. Other customers need to prevent the downloading of their encrypted software by users in certain countries. These customers can configure a blacklist so that end-users from those countries are blocked from downloading their software.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Geo Restriction feature of Amazon CloudFront in a VPC</strong> - Geo Restriction feature of CloudFront helps in restricting traffic based on the user's geographic location. But, CloudFront works from edge locations and doesn't belong to a VPC. Hence, this option itself is incorrect and given only as a distractor.</p>\n\n<p><strong>Configure the security group on the Application Load Balancer</strong></p>\n\n<p><strong>Configure the security group for the EC2 instances</strong></p>\n\n<p>Security Groups cannot restrict access based on the user's Geographic location.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/\">https://aws.amazon.com/about-aws/whats-new/2017/10/aws-waf-now-supports-geographic-match/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/\">https://aws.amazon.com/blogs/aws/aws-web-application-firewall-waf-for-application-load-balancers/</a></p>\n\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/\">https://aws.amazon.com/about-aws/whats-new/2016/12/AWS-WAF-now-available-on-Application-Load-Balancer/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Configure the security group on the Application Load Balancer</p>",
				"<p>Configure AWS WAF on the Application Load Balancer in a VPC</p>",
				"<p>Use Geo Restriction feature of Amazon CloudFront in a VPC</p>",
				"<p>Configure the security group for the EC2 instances</p>"
			],
			"question": "<p>A media company runs a photo-sharing web application that is currently accessed across three different countries. The application is deployed on several Amazon EC2 instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company.</p>\n\n<p>Which configuration should be used to meet this changed requirement?</p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A media company runs a photo-sharing web application that is currently accessed across three different countries. The application is deployed on several Amazon EC2 instances running behind an Application Load Balancer. With new government regulations, the company has been asked to block access from two countries and allow access only from the home country of the company.\n\nWhich configuration should be used to meet this changed requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815734,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases. Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. Aurora is not an in-memory database.</p>\n\n<p>Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Amazon Aurora Global Database is the correct choice for the given use-case.</p>\n\n<p>For the given use-case, we, therefore, need to have two Aurora clusters, one for the global table (games table) and the other one for the local tables (users and games_played tables).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use an Amazon Aurora Global Database for the <code>games</code> table and use DynamoDB for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p><strong>Use a DynamoDB Global Table for the <code>games</code> table and use DynamoDB for the <code>users</code> and <code>games_played</code> tables</strong></p>\n\n<p>Here, we want minimal application refactoring. DynamoDB and Aurora have a completely different API, due to Aurora being SQL and DynamoDB being NoSQL. So all three options are incorrect, as they have DynamoDB as one of the components.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/faqs/\">https://aws.amazon.com/rds/aurora/faqs/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>You are working as a Solutions Architect with a gaming company that has deployed an application that allows its customers to play games online. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the <code>games</code> table to be accessible globally but needs the <code>users</code> and <code>games_played</code> table to be regional only.</p>\n\n<p>How would you implement this with minimal application refactoring?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use an Amazon Aurora Global Database for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</p>",
				"<p>Use an Amazon Aurora Global Database for the <code>games</code> table and use DynamoDB for the <code>users</code> and <code>games_played</code> tables</p>",
				"<p>Use a DynamoDB Global Table for the <code>games</code> table and use Amazon Aurora for the <code>users</code> and <code>games_played</code> tables</p>",
				"<p>Use a DynamoDB Global Table for the <code>games</code> table and use DynamoDB for the <code>users</code> and <code>games_played</code> tables</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "You are working as a Solutions Architect with a gaming company that has deployed an application that allows its customers to play games online. The application connects to an Amazon Aurora database, and the entire stack is currently deployed in the United States. The company has plans to expand to Europe and Asia for its operations. It needs the games table to be accessible globally but needs the users and games_played table to be regional only.\n\nHow would you implement this with minimal application refactoring?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815736,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</p>",
				"<p>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</p>",
				"<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</p>",
				"<p>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</p>"
			],
			"question": "<p>A media agency stores its re-creatable artifacts on Amazon S3 buckets. The artifacts are accessed by a large volume of users for the first few days and the frequency of access falls down drastically after a week. Although the artifacts would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the artifacts on S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible.</p>\n\n<p>As a Solutions Architect, can you suggest a way to lower the storage costs while fulfilling the business requirements</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days</strong> - S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. The minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 One Zone-IA.</p>\n\n<p>S3 One Zone-IA offers the same high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. S3 Storage Classes can be configured at the object level, and a single bucket can contain objects stored across S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can also use S3 Lifecycle policies to automatically transition objects between storage classes without any application changes.</p>\n\n<p>Constraints for Lifecycle storage class transitions:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q8-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p>Supported S3 lifecycle transitions:\n<img src=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/images/lifecycle-transitions-v2.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days</strong></p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) after 7 days</strong></p>\n\n<p>As mentioned earlier, the minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 One Zone-IA or S3 Standard-IA, so both these options are added as distractors.</p>\n\n<p><strong>Configure a lifecycle policy to transition the objects to Amazon S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. But, it costs more than S3 One Zone-IA because of the redundant storage across availability zones. As the data is re-creatable, so you don't need to incur this additional cost.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html</a></p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A media agency stores its re-creatable artifacts on Amazon S3 buckets. The artifacts are accessed by a large volume of users for the first few days and the frequency of access falls down drastically after a week. Although the artifacts would be accessed occasionally after the first week, but they must continue to be immediately accessible when required. The cost of maintaining all the artifacts on S3 storage is turning out to be very expensive and the agency is looking at reducing costs as much as possible.\n\nAs a Solutions Architect, can you suggest a way to lower the storage costs while fulfilling the business requirements",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815738,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p>Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. It also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning to identify threats more accurately.</p>\n\n<p><strong>Disable the service in the general settings</strong> - Disabling the service will delete all remaining data, including your findings and configurations before relinquishing the service permissions and resetting the service. So, this is the correct option for our use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Suspend the service in the general settings</strong> - You can stop Amazon GuardDuty from analyzing your data sources at any time by choosing to suspend the service in the general settings. This will immediately stop the service from analyzing data, but does not delete your existing findings or configurations.</p>\n\n<p><strong>De-register the service under services tab</strong> - This is a made-up option, used only as a distractor.</p>\n\n<p><strong>Raise a service request with Amazon to completely delete the data from all their backups</strong> - There is no need to create a service request as you can delete the existing findings by disabling the service.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/faqs/\">https://aws.amazon.com/guardduty/faqs/</a></p>\n",
			"question": "<p>A financial services company uses Amazon GuardDuty for analyzing their AWS account metadata to adhere to the compliance requirements mandated by the regulatory authorities. However, the company has now decided to stop using GuardDuty service. All the existing findings have to be deleted and cannot persist anywhere on AWS Cloud.</p>\n\n<p>Which of the following techniques will help the company meet this requirement?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Suspend the service in the general settings</p>",
				"<p>De-register the service under services tab</p>",
				"<p>Raise a service request with Amazon to completely delete the data from all their backups</p>",
				"<p>Disable the service in the general settings</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A financial services company uses Amazon GuardDuty for analyzing their AWS account metadata to adhere to the compliance requirements mandated by the regulatory authorities. However, the company has now decided to stop using GuardDuty service. All the existing findings have to be deleted and cannot persist anywhere on AWS Cloud.\n\nWhich of the following techniques will help the company meet this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815740,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>VPC Flow Logs, API Gateway logs, S3 access logs</p>",
				"<p>VPC Flow Logs, DNS logs, CloudTrail events</p>",
				"<p>ELB logs, DNS logs, CloudTrail events</p>",
				"<p>CloudFront logs, API Gateway logs, CloudTrail events</p>"
			],
			"question": "<p>The engineering team at a retail company uses EC2 instances, API Gateway, Amazon RDS, Elastic Load Balancer and CloudFront services. As per suggestions from a risk advisory group, all the development teams have been advised to ramp up the security of the applications by analyzing the data sources from AWS services that are part of their stack. The CTO at the company also wants the teams to assess the viability of using Amazon GuardDuty.</p>\n\n<p>As a Solutions Architect, can you identify the data sources that GuardDuty analyzes?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>VPC Flow Logs, DNS logs, CloudTrail events</strong> - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts, workloads, and data stored in Amazon S3. With the cloud, the collection and aggregation of account and network activities is simplified, but it can be time-consuming for security teams to continuously analyze event log data for potential threats. With GuardDuty, you now have an intelligent and cost-effective option for continuous threat detection in AWS. The service uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize potential threats.</p>\n\n<p>GuardDuty analyzes tens of billions of events across multiple AWS data sources, such as AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs.</p>\n\n<p>With a few clicks in the AWS Management Console, GuardDuty can be enabled with no software or hardware to deploy or maintain. By integrating with Amazon CloudWatch Events, GuardDuty alerts are actionable, easy to aggregate across multiple accounts, and straightforward to push into existing event management and workflow systems.</p>\n\n<p>How GuardDuty works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\">\nvia - <a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Flow Logs, API Gateway logs, S3 access logs</strong></p>\n\n<p><strong>ELB logs, DNS logs, CloudTrail events</strong></p>\n\n<p><strong>CloudFront logs, API Gateway logs, CloudTrail events</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "The engineering team at a retail company uses EC2 instances, API Gateway, Amazon RDS, Elastic Load Balancer and CloudFront services. As per suggestions from a risk advisory group, all the development teams have been advised to ramp up the security of the applications by analyzing the data sources from AWS services that are part of their stack. The CTO at the company also wants the teams to assess the viability of using Amazon GuardDuty.\n\nAs a Solutions Architect, can you identify the data sources that GuardDuty analyzes?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815742,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Kinesis Data Streams</strong> - Amazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream (for example, to perform counting, aggregation, and filtering).</p>\n\n<p>AWS recommends Amazon Kinesis Data Streams for use cases with requirements that are similar to the following:</p>\n\n<ol>\n<li>Routing related records to the same record processor (as in streaming MapReduce). For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor.</li>\n<li>Ordering of records. For example, you want to transfer log data from the application host to the processing/archival host while maintaining the order of log statements.</li>\n<li>Ability for multiple applications to consume the same stream concurrently. For example, you have one application that updates a real-time dashboard and another that archives data to Amazon Redshift. You want both applications to consume data from the same stream concurrently and independently.</li>\n<li>Ability to consume records in the same order a few hours later. For example, you have a billing application and an audit application that runs a few hours behind the billing application. Because Amazon Kinesis Data Streams stores data for up to 7 days, you can run the audit application up to 7 days behind the billing application.</li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS is a notification service and cannot be used for real-time processing of data.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. Since multiple applications need to consume the same data stream concurrently, Kinesis is a better choice when compared to the combination of SQS with SNS.</p>\n\n<p><strong>Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)</strong> - As discussed above, Kinesis is a better option for this use case in comparison to SQS. Also, SES does not fit this use-case. Hence, this option is an incorrect answer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A telecommunications company is looking at moving its real-time traffic analytics infrastructure to AWS Cloud. The company owns thousands of hardware devices like switches, routers, cables, and so on. The status of all these devices has to be fed into an analytics system for real-time processing. If a malfunction is detected, communication has to be initiated to the responsible team, to fix the hardware. Also, another application needs to read this same incoming data in parallel and analyze all the connecting lines that may go down because of the hardware failure.</p>\n\n<p>As a Solutions Architect, can you suggest the right solution to be used for this requirement?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Amazon Simple Notification Service (SNS)</p>",
				"<p>Amazon Simple Queue Service (SQS) with Amazon Simple Notification Service (SNS)</p>",
				"<p>Amazon Kinesis Data Streams</p>",
				"<p>Amazon Simple Queue Service (SQS) with Amazon Simple Email Service (Amazon SES)</p>"
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A telecommunications company is looking at moving its real-time traffic analytics infrastructure to AWS Cloud. The company owns thousands of hardware devices like switches, routers, cables, and so on. The status of all these devices has to be fed into an analytics system for real-time processing. If a malfunction is detected, communication has to be initiated to the responsible team, to fix the hardware. Also, another application needs to read this same incoming data in parallel and analyze all the connecting lines that may go down because of the hardware failure.\n\nAs a Solutions Architect, can you suggest the right solution to be used for this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815744,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Add a policy to the topic under the Finance account, where the <code>Principal</code> is defined as the Human Resources account</strong></p>\n\n<p>You should use AWS Identity and Access Management (IAM) to allow only appropriate users to read/publish to the Amazon SNS topic or to read/delete messages from an SQS queue.</p>\n\n<p>There are two ways to control access to a topic or queue:</p>\n\n<ol>\n<li><p>Add a policy to an IAM user or group. The simplest way to give users permissions to topics or queues is to create a group and add the appropriate policy to the group and then add users to that group. It's much easier to add and remove users from a group than to keep track of which policies you set on individual users.</p></li>\n<li><p>Add a policy to a topic or queue. If you want to give permissions to a topic or queue to another AWS account, the only way you can do that is by adding a policy that has as its principal the AWS account you want to give permissions to.</p></li>\n</ol>\n\n<p>You should use the first method for most cases (apply policies to groups and manage permissions for users by adding or removing the appropriate users to the groups). If you need to give permissions to a user in another account, you should use the second method.</p>\n\n<p>Example policy for reference:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q12-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/sns/latest/dg/subscribe-sqs-queue-to-sns-topic.html\">https://docs.aws.amazon.com/sns/latest/dg/subscribe-sqs-queue-to-sns-topic.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Add the appropriate policy to all the IAM users of the Human Resources account that need access to the topic</strong> - As mentioned earlier, for cross-account access, you need to add a policy that has as its <code>Principal</code> the AWS account you want to give permissions to. You cannot apply a policy to IAM users of another account to address this use-case.</p>\n\n<p><strong>Create a group and add all IAM users under it (users from both the accounts). Add the appropriate policy to allow access to the topic</strong> - A group cannot have users from another AWS account, so this option just acts as a distractor.</p>\n\n<p><strong>Add a policy to the topic under the Finance account, where the <code>Resource</code> is defined as the Human Resources account</strong> - For cross-account access, you need to add a policy that has as its <code>Principal</code> (rather than <code>Resource</code>) the AWS account you want to give permissions to.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/subscribe-sqs-queue-to-sns-topic.html\">https://docs.aws.amazon.com/sns/latest/dg/subscribe-sqs-queue-to-sns-topic.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Add the appropriate policy to all the IAM users of the Human Resources account that need access to the topic</p>",
				"<p>Create a group and add all IAM users under it (users from both the accounts). Add the appropriate policy to allow access to the topic</p>",
				"<p>Add a policy to the topic under the Finance account, where the <code>Resource</code> is defined as the Human Resources account</p>",
				"<p>Add a policy to the topic under the Finance account, where the <code>Principal</code> is defined as the Human Resources account</p>"
			],
			"relatedLectureIds": "",
			"question": "<p>A multi-national company uses different AWS accounts for their distinct business divisions. The communication between these divisions is heavily based on Amazon Simple Notification Service (SNS). For a particular use case, the AWS  account for the Human Resources division needs to have access to an Amazon SNS topic that falls under the AWS account of the Finance division.</p>\n\n<p>Which of the following represents the best solution for the given requirement?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A multi-national company uses different AWS accounts for their distinct business divisions. The communication between these divisions is heavily based on Amazon Simple Notification Service (SNS). For a particular use case, the AWS  account for the Human Resources division needs to have access to an Amazon SNS topic that falls under the AWS account of the Finance division.\n\nWhich of the following represents the best solution for the given requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815746,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>From the SES account, configure SES to push data to an Amazon SNS topic. Subscribe a Lambda function to this SNS topic. Lambda permissions need to be set up for both the accounts (to read from one and write to another). Set up Lambda to send data to Kinesis Data Streams, present in the second AWS account (Here, SES, SNS, and Lambda are present in SES account and Kinesis data streams is present in the second account)</strong> -</p>\n\n<p>To set up Amazon SES event notifications to an Amazon Kinesis data stream in another account, do the following:</p>\n\n<ol>\n<li><p>Start at the SES source. Update your configuration settings in SES to push to an Amazon Simple Notification Service (Amazon SNS) topic. The updated setting configures AWS Lambda to subscribe to the topic as a trigger.</p></li>\n<li><p>Subscribe to SNS.</p></li>\n<li><p>Set up cross-account access.</p></li>\n<li><p>Set up Lambda to send data to Amazon Kinesis Data Streams. Lambda then uses the code to grab the full event from the Amazon SNS topic. Lambda also assumes a role in another account to put the records into the data stream.</p></li>\n<li><p>Use the Amazon Kinesis Client Library (KCL) to process records in the stream.</p></li>\n</ol>\n\n<p>Incorrect options:</p>\n\n<p><strong>From the SES account, configure SES to push data to the Lambda function. Lambda permissions need to be set up for both the accounts (to read from one and write to another). Set up Lambda to send data to Kinesis Data Streams, present in the second AWS account (Here, SES, SNS, and Lambda are present in SES account and Kinesis data streams is present in the second account)</strong> - When Lambda is directly configured with SES, Amazon SES sends an event record to Lambda every time it receives an incoming message. However, it omits the body of the message. We will need the entire message for the current use case. Hence, this is not the correct option.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q13-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/ses/latest/DeveloperGuide/receiving-email-action-lambda-event.html\">https://docs.aws.amazon.com/ses/latest/DeveloperGuide/receiving-email-action-lambda-event.html</a></p>\n\n<p><strong>AWS Lambda cannot write across accounts. So, Amazon SNS has to be set up in both the accounts that need to communicate. SNS topic from account one will write data to SNS topic of the second account. In the second account, use Lambda as a subscriber to SNS. When Lambda fires, it will update Amazon Kinesis data stream with the information payload</strong> - AWS Lambda can write across accounts, and SNS topic cannot subscribe to another SNS topic. So this option just acts as a distractor.</p>\n\n<p><strong>Configure SES for multiple AWS accounts. This way, SES can directly write to Amazon Kinesis Data streams in the other account</strong> - AWS does not suggest using SES across accounts. However, there are certain conditions under which, it is allowed. Also, SES cannot directly write to Kinesis Data Streams. So, this cannot be the solution for the given use case.</p>\n\n<p>Exceptions where the use of multiple AWS Accounts with SES is allowed:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q13-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/messaging-and-targeting/can-i-use-multiple-aws-accounts-with-ses/\">https://aws.amazon.com/blogs/messaging-and-targeting/can-i-use-multiple-aws-accounts-with-ses/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-cross-account-ses-notifications/\">https://aws.amazon.com/premiumsupport/knowledge-center/kinesis-cross-account-ses-notifications/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/ses/latest/DeveloperGuide/receiving-email-action-lambda-event.html\">https://docs.aws.amazon.com/ses/latest/DeveloperGuide/receiving-email-action-lambda-event.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/messaging-and-targeting/can-i-use-multiple-aws-accounts-with-ses/\">https://aws.amazon.com/blogs/messaging-and-targeting/can-i-use-multiple-aws-accounts-with-ses/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>From the SES account, configure SES to push data to the Lambda function. Lambda permissions need to be set up for both the accounts (to read from one and write to another). Set up Lambda to send data to Kinesis Data Streams, present in the second AWS account (Here, SES, SNS, and Lambda are present in SES account and Kinesis data streams is present in the second account)</p>",
				"<p>AWS Lambda cannot write across accounts. So, Amazon SNS has to be set up in both the accounts that need to communicate. SNS topic from account one will write data to SNS topic of the second account. In the second account, use Lambda as a subscriber to SNS. When Lambda fires, it will update Amazon Kinesis data streams with the information payload</p>",
				"<p>From the SES account, configure SES to push data to an Amazon SNS topic. Subscribe a Lambda function to this SNS topic. Lambda permissions need to be set up for both the accounts (to read from one and write to another). Set up Lambda to send data to Kinesis Data Streams, present in the second AWS account (Here, SES, SNS, and Lambda are present in SES account and Kinesis data streams is present in the second account)</p>",
				"<p>Configure SES for multiple AWS accounts. This way, SES can directly write to Amazon Kinesis Data streams in the other account</p>"
			],
			"question": "<p>An IT company is working on multiple client projects and some of these projects span across multiple teams that use different AWS accounts. For one such project, two of the teams have a requirement to set up Amazon Simple Email Service (Amazon SES) event notification in one AWS account that needs to send data to an Amazon Kinesis data stream in another AWS account.</p>\n\n<p>As a Solutions Architect, which of the following would you recommend as the MOST optimal solution to address this requirement?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An IT company is working on multiple client projects and some of these projects span across multiple teams that use different AWS accounts. For one such project, two of the teams have a requirement to set up Amazon Simple Email Service (Amazon SES) event notification in one AWS account that needs to send data to an Amazon Kinesis data stream in another AWS account.\n\nAs a Solutions Architect, which of the following would you recommend as the MOST optimal solution to address this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815748,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A media company uses Amazon S3 buckets for storing their business-critical files. Initially, the development team used to provide bucket access to specific users within the same account. With changing business requirements, cross-account S3 access requirements are also growing. The company is looking for a granular solution that can offer user level as well as account-level access permissions for the data stored in S3 buckets.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 Bucket Policies</strong></p>\n\n<p>Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources.</p>\n\n<p>You can further restrict access to specific resources based on certain conditions. For example, you can restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean Conditions), a requester’s IP address (IP Address Condition), or based on the requester's client application (String Conditions). To identify these conditions, you use policy keys.</p>\n\n<p>Types of access control in S3:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q14-i1.jpg\">\nvia - <a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Identity and Access Management (IAM) policies</strong> - AWS IAM enables organizations with many employees to create and manage multiple users under a single AWS account. IAM policies are attached to the users, enabling centralized control of permissions for users under your AWS Account to access buckets or objects. With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources. So, this is not the right choice for the current requirement.</p>\n\n<p><strong>Use Access Control Lists (ACLs)</strong> - Within Amazon S3, you can use ACLs to give read or write access on buckets or objects to groups of users. With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. So, this is not the right choice for the current requirement.</p>\n\n<p><strong>Use Security Groups</strong> - A security group acts as a virtual firewall for EC2 instances to control incoming and outgoing traffic. S3 does not support Security Groups, this option just acts as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\">https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf</a></p>\n",
			"answers": [
				"<p>Use Amazon S3 Bucket Policies</p>",
				"<p>Use Identity and Access Management (IAM) policies</p>",
				"<p>Use Access Control Lists (ACLs)</p>",
				"<p>Use Security Groups</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A media company uses Amazon S3 buckets for storing their business-critical files. Initially, the development team used to provide bucket access to specific users within the same account. With changing business requirements, cross-account S3 access requirements are also growing. The company is looking for a granular solution that can offer user level as well as account-level access permissions for the data stored in S3 buckets.\n\nAs a Solutions Architect, which of the following would you suggest as the MOST optimized way of controlling access for this use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815750,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>When objects are uploaded to S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. It is an upload error that can be fixed by providing manual access from AWS console</p>",
				"<p>The owner of an S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress</p>",
				"<p>When two different AWS accounts are accessing an S3 bucket, both the accounts need to share the bucket policies, explicitly defining the actions possible for each account. An erroneous policy can lead to such permission failures</p>",
				"<p>By default, an S3 object is owned by the AWS account that uploaded it. So the S3 bucket owner will not implicitly have access to the objects written by the Redshift cluster</p>"
			],
			"question": "<p>An IT company is working on a project that uses two separate AWS accounts for accessing different AWS services. The team has just configured an Amazon S3 bucket in the first AWS account for writing data from the Amazon Redshift cluster present in the second AWS account. The developer has noticed that the files created in the S3 bucket using the UNLOAD command from the Redshift cluster are not accessible to the S3 bucket owner.</p>\n\n<p>What could be the reason for this denial of permission for resources created in the same AWS account?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>By default, an S3 object is owned by the AWS account that uploaded it. So the S3 bucket owner will not implicitly have access to the objects written by Redshift cluster</strong> - By default, an S3 object is owned by the AWS account that uploaded it. This is true even when the bucket is owned by another account. Because the Amazon Redshift data files from the UNLOAD command were put into your bucket by another account, you (the bucket owner) don't have default permission to access those files.</p>\n\n<p>To get access to the data files, an AWS Identity and Access Management (IAM) role with cross-account permissions must run the UNLOAD command again. Follow these steps to set up the Amazon Redshift cluster with cross-account permissions to the bucket:</p>\n\n<ol>\n<li><p>From the account of the S3 bucket, create an IAM role (Bucket Role) with permissions to the bucket.</p></li>\n<li><p>From the account of the Amazon Redshift cluster, create another IAM role (Cluster Role) with permissions to assume the Bucket Role.</p></li>\n<li><p>Update the Bucket Role to grant bucket access and create a trust relationship with the Cluster Role.</p></li>\n<li><p>From the Amazon Redshift cluster, run the UNLOAD command using the Cluster Role and Bucket Role.</p></li>\n</ol>\n\n<p>This solution doesn't apply to Amazon Redshift clusters or S3 buckets that use server-side encryption with AWS Key Management Service (AWS KMS).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>When objects are uploaded to S3 bucket from a different AWS account, the S3 bucket owner will get implicit permissions to access these objects. It is an upload error that can be fixed by providing manual access from AWS console</strong> - By default, an S3 object is owned by the AWS account that uploaded it. So, the bucket owner will not have any default permissions on the objects.</p>\n\n<p><strong>The owner of an S3 bucket has implicit access to all objects in his bucket. Permissions are set on objects after they are completely copied to the target location. Since the owner is unable to access the uploaded files, the write operation may be still in progress</strong> - This is an incorrect statement, given only as a distractor.</p>\n\n<p><strong>When two different AWS accounts are accessing an S3 bucket, both the accounts need to share the bucket policies, explicitly defining the actions possible for each account. An erroneous policy can lead to such permission failures</strong> - This is an incorrect statement, given only as a distractor.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-redshift-unload/</a></p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An IT company is working on a project that uses two separate AWS accounts for accessing different AWS services. The team has just configured an Amazon S3 bucket in the first AWS account for writing data from the Amazon Redshift cluster present in the second AWS account. The developer has noticed that the files created in the S3 bucket using the UNLOAD command from the Redshift cluster are not accessible to the S3 bucket owner.\n\nWhat could be the reason for this denial of permission for resources created in the same AWS account?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815752,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A retail company has built their AWS solution using serverless architecture by leveraging AWS Lambda and Amazon S3. The development team has a requirement to implement AWS Lambda across AWS accounts. The requirement entails using a Lambda function with an IAM role from an AWS account A to access an Amazon S3 bucket in AWS account B.</p>\n\n<p>As a Solutions Architect, which of the following will you recommend as the BEST solution to meet this requirement?</p>\n",
			"answers": [
				"<p>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role. Verify that the bucket policy grants access to the Lambda function's execution role</p>",
				"<p>AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda</p>",
				"<p>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role</p>",
				"<p>The S3 bucket owner can delegate permissions to users in the other AWS account</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role. Verify that the bucket policy grants access to the Lambda function's execution role</strong></p>\n\n<p>If the IAM role that you create for the Lambda function is in the same AWS account as the bucket, then you don't need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Instead, you can grant the permissions on the IAM role and then verify that the bucket policy doesn't explicitly deny access to the Lambda function role. If the IAM role and the bucket are in different accounts, then you need to grant Amazon S3 permissions on both the IAM role and the bucket policy. Therefore, this is the right way of giving access to AWS Lambda for the given use-case.</p>\n\n<p>Complete list of steps to be followed:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q16-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Lambda cannot access resources across AWS accounts. Use Identity federation to work around this limitation of Lambda</strong> - This is an incorrect statement, used only as a distractor.</p>\n\n<p><strong>Create an AWS Identity and Access Management (IAM) role for the Lambda function that also grants access to the S3 bucket. Set the IAM role as the Lambda function's execution role</strong> - When the execution role of Lambda and S3 bucket to be accessed are from different accounts, then you need to grant S3 bucket access permissions to the IAM role and also ensure that the bucket policy grants access to the Lambda function's execution role.</p>\n\n<p><strong>The S3 bucket owner can delegate permissions to users in the other AWS account</strong> - Bucket owner account can delegate permissions to users in its own account, but it cannot delegate permissions to other AWS accounts, since cross-account delegation is not supported.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail company has built their AWS solution using serverless architecture by leveraging AWS Lambda and Amazon S3. The development team has a requirement to implement AWS Lambda across AWS accounts. The requirement entails using a Lambda function with an IAM role from an AWS account A to access an Amazon S3 bucket in AWS account B.\n\nAs a Solutions Architect, which of the following will you recommend as the BEST solution to meet this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815754,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</p>",
				"<p>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</p>",
				"<p>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</p>",
				"<p>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your Application Load Balancer</strong></p>\n\n<p>Application Load Balancer can be used to securely authenticate users for accessing your applications. This enables you to offload the work of authenticating users to your load balancer so that your applications can focus on their business logic. You can use Cognito User Pools to authenticate users through well-known social IdPs, such as Amazon, Facebook, or Google, through the user pools supported by Amazon Cognito or through corporate identities, using SAML, LDAP, or Microsoft AD, through the user pools supported by Amazon Cognito. You configure user authentication by creating an authenticate action for one or more listener rules.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q17-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p>Exam Alert:</p>\n\n<p>Please review the following note to understand the differences between Cognito User Pools and Cognito Identity Pools:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q17-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your Application Load Balancer</strong> - There is no such thing as using Cognito Authentication via Cognito Identity Pools for managing user authentication for the application. Application-specific user authentication can be provided via Cognito User Pools. Amazon Cognito identity pools provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and received a token.</p>\n\n<p><strong>Use Cognito Authentication via Cognito User Pools for your CloudFront distribution</strong> - You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. This involves additional development effort, so this option is not the best fit for the given use-case.</p>\n\n<p><strong>Use Cognito Authentication via Cognito Identity Pools for your CloudFront distribution</strong> - You cannot use Cognito Identity Pools for managing user authentication, so this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/listener-authenticate-users.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/\">https://aws.amazon.com/blogs/networking-and-content-delivery/authorizationedge-using-cookies-protect-your-amazon-cloudfront-content-from-being-downloaded-by-unauthenticated-users/</a></p>\n",
			"question": "<p>A social media company manages its flagship application on an EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The engineering team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.</p>\n\n<p>As a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A social media company manages its flagship application on an EC2 server fleet running behind an Application Load Balancer and the traffic is fronted by a CloudFront distribution. The engineering team wants to decouple the user authentication process for the application so that the application servers can just focus on the business logic.\n\nAs a Solutions Architect, which of the following solutions would you recommend to the development team so that it requires minimal development effort?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815756,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:</p>\n\n<p><strong>The health check grace period for the instance has not expired</strong> - Amazon EC2 Auto Scaling doesn't terminate an instance that came into service based on EC2 status checks and ELB health checks until the health check grace period expires.</p>\n\n<p>More on Health check grace period:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q18-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html#health-check-grace-period</a></p>\n\n<p><strong>The instance maybe in Impaired status</strong> - Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status. Instead, Amazon EC2 Auto Scaling waits a few minutes for the instance to recover. Amazon EC2 Auto Scaling might also delay or not terminate instances that fail to report data for status checks. This usually happens when there is insufficient data for the status check metrics in Amazon CloudWatch.</p>\n\n<p><strong>The instance has failed the ELB health check status</strong> - By default, Amazon EC2 Auto Scaling doesn't use the results of ELB health checks to determine an instance's health status when the group's health check configuration is set to EC2. As a result, Amazon EC2 Auto Scaling doesn't terminate instances that fail ELB health checks. If an instance's status is OutofService on the ELB console, but the instance's status is Healthy on the Amazon EC2 Auto Scaling console, confirm that the health check type is set to ELB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EC2 instance could be a spot instance type, which cannot be terminated by ASG</strong> - This is an incorrect statement. Amazon EC2 Auto Scaling terminates Spot instances when capacity is no longer available or the Spot price exceeds your maximum price.</p>\n\n<p><strong>A user might have updated the configuration of ASG and increased the minimum number of instances forcing ASG to keep all instances alive</strong> - This statement is incorrect. If the configuration is updated and ASG needs more number of instances, ASG will launch new, healthy instances and does not keep unhealthy ones alive.</p>\n\n<p><strong>A custom health check might have failed. ASG does not terminate instances that are set unhealthy by custom checks</strong> - This statement is incorrect. You can define custom health checks in Amazon EC2 Auto Scaling. When a custom health check determines that an instance is unhealthy, the check manually triggers SetInstanceHealth and then sets the instance's state to Unhealthy. Amazon EC2 Auto Scaling then terminates the unhealthy instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-instance-how-terminated/</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>A user might have updated the configuration of ASG and increased the minimum number of instances forcing ASG to keep all instances alive</p>",
				"<p>The EC2 instance could be a spot instance type, which cannot be terminated by ASG</p>",
				"<p>The health check grace period for the instance has not expired</p>",
				"<p>The instance maybe in Impaired status</p>",
				"<p>The instance has failed the ELB health check status</p>",
				"<p>A custom health check might have failed. ASG does not terminate instances that are set unhealthy by custom checks</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"question": "<p>The engineering team at a logistics company is working on a shipments application deployed on a fleet of Amazon EC2 instances behind an Auto Scaling Group (ASG). While configuring new changes for an upcoming release, a team member has noticed that the ASG is not terminating an unhealthy instance.</p>\n\n<p>As a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)</p>\n"
		},
		"correct_response": [
			"c",
			"d",
			"e"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The engineering team at a logistics company is working on a shipments application deployed on a fleet of Amazon EC2 instances behind an Auto Scaling Group (ASG). While configuring new changes for an upcoming release, a team member has noticed that the ASG is not terminating an unhealthy instance.\n\nAs a Solutions Architect, which of the following options would you suggest to troubleshoot the issue? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815758,
		"assessment_type": "multi-select",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>By default, user data is executed every time an EC2 instance is re-started</p>",
				"<p>When an instance is running, you can update user data by using root user credentials</p>",
				"<p>By default, scripts entered as user data are executed with root user privileges</p>",
				"<p>By default, user data runs only during the boot cycle when you first launch an instance</p>",
				"<p>By default, scripts entered as user data do not have root user privileges for executing</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"question": "<p>A financial services firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The engineering team has been told to use the user data feature of EC2 instances to ensure new instances are ready for operations.</p>\n\n<p>Which of the following are true about the EC2 user data configuration? (Select two)</p>\n",
			"explanation": "<p>Correct options:</p>\n\n<p>User Data is generally used to perform common automated configuration tasks and even run scripts after the instance starts. When you launch an instance in Amazon EC2, you can pass two types of user data - shell scripts and cloud-init directives. You can also pass this data into the launch wizard as plain text or as a file.</p>\n\n<p><strong>By default, scripts entered as user data are executed with root user privileges</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script. Any files you create will be owned by root; if you need non-root users to have file access, you should modify the permissions accordingly in the script.</p>\n\n<p><strong>By default, user data runs only during the boot cycle when you first launch an instance</strong> - By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>By default, user data is executed every time an EC2 instance is re-started</strong> - As discussed above, this is not a default configuration of the system. But, can be achieved by explicitly configuring the instance.</p>\n\n<p><strong>When an instance is running, you can update user data by using root user credentials</strong> - You can't change the user data if the instance is running (even by using root user credentials), but you can view it.</p>\n\n<p><strong>By default, scripts entered as user data do not have root user privileges for executing</strong> - Scripts entered as user data are executed as the root user, hence do not need the sudo command in the script.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n"
		},
		"correct_response": [
			"c",
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A financial services firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The engineering team has been told to use the user data feature of EC2 instances to ensure new instances are ready for operations.\n\nWhich of the following are true about the EC2 user data configuration? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815760,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>DNS hostnames and DNS resolution are required settings for private hosted zones</strong> - DNS hostnames and DNS resolution are required settings for private hosted zones. DNS queries for private hosted zones can be resolved by the Amazon-provided VPC DNS server only. As a result, these options must be enabled for your private hosted zone to work.</p>\n\n<p>DNS hostnames: For non-default virtual private clouds that aren't created using the Amazon VPC wizard, this option is disabled by default. If you create a private hosted zone for a domain and create records in the zone without enabling DNS hostnames, private hosted zones aren't enabled. To use a private hosted zone, this option must be enabled.</p>\n\n<p>DNS resolution: Private hosted zones accept DNS queries only from a VPC DNS server. The IP address of the VPC DNS server is the reserved IP address at the base of the VPC IPv4 network range plus two. Enabling DNS resolution allows you to use the VPC DNS server as a Resolver for performing DNS resolution. Keep this option disabled if you're using a custom DNS server in the DHCP Options set, and you're not using a private hosted zone.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>You might have private and public hosted zones that have overlapping namespaces</strong> - If you have private and public hosted zones that have overlapping namespaces, such as example.com and accounting.example.com, Resolver routes traffic based on the most specific match. It won't result in any error, hence this option is wrong.</p>\n\n<p><strong>Name server (NS) record and Start Of Authority (SOA) records are created with wrong configurations</strong> - When you create a hosted zone, Amazon Route 53 automatically creates a name server (NS) record and a start of authority (SOA) record for the zone for public hosted zone. The issue is about the private hosted zone, hence this is an incorrect option.</p>\n\n<p><strong>This error may happen when there is a private hosted zone and a Resolver rule that routes traffic to your network for the same domain name, resulting in ambiguity over the route to be taken</strong> - If you have a private hosted zone (example.com) and a Resolver rule that routes traffic to your network for the same domain name, the Resolver rule takes precedence. It doesn't result in any error.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-enable-private-hosted-zone/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-enable-private-hosted-zone/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-considerations.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-public-considerations.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-public-considerations.html</a></p>\n",
			"answers": [
				"<p>You might have private and public hosted zones that have overlapping namespaces</p>",
				"<p>Name server (NS) record and Start Of Authority (SOA) records are created with wrong configurations</p>",
				"<p>This error may happen when there is a private hosted zone and a Resolver rule that routes traffic to your network for the same domain name, resulting in ambiguity over the route to be taken</p>",
				"<p>DNS hostnames and DNS resolution are required settings for private hosted zones</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A large retail company uses separate VPCs with different configurations for each of their lines of business. As part of the specific security requirement, an administrator had created a private hosted zone and associated it with the required virtual private cloud (VPC). However, the domain names remain unresolved thereby resulting in errors.</p>\n\n<p>As a Solutions Architect, can you identify the Amazon VPC options to configure, to get the private hosted zone to work?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A large retail company uses separate VPCs with different configurations for each of their lines of business. As part of the specific security requirement, an administrator had created a private hosted zone and associated it with the required virtual private cloud (VPC). However, the domain names remain unresolved thereby resulting in errors.\n\nAs a Solutions Architect, can you identify the Amazon VPC options to configure, to get the private hosted zone to work?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815762,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Centralized VPC Endpoints for connecting with multiple VPCs, also known as shared services VPC</strong> - A VPC endpoint allows you to privately connect your VPC to supported AWS services without requiring an Internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Endpoints are virtual devices that are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>VPC endpoints enable you to reduce data transfer charges resulting from network communication between private VPC resources (such as Amazon Elastic Cloud Compute—or EC2—instances) and AWS Services (such as Amazon Quantum Ledger Database, or QLDB). Without VPC endpoints configured, communications that originate from within a VPC destined for public AWS services must egress AWS to the public Internet in order to access AWS services. This network path incurs outbound data transfer charges. Data transfer charges for traffic egressing from Amazon EC2 to the Internet vary based on volume. With VPC endpoints configured, communication between your VPC and the associated AWS service does not leave the Amazon network. If your workload requires you to transfer significant volumes of data between your VPC and AWS, you can reduce costs by leveraging VPC endpoints.</p>\n\n<p>In larger multi-account AWS environments, network design can vary considerably. Consider an organization that has built a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts, perhaps to facilitate network isolation or to enable delegated network administration. When deploying distributed architectures such as this, a popular approach is to build a \"shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost.</p>\n\n<p>Centralized VPC Endpoints (multiple VPCs):\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q21-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/\">https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Transit VPC to reduce cost and share the resources across VPCs</strong> - Transit VPC uses customer-managed Amazon Elastic Compute Cloud (Amazon EC2) VPN instances in a dedicated transit VPC with an Internet gateway. This design requires the customer to deploy, configure, and manage EC2-based VPN appliances, which will result in additional EC2, and potentially third-party product and licensing charges. Note that this design will generate additional data transfer charges for traffic traversing the transit VPC: data is charged when it is sent from a spoke VPC to the transit VPC, and again from the transit VPC to the on-premises network or a different AWS Region. Transit VPC is not the right choice here.</p>\n\n<p>More on Transit VPC:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q21-i2.jpg\">\nvia - <a href=\"https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\">https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf</a></p>\n\n<p><strong>Use Fully meshed VPC Peers</strong> - This approach creates multiple peering connections to facilitate the sharing of information between resources in different VPCs. This design connects multiple VPCs in a fully meshed configuration, with peering connections between each pair of VPCs. With this configuration, each VPC has access to the resources in all other VPCs. Each peering connection requires modifications to all the other VPCs’ route tables and, as the number of VPCs grows, this can be difficult to maintain. And keep in mind that AWS recommends a maximum of 125 peering connections per VPC. It's complex to manage and isn't a right fit for the current scenario.</p>\n\n<p>More on Fully meshed VPC Peers:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q21-i3.jpg\">\nvia - <a href=\"https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\">https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf</a></p>\n\n<p><strong>Use VPCs connected with AWS Direct Connect</strong> - This approach is a good alternative for customers who need to connect a high number of VPCs to a central VPC or on-premises resources, or who already have an AWS Direct Connect connection in place. This design also offers customers the ability to incorporate transitive routing into their network design. For example, if VPC A and VPC B are both connected to an on-premises network using AWS Direct Connect connections, then the two VPCs can be connected to each other via AWS Direct Connect. Direct Connect requires physical cables and takes about a month for setting up, this is not an ideal solution for the given scenario.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/\">https://aws.amazon.com/blogs/architecture/reduce-cost-and-increase-security-with-amazon-vpc-endpoints/</a></p>\n\n<p><a href=\"https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf\">https://d0.awsstatic.com/aws-answers/AWS_Single_Region_Multi_VPC_Connectivity.pdf</a></p>\n",
			"question": "<p>An e-commerce company has created a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts to facilitate network isolation and delegate network administration. The company is looking for a cost-effective, easy, and secure way of maintaining this distributed architecture.</p>\n\n<p>As a Solutions Architect, which of the following options would you recommend to address the given requirement?</p>\n",
			"answers": [
				"<p>Use Centralized VPC Endpoints for connecting with multiple VPCs, also known as shared services VPC</p>",
				"<p>Use Transit VPC to reduce cost and share the resources across VPCs</p>",
				"<p>Use Fully meshed VPC Peers</p>",
				"<p>Use VPCs connected with AWS Direct Connect</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An e-commerce company has created a hub-and-spoke network with AWS Transit Gateway. VPCs have been provisioned into multiple AWS accounts to facilitate network isolation and delegate network administration. The company is looking for a cost-effective, easy, and secure way of maintaining this distributed architecture.\n\nAs a Solutions Architect, which of the following options would you recommend to address the given requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815764,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A retail company manages its flagship application on AWS Cloud and their engineering team runs several deployments as part of phased rollouts. The digital marketing team wants to test its blue-green deployment on the customer base in the next couple of days. Most of the customers use mobile phones which are prone to DNS caching. The company has only two days left for the annual Thanksgiving sale to commence.</p>\n\n<p>As a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use Route 53 weighted routing to spread traffic across different deployments</p>",
				"<p>Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment</p>",
				"<p>Use Elastic Load Balancer to distribute traffic across deployments</p>",
				"<p>Use AWS CodeDeploy deployment options to choose the right deployment</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p>Blue/green deployment is a technique for releasing applications by shifting traffic between two identical environments running different versions of the application: \"Blue\" is the currently running version and \"green\" the new version. This type of deployment allows you to test features in the green environment without impacting the currently running version of your application. When you’re satisfied that the green version is working properly, you can gradually reroute the traffic from the old blue environment to the new green environment. Blue/green deployments can mitigate common risks associated with deploying software, such as downtime and rollback capability.</p>\n\n<p><strong>Use AWS Global Accelerator to distribute a portion of traffic to a particular deployment</strong> - AWS Global Accelerator is a network layer service that directs traffic to optimal endpoints over the AWS global network, this improves the availability and performance of your internet applications. It provides two static anycast IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers, Elastic IP addresses or Amazon EC2 instances, in a single or in multiple AWS regions.</p>\n\n<p>AWS Global Accelerator uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group, and traffic dials to control the percentage of traffic that is directed to an endpoint group (an AWS region where your application is deployed).</p>\n\n<p>While relying on the DNS service is a great option for blue/green deployments, it may not fit use-cases that require a fast and controlled transition of the traffic. Some client devices and internet resolvers cache DNS answers for long periods; this DNS feature improves the efficiency of the DNS service as it reduces the DNS traffic across the Internet, and serves as a resiliency technique by preventing authoritative name-server overloads. The downside of this in blue/green deployments is that you don’t know how long it will take before all of your users receive updated IP addresses when you update a record, change your routing preference or when there is an application failure.</p>\n\n<p>With AWS Global Accelerator, you can shift traffic gradually or all at once between the blue and the green environment and vice-versa without being subject to DNS caching on client devices and internet resolvers, traffic dials and endpoint weights changes are effective within seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Route 53 weighted routing to spread traffic across different deployments</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. As discussed earlier, DNS caching is a negative behavior for this use case and hence Route 53 is not a good option.</p>\n\n<p><strong>Use Elastic Load Balancer to distribute traffic across deployments</strong> - An ELB can distribute traffic across healthy instances. It is not possible to manually set traffic distribution for Blue/Green deployments by using an ELB.</p>\n\n<p><strong>Use AWS CodeDeploy deployment options to choose the right deployment</strong> - In CodeDeploy, a deployment is the process, and the components involved in the process, of installing content on one or more instances. This content can consist of code, web and configuration files, executables, packages, scripts, and so on. CodeDeploy deploys content that is stored in a source repository, according to the configuration rules you specify. Blue/Green deployment is one of the deployment types that CodeDeploy supports. Traffic distribution across instances, in real-time, is not a feature of CodeDeploy.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments\">https://aws.amazon.com/blogs/networking-and-content-delivery/using-aws-global-accelerator-to-achieve-blue-green-deployments</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A retail company manages its flagship application on AWS Cloud and their engineering team runs several deployments as part of phased rollouts. The digital marketing team wants to test its blue-green deployment on the customer base in the next couple of days. Most of the customers use mobile phones which are prone to DNS caching. The company has only two days left for the annual Thanksgiving sale to commence.\n\nAs a Solutions Architect, which of the following options would you recommend to test the deployment on as many users as possible in the given time frame",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815766,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q23-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q23-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n",
			"question": "<p>A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?</p>\n",
			"answers": [
				"<p>Dedicated Instances</p>",
				"<p>Spot Instances</p>",
				"<p>Dedicated Hosts</p>",
				"<p>On-Demand Instances</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A health-care solutions company wants to run their applications on single-tenant hardware to meet regulatory guidelines.\n\nWhich of the following is the MOST cost-effective way of isolating their Amazon EC2 instances to a single tenant?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815768,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Cognito User Pools</strong> - A user pool is a user directory in Amazon Cognito. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers. Whether your users sign-in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).</p>\n\n<p>User pools provide:\n1. Sign-up and sign-in services.\n2. A built-in, customizable web UI to sign in users.\n3. Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.\n4. User directory management and user profiles.\n5. Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.\n6. Customized workflows and user migration through AWS Lambda triggers.</p>\n\n<p>After successfully authenticating a user, Amazon Cognito issues JSON web tokens (JWT) that you can use to secure and authorize access to your own APIs, or exchange for AWS credentials.</p>\n\n<p>Amazon Cognito User Pools:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q24-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html\">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use AWS_IAM authorization</strong> - For consumers who currently are located within your AWS environment or have the means to retrieve AWS Identity and Access Management (IAM) temporary credentials to access your environment, you can use AWS_IAM authorization and add least-privileged permissions to the respective IAM role to securely invoke your API. API Gateway API Keys is not a security mechanism and should not be used for authorization unless it’s a public API. It should be used primarily to track a consumer’s usage across your API.</p>\n\n<p><strong>Use PI Gateway Lambda authorizer</strong> - If you have an existing Identity Provider (IdP), you can use an API Gateway Lambda authorizer to invoke a Lambda function to authenticate/validate a given user against your IdP. You can use a Lambda authorizer for custom validation logic based on identity metadata.</p>\n\n<p>A Lambda authorizer can send additional information derived from a bearer token or request context values to your backend service. For example, the authorizer can return a map containing user IDs, user names, and scope. By using Lambda authorizers, your backend does not need to map authorization tokens to user-centric data, allowing you to limit the exposure of such information to just the authorization function.</p>\n\n<p>When using Lambda authorizers, AWS strictly advises against passing credentials or any sort of sensitive data via query string parameters or headers, so this is not as secure as using Cognito User Pools.</p>\n\n<p><strong>Use Amazon Cognito Identity Pool</strong> - The two main components of Amazon Cognito are user pools and identity pools. Identity pools provide AWS credentials to grant your users access to other AWS services. To enable users in your user pool to access AWS resources, you can configure an identity pool to exchange user pool tokens for AWS credentials. So, identity pools aren't an authentication mechanism in themselves and hence aren't a choice for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html\">https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/identity-and-access-management.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>An IT company is looking at securing their APIs using AWS best practices. APIs are often targeted by attackers because of the operations that they can perform and the valuable data they can provide. The company has hired you as a Solutions Architect to advise the company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the API Gateway.</p>\n\n<p>Which of the following would you suggest as the most secure solution for the given use-case?</p>\n",
			"answers": [
				"<p>Use AWS_IAM authorization</p>",
				"<p>Use Amazon Cognito User Pools</p>",
				"<p>Use API Gateway Lambda authorizer</p>",
				"<p>Use Amazon Cognito Identity Pool</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An IT company is looking at securing their APIs using AWS best practices. APIs are often targeted by attackers because of the operations that they can perform and the valuable data they can provide. The company has hired you as a Solutions Architect to advise the company on the various authentication/authorization mechanisms that AWS offers to authorize an API call within the API Gateway.\n\nWhich of the following would you suggest as the most secure solution for the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815770,
		"assessment_type": "multi-select",
		"prompt": {
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"question": "<p>A financial services company is looking at moving their on-premises infrastructure to AWS Cloud and leverage the serverless architecture. As part of this process, their engineering team has been studying various best practices for developing a serverless solution. They intend to use AWS Lambda extensively and want to focus on the key points to consider when using Lambda as a backbone for this architecture.</p>\n\n<p>As a Solutions Architect, which of the following options would you recommend for this requirement? (Select three)</p>\n",
			"answers": [
				"<p>By default, Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once a Lambda function is VPC-enabled, it will need a route through a NAT gateway in a public subnet to access public resources</p>",
				"<p>Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of Lambda functions</p>",
				"<p>The bigger your deployment package, the slower your Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual Lambda package</p>",
				"<p>Since Lambda functions can scale extremely quickly, it's a good idea to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold</p>",
				"<p>If you intend to reuse code in more than one Lambda function, you should consider creating a Lambda Layer for the reusable code</p>",
				"<p>Serverless architecture and containers complement each other and you should leverage Docker containers within the Lambda functions</p>"
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>By default, Lambda functions always operate from an AWS-owned VPC and hence have access to any public internet address or public AWS APIs. Once a Lambda function is VPC-enabled, it will need a route through a NAT gateway in a public subnet to access public resources</strong> - Lambda functions always operate from an AWS-owned VPC. By default, your function has the full ability to make network requests to any public internet address — this includes access to any of the public AWS APIs. For example, your function can interact with AWS DynamoDB APIs to PutItem or Query for records. You should only enable your functions for VPC access when you need to interact with a private resource located in a private subnet. An RDS instance is a good example.</p>\n\n<p>Once your function is VPC-enabled, all network traffic from your function is subject to the routing rules of your VPC/Subnet. If your function needs to interact with a public resource, you will need a route through a NAT gateway in a public subnet.</p>\n\n<p>When to VPC-Enable a Lambda Function:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q25-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\">https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/</a></p>\n\n<p><strong>Since Lambda functions can scale extremely quickly, its a good idea to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds the expected threshold</strong> - Since Lambda functions can scale extremely quickly, this means you should have controls in place to notify you when you have a spike in concurrency. A good idea is to deploy a CloudWatch Alarm that notifies your team when function metrics such as ConcurrentExecutions or Invocations exceeds your threshold. You should create an AWS Budget so you can monitor costs on a daily basis.</p>\n\n<p><strong>If you intend to reuse code in more than one Lambda function, you should consider creating a Lambda Layer for the reusable code</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package. Layers let you keep your deployment package small, which makes development easier. A function can use up to 5 layers at a time.</p>\n\n<p>You can create layers, or use layers published by AWS and other AWS customers. Layers support resource-based policies for granting layer usage permissions to specific AWS accounts, AWS Organizations, or all accounts. The total unzipped size of the function and all layers can't exceed the unzipped deployment package size limit of 250 MB.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Lambda allocates compute power in proportion to the memory you allocate to your function. AWS, thus recommends to over provision your function time out settings for the proper performance of Lambda functions</strong> - Lambda allocates compute power in proportion to the memory you allocate to your function. This means you can over-provision memory to run your functions faster and potentially reduce your costs. However, AWS recommends that you should not over provision your function time out settings. Always understand your code performance and set a function time out accordingly. Overprovisioning function timeout often results in Lambda functions running longer than expected and unexpected costs.</p>\n\n<p><strong>The bigger your deployment package, the slower your Lambda function will cold-start. Hence, AWS suggests packaging dependencies as a separate package from the actual Lambda package</strong> - This statement is incorrect and acts as a distractor. All the dependencies are also packaged into the single Lambda deployment package.</p>\n\n<p><strong>Serverless architecture and containers complement each other and you should leverage Docker containers within the Lambda functions</strong> - This statement is incorrect. AWS Lambda does not support running Docker containers.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/\">https://aws.amazon.com/blogs/architecture/best-practices-for-developing-on-aws-lambda/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html</a></p>\n"
		},
		"correct_response": [
			"a",
			"d",
			"e"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A financial services company is looking at moving their on-premises infrastructure to AWS Cloud and leverage the serverless architecture. As part of this process, their engineering team has been studying various best practices for developing a serverless solution. They intend to use AWS Lambda extensively and want to focus on the key points to consider when using Lambda as a backbone for this architecture.\n\nAs a Solutions Architect, which of the following options would you recommend for this requirement? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815772,
		"assessment_type": "multi-select",
		"prompt": {
			"answers": [
				"<p>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</p>",
				"<p>The route for the health check is misconfigured</p>",
				"<p>The EBS volumes have been improperly mounted</p>",
				"<p>Your web-app has a runtime that is not supported by the Application Load Balancer</p>",
				"<p>You need to attach Elastic IP to the EC2 instances</p>"
			],
			"question": "<p>The lead engineer at a social media company has created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when he enters the IP address of the EC2 instances in the web browser, he can access the website.</p>\n\n<p>What could be the reason the instances are being marked as unhealthy? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct options</p>\n\n<p><strong>The security group of the EC2 instance does not allow for traffic from the security group of the Application Load Balancer</strong></p>\n\n<p><strong>The route for the health check is misconfigured</strong></p>\n\n<p>You must ensure that your load balancer can communicate with registered targets on both the listener port and the health check port. Whenever you add a listener to your load balancer or update the health check port for a target group used by the load balancer to route requests, you must verify that the security groups associated with the load balancer allow traffic on the new port in both directions.</p>\n\n<p>Application Load Balancer Configuration for Security Groups and Health Check Routes:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q26-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The EBS volumes have been improperly mounted</strong> - You can access the website using the IP address which means there is no issue with the EBS volumes. So this option is not correct.</p>\n\n<p><strong>Your web-app has a runtime that is not supported by the Application Load Balancer</strong> - There is no connection between a web app runtime and the application load balancer. This option has been added as a distractor.</p>\n\n<p><strong>You need to attach Elastic IP to the EC2 instances</strong> - This option is a distractor as Elastic IPs do not need to be assigned to EC2 instances while using an Application Load Balancer.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-update-security-groups.html</a></p>\n"
		},
		"correct_response": [
			"a",
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "The lead engineer at a social media company has created an Elastic Load Balancer that has marked all the EC2 instances in the target group as unhealthy. Surprisingly, when he enters the IP address of the EC2 instances in the web browser, he can access the website.\n\nWhat could be the reason the instances are being marked as unhealthy? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815774,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A data analytics company uses custom data-integration services to produce data and log files in S3 buckets. As part of the process re-engineering, the company now wants to stream the existing data files and ongoing changes from Amazon S3 to Amazon Kinesis Data Streams. The timelines are quite stringent and the company is looking at implementing this functionality as soon as possible.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest as the fastest possible way of getting the updated solution deployed in production?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Configure CloudWatch events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the CloudWatch event that will send the necessary data to Amazon Kinesis Data Streams</p>",
				"<p>Leverage S3 event notification to trigger a Lambda function for the file create event. The Lambda function will then send the necessary data to Amazon Kinesis Data Streams</p>",
				"<p>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (SNS). SNS can then be used to send the updates to Amazon Kinesis Data Streams</p>",
				"<p>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage AWS Database Migration Service (AWS DMS) as a bridge between Amazon S3 and Amazon Kinesis Data Streams</strong> - You can achieve this by using AWS Database Migration Service (AWS DMS). AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in AWS cloud.</p>\n\n<p>The given requirement needs the functionality to be implemented in the least possible time. You can use AWS DMS for such data-processing requirements. AWS DMS lets you expand the existing application to stream data from Amazon S3 into Amazon Kinesis Data Streams for real-time analytics without writing and maintaining new code. AWS DMS supports specifying Amazon S3 as the source and streaming services like Kinesis and Amazon Managed Streaming of Kafka (Amazon MSK) as the target. AWS DMS allows migration of full and change data capture (CDC) files to these services. AWS DMS performs this task out of box without any complex configuration or code development. You can also configure an AWS DMS replication instance to scale up or down depending on the workload.</p>\n\n<p>AWS DMS supports Amazon S3 as the source and Kinesis as the target, so data stored in an S3 bucket is streamed to Kinesis. Several consumers, such as AWS Lambda, Amazon Kinesis Data Firehose, Amazon Kinesis Data Analytics, and the Kinesis Consumer Library (KCL), can consume the data concurrently to perform real-time analytics on the dataset. Each AWS service in this architecture can scale independently as needed.</p>\n\n<p>Architecture of the proposed solution:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q27-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\">https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure CloudWatch events for the bucket actions on Amazon S3. An AWS Lambda function can then be triggered from the CloudWatch event that will send the necessary data to Amazon Kinesis Data Streams</strong> - You will need to enable a Cloudtrail trail to use object-level actions as a trigger for CloudWatch events. Also, using Lambda functions would require significant custom development to write the data into Kinesis Data Streams, so this option is not the right fit.</p>\n\n<p><strong>Leverage S3 event notification to trigger a Lambda function for the file create event. The Lambda function will then send the necessary data to Amazon Kinesis Data Streams</strong> - Using Lambda functions would require significant custom development to write the data into Kinesis Data Streams, so this option is not the right fit.</p>\n\n<p><strong>Amazon S3 bucket actions can be directly configured to write data into Amazon Simple Notification Service (SNS). SNS can then be used to send the updates to Amazon Kinesis Data Streams</strong> - S3 cannot directly write data into SNS, although it can certainly use S3 event notifications to send an event to SNS. Also, SNS cannot directly send messages to Kinesis Data Streams. So this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/\">https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/</a></p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A data analytics company uses custom data-integration services to produce data and log files in S3 buckets. As part of the process re-engineering, the company now wants to stream the existing data files and ongoing changes from Amazon S3 to Amazon Kinesis Data Streams. The timelines are quite stringent and the company is looking at implementing this functionality as soon as possible.\n\nAs a Solutions Architect, which of the following would you suggest as the fastest possible way of getting the updated solution deployed in production?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815776,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks</p>",
				"<p>Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks</p>",
				"<p>Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</p>",
				"<p>Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</p>"
			],
			"question": "<p>A Pharmaceuticals company has decided to move most of their IT infrastructure to AWS Cloud. Some of the applications, however, will remain on their on-premises data center to meet certain regulatory guidelines. The company is looking for a scalable solution to connect the on-premises applications to the ones on AWS Cloud.</p>\n\n<p>As a Solutions Architect, can you suggest the MOST optimal solution for this requirement?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Transit Gateway to connect the Amazon VPCs to the on-premises networks</strong> - The AWS Transit Gateway allows customers to connect their Amazon VPCs and their on-premises networks to a single gateway. As your number of workloads running on AWS increases, you need to be able to scale your networks across multiple accounts and Amazon VPCs to keep up with the growth. With AWS Transit Gateway, you only have to create and manage a single connection from the central gateway into each Amazon VPC, on-premises data center, or remote office across your network. AWS Transit Gateway acts as a hub that controls how traffic is routed among all the connected networks, which act like spokes. This hub and spoke model simplifies management and reduces operational costs because each network only has to connect to the Transit Gateway and not to every other network.</p>\n\n<p>AWS Transit Gateway:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q28-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Transit VPC Solution to connect the Amazon VPCs to the on-premises networks</strong> - The Transit VPC can be used to enable connectivity between various VPC’s in different regions and customer data centers. You can use this to connect multiple VPCs that are geographically disparate and/or running in separate AWS accounts, to a common VPC that serves as a global network transit center. This network topology simplifies network management and minimizes the number of connections that you need to set up.</p>\n\n<p>Transit VPC:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q28-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/transit-gateway/\">https://aws.amazon.com/transit-gateway/</a></p>\n\n<p>Transit VPC is not the right solution for this use-case as Transit Gateway provides several advantages over Transit VPC:\n1. Transit Gateway abstracts away the complexity of maintaining VPN connections with hundreds of VPCs.\n2. Transit Gateway removes the need to manage and scale EC2 based software appliances. AWS is responsible for managing all resources needed to route traffic.\n3. Transit Gateway removes the need to manage high availability by providing a highly available and redundant Multi-AZ infrastructure.\n4. Transit Gateway improves bandwidth for inter-VPC communication to burst speeds of 50 Gbps per AZ.\n5. Transit Gateway streamlines user costs to a simple per hour per/GB transferred model.\n6. Transit Gateway decreases latency by removing EC2 proxies and the need for VPN encapsulation.</p>\n\n<p><strong>Partially meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</strong></p>\n\n<p><strong>Fully meshed VPC peering can be used to connect the Amazon VPCs to the on-premises networks</strong></p>\n\n<p>The simplest way to connect two VPCs is to use VPC Peering. In this setup, a connection enables full bidirectional connectivity between the VPCs. This peering connection is used to route traffic between the VPCs. VPCs across accounts and AWS Regions can also be peered together. VPC peering only incurs costs for traffic traveling over the connection (there is no hourly infrastructure fee).</p>\n\n<p>VPC peering is point-to-point connectivity, and it does not support transitive routing. If you are using VPC peering, on-premises connectivity (VPN and/or Direct Connect) must be made to each VPC. Resources in a VPC cannot reach on-premises using the hybrid connectivity of a peered VPC. VPC peering is best used when resources in one VPC must communicate with resources in another VPC, the environment of both VPCs is controlled and secured, and the number of VPCs to be connected is less than 10 (to allow for the individual management of each connection). VPC peering offers the lowest overall cost when compared to other options for inter-VPC connectivity.</p>\n\n<p>Network setup using VPC Peering:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q28-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html</a></p>\n\n<p>You cannot use VPC Peering to establish on-premises connectivity with AWS Cloud, so both these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway-vs-transit-vpc.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A Pharmaceuticals company has decided to move most of their IT infrastructure to AWS Cloud. Some of the applications, however, will remain on their on-premises data center to meet certain regulatory guidelines. The company is looking for a scalable solution to connect the on-premises applications to the ones on AWS Cloud.\n\nAs a Solutions Architect, can you suggest the MOST optimal solution for this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815778,
		"assessment_type": "multi-select",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct options:</p>\n\n<p><strong>For application data, you must initiate and ensure the EBS Snapshots of your data volumes are configured for cross-region copy</strong> -  To ensure you have quicker recovery points, data persistence, and expected Amazon EBS capacity, you can\nreplicate your EBS Snapshots to another region and then create (unattached) volumes from them. By creating unattached volumes you block the capacity, effectively creating a reserved allocation for storage in the alternate region. For your application data, you must initiate and ensure the EBS Snapshots of your data volumes are configured for cross-region copy.</p>\n\n<p><strong>For static application data stored in Amazon S3, you need to enable Cross-Region Replication (CRR)</strong> - For static application data stored in Amazon S3 you can leverage Cross-Region Replication (CRR), which allows your data to be available in other regions too.</p>\n\n<p><strong>For data stored in databases, Amazon RDS Read Replicas provide enhanced performance and durability for database instances</strong> - For data stored in databases, Amazon RDS Read Replicas provide enhanced performance and durability for database instances. Read replicas can be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, and PostgreSQL as well as Amazon Aurora. It is important to note that there may be replication latency depending on the distance of the target region from the source region. You can monitor the replication lag using use Amazon\nCloudWatch when you implement any of these use cases.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Elastic File System (Amazon EFS), used to store files, is scoped to individual AZ. You need to employ EFS File Sync to quickly replicate files across multiple AZs</strong> - This statement is incorrect. Amazon Elastic File System (Amazon EFS) is regionally scoped storage service.  Amazon EFS allows you to mount a single volume to multiple instances, and is stored redundantly across multiple AZs within that single region. To ensure high data availability, you can employ EFS File Sync to quickly replicate files and their corresponding metadata to another region.</p>\n\n<p><strong>For OS images, when using Amazon EC2 and Amazon EBS, the appropriate Amazon Machine Images (AMIs) are automatically copied and available in the alternate Region as specified by the user</strong> - For OS images, when using Amazon EC2 and Amazon EBS, you must manually initiate the copy and ensure that the appropriate Amazon Machine Images (AMIs) are copied and available in the alternate region.</p>\n\n<p><strong>Amazon EBS volumes are regionally scoped. To ensure high availability, you should replicate your EBS Snapshots to another region</strong> - This statement is incorrect. Amazon EBS volumes are scoped to an individual AZ. For Amazon EBS volumes attached to your compute resources, creating a snapshot in another region will permit data from your local volumes to be available in another region.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://d1.awsstatic.com/Financial%20Services/Resilient%20Applications%20on%20AWS%20for%20Financial%20Services.pdf\">https://d1.awsstatic.com/Financial%20Services/Resilient%20Applications%20on%20AWS%20for%20Financial%20Services.pdf</a></p>\n",
			"answers": [
				"<p>For application data, you must initiate and ensure the EBS Snapshots of your data volumes are configured for cross-region copy</p>",
				"<p>Amazon Elastic File System (Amazon EFS), used to store files, is scoped to individual AZ. You need to employ EFS File Sync to quickly replicate files across multiple AZs</p>",
				"<p>For OS images, when using Amazon EC2 and Amazon EBS, the appropriate Amazon Machine Images (AMIs) are automatically copied and available in the alternate Region as specified by the user</p>",
				"<p>For static application data stored in Amazon S3, you need to enable Cross-Region Replication (CRR)</p>",
				"<p>Amazon EBS volumes are regionally scoped. To ensure high availability, you should replicate your EBS Snapshots to another region</p>",
				"<p>For data stored in databases, Amazon RDS Read Replicas provide enhanced performance and durability for database instances</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"question": "<p>A retail company has its business-critical data stored on the AWS Cloud in different forms such as files on a filesystem, object storage in S3, block storage and in relational databases. The company has hired you as a Solutions Architect to make this data available in alternate regions as part of the disaster recovery strategy.</p>\n\n<p>Which of the following would you suggest as the key points to consider for each of the storage technologies that the company is currently using? (Select three)</p>\n"
		},
		"correct_response": [
			"a",
			"d",
			"f"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A retail company has its business-critical data stored on the AWS Cloud in different forms such as files on a filesystem, object storage in S3, block storage and in relational databases. The company has hired you as a Solutions Architect to make this data available in alternate regions as part of the disaster recovery strategy.\n\nWhich of the following would you suggest as the key points to consider for each of the storage technologies that the company is currently using? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815780,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Cost Explorer Resource Optimization to get a report of EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations</strong> - AWS Cost Explorer helps you identify under-utilized EC2 instances that may be downsized on an instance by instance basis within the same instance family, and also understand the potential impact on your AWS bill by taking into account your Reserved Instances and Savings Plans.</p>\n\n<p>AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies</strong> -</p>\n\n<p>By using Amazon S3 Analytics Storage Class analysis you can analyze storage access patterns to help you decide when to transition the right data to the right storage class. This new Amazon S3 analytics feature observes data access patterns to help you determine when to transition less frequently accessed STANDARD storage to the STANDARD_IA (IA, for infrequent access) storage class. Storage class analysis does not give recommendations for transitions to the ONEZONE_IA or S3 Glacier storage classes.</p>\n\n<p><strong>Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew Reserved Instances. Trusted advisor also suggests Amazon RDS idle DB instances</strong> - AWS Trusted Advisor checks for Amazon EC2 Reserved Instances that are scheduled to expire within the next 30 days or have expired in the preceding 30 days. Reserved Instances do not renew automatically; you can continue using an EC2 instance covered by the reservation without interruption, but you will be charged On-Demand rates. Trusted advisor does not have a feature to auto-renew Reserved Instances.</p>\n\n<p><strong>Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs</strong> - AWS Compute Optimizer recommends optimal AWS Compute resources for your workloads to reduce costs and improve performance by using machine learning to analyze historical utilization metrics. Over-provisioning compute can lead to unnecessary infrastructure cost and under-provisioning compute can lead to poor application performance. Compute Optimizer helps you choose the optimal Amazon EC2 instance types, including those that are part of an Amazon EC2 Auto Scaling group, based on your utilization data. It does not recommend instance purchase options.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/compute-optimizer/\">https://aws.amazon.com/compute-optimizer/</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/analytics-storage-class.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/analytics-storage-class.html</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>A Silicon Valley startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high to support their business requirements.</p>\n\n<p>As a Solutions Architect, which of the following options represents a valid cost-optimization solution?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use Amazon S3 Storage class analysis to get recommendations for transitions of objects to S3 Glacier storage classes to reduce storage costs. You can also automate moving these objects into lower-cost storage tier using Lifecycle Policies</p>",
				"<p>Use AWS Cost Explorer Resource Optimization to get a report of EC2 instances that are either idle or have low utilization and use AWS Compute Optimizer to look at instance type recommendations</p>",
				"<p>Use AWS Trusted Advisor checks on Amazon EC2 Reserved Instances to automatically renew Reserved Instances. Trusted advisor also suggests Amazon RDS idle DB instances</p>",
				"<p>Use AWS Compute Optimizer recommendations to help you choose the optimal Amazon EC2 purchasing options and help reserve your instance capacities at reduced costs</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A Silicon Valley startup's cloud infrastructure consists of a few Amazon EC2 instances, Amazon RDS instances and Amazon S3 storage. A year into their business operations, the startup is incurring costs that seem too high to support their business requirements.\n\nAs a Solutions Architect, which of the following options represents a valid cost-optimization solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815782,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A multi-national retail company has multiple business divisions with each division having its own AWS account. The engineering team at the company would like to debug and trace data across these AWS accounts and visualize it in a centralized account.</p>\n\n<p>As a Solutions Architect, which of the following solutions would you suggest for the given use-case?</p>\n",
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>X-Ray</strong></p>\n\n<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components.</p>\n\n<p>You can use X-Ray to collect data across AWS Accounts. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. This enables you to publish data from various components of your application into a central account.</p>\n\n<p>How X-Ray Works:\n<img src=\"https://d1.awsstatic.com/Products/product-name/Images/product-page-diagram_AWS-X-Ray_how-it-works.2922edd4bfe011e997dbf32fdf8bd520bcbc85fb.png\">\nvia - <a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>VPC Flow Logs</strong>: VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data is used to analyze network traces and helps with network security. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. You cannot use VPC Flow Logs to debug and trace data across accounts.</p>\n\n<p><strong>CloudWatch Events</strong>: Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. These help to trigger notifications based on changes happening in AWS services. You cannot use CloudWatch Events to debug and trace data across accounts.</p>\n\n<p><strong>CloudTrail</strong>: With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. You can use AWS CloudTrail to answer questions such as - “Who made an API call to modify this resource?”. CloudTrail provides event history of your AWS account activity thereby enabling governance, compliance, operational auditing, and risk auditing of your AWS account. You cannot use CloudTrail to debug and trace data across accounts.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/xray/\">https://aws.amazon.com/xray/</a></p>\n",
			"answers": [
				"<p>CloudTrail</p>",
				"<p>VPC Flow Logs</p>",
				"<p>CloudWatch Events</p>",
				"<p>X-Ray</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A multi-national retail company has multiple business divisions with each division having its own AWS account. The engineering team at the company would like to debug and trace data across these AWS accounts and visualize it in a centralized account.\n\nAs a Solutions Architect, which of the following solutions would you suggest for the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815784,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"question": "<p>A media company has set up its technology infrastructure using AWS services such as Amazon EC2 instances, Lambda functions, Amazon S3 storage service and Amazon ElastiCache Redis to enhance the performance of its RDS database layer. The company has hired you as a Solutions Architect to implement a robust disaster recovery strategy for its caching layer so that it guarantees minimal downtime and data loss while ensuring top application performance.</p>\n\n<p>Which of the following solutions will you recommend to address the given use-case?</p>\n",
			"answers": [
				"<p>Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure</p>",
				"<p>Schedule daily automatic backups at a time when you expect low resource utilization for your cluster</p>",
				"<p>Schedule Manual backups using Redis append-only file (AOF)</p>",
				"<p>Add read-replicas across multiple availability zones to reduce the risk of potential data loss because of failure</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for Multi-AZ configuration with automatic failover functionality to help mitigate failure</strong> - Multi-AZ is the best option when data retention, minimal downtime, and application performance are a priority.</p>\n\n<p>Data-loss potential - Low. Multi-AZ provides fault tolerance for every scenario, including hardware-related issues.</p>\n\n<p>Performance impact - Low. Of the available options, Multi-AZ provides the fastest time to recovery, because there is no manual procedure to follow after the process is implemented.</p>\n\n<p>Cost - Low to high. Multi-AZ is the lowest-cost option. Use Multi-AZ when you can't risk losing data because of hardware failure or you can't afford the downtime required by other options in your response to an outage.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Schedule daily automatic backups at a time when you expect low resource utilization for your cluster</strong> - Data loss potential is high, almost up to a day's worth of data. Hence, this is not the right option.</p>\n\n<p><strong>Schedule Manual backups using Redis append-only file (AOF)</strong> - Manual backups using AOF are retained indefinitely and are useful for testing and archiving. You can schedule manual backups to occur up to 20 times per node within any 24-hour period. Although AOF provides a measure of fault tolerance, it can't protect your data from a hardware-related cache node failure, so there is a risk of data loss.</p>\n\n<p><strong>Add read-replicas across multiple availability zones to reduce the risk of potential data loss because of failure</strong> - To scale read capacity, ElastiCache allows you to add up to five read replicas across multiple availability zones. Read replicas are used to ease out read traffic from the primary database and cannot be used as a complete fault-tolerant solution in itself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/FaultTolerance.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/FaultTolerance.html</a></p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A media company has set up its technology infrastructure using AWS services such as Amazon EC2 instances, Lambda functions, Amazon S3 storage service and Amazon ElastiCache Redis to enhance the performance of its RDS database layer. The company has hired you as a Solutions Architect to implement a robust disaster recovery strategy for its caching layer so that it guarantees minimal downtime and data loss while ensuring top application performance.\n\nWhich of the following solutions will you recommend to address the given use-case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815786,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>TTL caching strategy is the right fit for this use case. Opt for Write-Through caching strategy. The write-through strategy adds or updates data in the cache whenever data is written to the database, making sure that the database never gets too many read requests</p>",
				"<p>Delete the existing cache keys and opt for Lazy loading cache technique for lesser hits on database</p>",
				"<p>When different application processes simultaneously request a cache key, get a cache miss, and then each hits the same database query for data, it results in the database getting swamped with identical queries. The solution is to prewarm the cache</p>",
				"<p>ElastiCache seems to be preconfigured with an eviction policy that is forcing data out of the memory. Disable the eviction policy to remedy the fault</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>When different application processes simultaneously request a cache key, get a cache miss, and then each hits the same database query for data, it results in the database getting swamped with identical queries. The solution is to prewarm the cache</strong></p>\n\n<p>Also known as dogpiling, the thundering herd effect is what happens when many different application processes simultaneously request a cache key, get a cache miss, and then each hits the same database query in parallel. The more expensive this query is, the bigger impact it has on the database. If the query involved is a top 10 query that requires ranking a large dataset, the impact can be a significant hit.</p>\n\n<p>One problem with adding TTLs to all of your cache keys is that it can exacerbate this problem. For example, let's say millions of people are following a popular user on your site. That user hasn't updated his profile or published any new messages, yet his profile cache still expires due to a TTL. Your database might suddenly be swamped with a series of identical queries.</p>\n\n<p>The solution is to prewarm the cache - Write a script that performs the same requests that your application will. If it's a web app, this script can be a shell script that hits a set of URLs. This makes sure that the cache is fresh, reducing the load running identical queries on the database.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>TTL caching strategy is the right fit for this use case. Opt for Write-Through caching strategy. The write-through strategy adds or updates data in the cache whenever data is written to the database, making sure that the database never gets too many read requests</strong> - The write-through strategy adds data or updates data in the cache whenever data is written to the database. Every write involves two trips: A write to the database and then a write to the cache, which adds latency to the process. Since it's a chatting application, a lag on every write message is not good for user experience and hence is not an optimal answer for the problem.</p>\n\n<p><strong>Delete the existing cache keys and opt for Lazy loading cache technique for lesser hits on database</strong> - As the name implies, lazy loading is a caching strategy that loads data into the cache only when necessary.  When a cache miss occurs, your application requests and receives the data from the database. Your application then, updates the cache with the new data. Lazy loading is not an optimal solution for chat applications, where user experience is completely dependent on latency.</p>\n\n<p><strong>ElastiCache seems to be preconfigured with an eviction policy that is forcing data out of the memory. Disable the eviction policy to remedy the fault</strong> - Evictions occur when memory is over-filled or greater than the max-memory setting in the cache, resulting in the engine to select keys to evict in order to manage its memory. The keys that are chosen are based on the eviction policy that is selected.</p>\n\n<p>If no eviction policy is set, ElastiCache will stop working when it runs out of memory, since it cannot remove any data from its memory. For the given use-case, this setting will be counter-productive.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/best-practices/\">https://aws.amazon.com/caching/best-practices/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/Strategies.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A popular chatting application has millions of users. The engineering team responsible for the application has used ElastiCache to bolster the performance even as the user base has increased by two folds over the past six months. After due diligence, the team has chosen TTL (Time to Live) caching strategy for their requirements which has worked reasonably well so far. Recently, the database administrator has found sudden spikes in queries to the database that are fetching the same data. This effect may cripple the database as more users sign-up for the application thereby causing a bad user experience.</p>\n\n<p>As a Solutions Architect, can you identify the pattern that could be causing these sudden spikes and a resolution to address the issue?</p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A popular chatting application has millions of users. The engineering team responsible for the application has used ElastiCache to bolster the performance even as the user base has increased by two folds over the past six months. After due diligence, the team has chosen TTL (Time to Live) caching strategy for their requirements which has worked reasonably well so far. Recently, the database administrator has found sudden spikes in queries to the database that are fetching the same data. This effect may cripple the database as more users sign-up for the application thereby causing a bad user experience.\n\nAs a Solutions Architect, can you identify the pattern that could be causing these sudden spikes and a resolution to address the issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815788,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Create IAM Roles with limited permissions for each of the teams. Any missing permissions can be added to these roles if developers face issues in accessing services they need</p>",
				"<p>Create service accounts for each of the teams and restrict the associated policy to only the permissions needed for the particular functionality</p>",
				"<p>Use Access Advisor to know the action last accessed information for the last few months and create IAM Groups with the permissions gained from these insights</p>",
				"<p>Use Access Advisor to determine the permissions the developers have used in the last few months and only give those permissions (with new IAM roles) while reverting the rest</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Access Advisor to determine the permissions the developers have used in the last few months and only give those permissions (with new IAM roles) while reverting the rest</strong></p>\n\n<p>Access advisor will determine the permissions your developers have used by analyzing the last timestamp when an IAM entity (for example, a user, role, or group) accessed an AWS service. This information helps you audit service access, remove unnecessary permissions, and set appropriate permissions across different environments (To view the last accessed information in the AWS Management Console, you must have a policy that grants the necessary permissions).</p>\n\n<p>For example, you can grant broad access to services in development accounts and then reduce permissions for access to specific services in production accounts. Finally, as you manage more IAM entities and AWS accounts, you need a way to scale these processes through automation. To help you achieve this automation, you can now use IAM access advisor APIs with the AWS Command Line Interface (AWS CLI) or a programmatic client.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create IAM Roles with limited permissions for each of the teams. Any missing permissions can be added to these roles if developers face issues in accessing services they need</strong> - This is a possible option but not an optimized one. For an organization spread across countries and holding multiple AWS accounts, this exercise will not only take a lot of time, but it will also need daily follow-ups to make sure all teams have necessary permissions.</p>\n\n<p><strong>Create service accounts for each of the teams and restrict the associated policy to only the permissions needed for the particular functionality</strong> - Service account is an account used for programmatic access by applications running outside of the AWS environment. This has no bearing on the current use case.</p>\n\n<p><strong>Use Access Advisor to know the action last accessed information for the last few months and create IAM Groups with the permissions gained from these insights</strong> - Action last accessed information is only relevant for Amazon S3 management actions, whereas the given use-case caters to access details for all the services used by the user, not just Amazon S3.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_access-advisor.html</a></p>\n",
			"question": "<p>You are working as an Engineering Manager at a company and you manage several development teams and monitor their access across multiple accounts. To get the teams up and running quickly, you had initially created multiple roles with broad permissions that are based on job function in the development accounts. Now, the developers are ready to deploy workloads to production accounts and you only want to grant them minimum possible permissions that the developers would actually need.</p>\n\n<p>Which of the following is the best way of implementing this requirement?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "You are working as an Engineering Manager at a company and you manage several development teams and monitor their access across multiple accounts. To get the teams up and running quickly, you had initially created multiple roles with broad permissions that are based on job function in the development accounts. Now, the developers are ready to deploy workloads to production accounts and you only want to grant them minimum possible permissions that the developers would actually need.\n\nWhich of the following is the best way of implementing this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815790,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Use Amazon Simple Queue Service (SQS), a fully managed message queuing service that enables you to asynchronously decouple and scale microservices, distributed systems, and serverless applications</p>",
				"<p>Use Amazon EventBridge, which is a serverless event bus that makes it easy to connect applications and is event-based, works asynchronously to decouple the system architecture</p>",
				"<p>Use Amazon Simple Notification Service (SNS), a fully managed messaging service for both system-to-system and app-to-person (A2P) communication. It enables you to communicate between systems through publish/subscribe (pub/sub) patterns that enable asynchronous messaging between decoupled microservice applications or to communicate directly to users via SMS, mobile push and email</p>",
				"<p>Use Elastic Load Balancing, that automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions for effective decoupling of system architecture</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A CRM company is moving their IT infrastructure to AWS Cloud to take advantage of the scalability, flexibility and cost optimization it offers. The company has a SaaS (Software as a Service) CRM application that feeds updates to a multitude of other in-house applications as well as several third-party SaaS applications. These in-house applications are also being migrated to use AWS services and the company is looking at connecting the SaaS CRM with the in-house as well as third-party SaaS applications.</p>\n\n<p>As a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon EventBridge, which is a serverless event bus that makes it easy to connect applications and is event-based, works asynchronously to decouple the system architecture</strong> - Both Amazon EventBridge and Amazon SNS can be used to develop event-driven applications, and in this use case, EventBridge alone works.</p>\n\n<p>Amazon EventBridge is recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Amazon EventBridge is the only event-based service that integrates directly with third-party SaaS partners. Amazon EventBridge also automatically ingests events from over 90 AWS services without requiring developers to create any resources in their account. Further, Amazon EventBridge uses a defined JSON-based structure for events and allows you to create rules that are applied across the entire event body to select events to forward to a target. Amazon EventBridge currently supports over 15 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, and Amazon Kinesis Streams and Firehose, among others. At launch, Amazon EventBridge is has limited throughput (see Service Limits) which can be increased upon request, and typical latency of around half a second.</p>\n\n<p>How EventBridge works:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q35-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (SNS), a fully managed messaging service for both system-to-system and app-to-person (A2P) communication. It enables you to communicate between systems through publish/subscribe (pub/sub) patterns that enable asynchronous messaging between decoupled microservice applications or to communicate directly to users via SMS, mobile push and email</strong> - As discussed above, SNS can be used for event-based services. But, our use case needs integration with third-party SaaS services, hence EventBridge is the right choice.</p>\n\n<p><strong>Use Amazon Simple Queue Service (SQS), a fully managed message queuing service that enables you to asynchronously decouple and scale microservices, distributed systems, and serverless applications</strong> - SQS is a message queuing service from amazon and works well for decoupling applications. It does not directly integrate with third-party SaaS services.</p>\n\n<p><strong>Use Elastic Load Balancing, that automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions for effective decoupling of system architecture</strong> - Elastic Load Balancing offers a synchronous decoupling of applications, which is not a right fit for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/eventbridge/\">https://aws.amazon.com/eventbridge/</a></p>\n\n<p><a href=\"https://aws.amazon.com/sns/\">https://aws.amazon.com/sns/</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A CRM company is moving their IT infrastructure to AWS Cloud to take advantage of the scalability, flexibility and cost optimization it offers. The company has a SaaS (Software as a Service) CRM application that feeds updates to a multitude of other in-house applications as well as several third-party SaaS applications. These in-house applications are also being migrated to use AWS services and the company is looking at connecting the SaaS CRM with the in-house as well as third-party SaaS applications.\n\nAs a Solutions Architect, which of the following would you suggest to asynchronously decouple the architecture?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815792,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>An enterprise is planning their journey to AWS Cloud and the CTO has decided to move the secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move the huge chunks of data and store it cost-effectively.</p>\n\n<p>What is an optimal solution that meets these requirements while keeping the costs to a minimum?</p>\n",
			"answers": [
				"<p>Tape Gateway can be used to move on-premises tape data onto AWS Cloud. From here, Amazon S3 archiving storage classes can be used to store data cost-effectively for years</p>",
				"<p>AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes</p>",
				"<p>Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs</p>",
				"<p>Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Tape Gateway can be used to move on-premises tape data onto AWS Cloud. From here, Amazon S3 archiving storage classes can be used to store data cost-effectively for years</strong> - Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows. Tape Gateway supports all leading backup applications and caches virtual tapes on-premises for low-latency data access. Tape Gateway encrypts data between the gateway and AWS for secure data transfer and compresses data while transitioning virtual tapes between Amazon S3 and Amazon S3 Glacier, or Amazon S3 Glacier Deep Archive, to minimize storage costs.</p>\n\n<p>Tape Gateway compresses and stores archived virtual tapes in the lowest-cost Amazon S3 storage classes, Amazon S3 Glacier and Amazon S3 Glacier Deep Archive. This makes it feasible for you to retain long-term data in the AWS Cloud at a very low cost. With Tape Gateway, you only pay for what you consume, with no minimum commitments and no upfront fees.</p>\n\n<p>Tape Gateway stores your virtual tapes in S3 buckets managed by the AWS Storage Gateway service, so you don’t have to manage your own Amazon S3 storage. Tape Gateway integrates with all leading backup applications allowing you to start using cloud storage for on-premises backup and archive without any changes to your backup and archive workflows.</p>\n\n<p>Tape Gateway Overview:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q36-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS DataSync makes it simple and fast to move large amounts of data online between on-premises storage and AWS Cloud. Data moved to Cloud can then be stored cost-effectively in Amazon S3 archiving storage classes</strong> - AWS DayaSync supports only NFS and SMB file types and hence is not the right choice for the given use case.</p>\n\n<p><strong>Use AWS Direct Connect, a cloud service solution that makes it easy to establish a dedicated network connection from on-premises to AWS to transfer data. Once this is done, Amazon S3 can be used to store data at lesser costs</strong> - AWS Direct Connect is used when customers need to retain on-premises structure because of compliance reasons and have moved the rest of the architecture to AWS Cloud. These businesses generally have an on-going requirement for low latency access to AWS Cloud and hence are willing to spend on installing the physical lines needed for this connection. The given use-case needs a cost-optimized solution and they do not have an ongoing requirement for high availability bandwidth.</p>\n\n<p><strong>Use AWS VPN connection between the on-premises datacenter and your Amazon VPC. Once this is established, you can use Amazon Elastic File System (Amazon EFS) to get a scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources</strong> - VPN connection is used when businesses have an on-going requirement for connectivity from the on-premises data center to AWS Cloud. Amazon EFS is a managed file system by AWS and cannot be used for archiving on-premises tape data onto AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/vtl/\">https://aws.amazon.com/storagegateway/vtl/</a></p>\n\n<p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An enterprise is planning their journey to AWS Cloud and the CTO has decided to move the secondary workloads such as backups and archives to AWS cloud. The CTO wishes to move the data stored on physical tapes to Cloud, without changing their current backup workflows. The company holds petabytes of data on tapes and needs a cost-optimized solution to move the huge chunks of data and store it cost-effectively.\n\nWhat is an optimal solution that meets these requirements while keeping the costs to a minimum?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815794,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon S3 for hosting the web application and use S3 Transfer Acceleration to reduce the latency that geographically dispersed users might face</strong></p>\n\n<p>Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in Internet routing, congestion, and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications.</p>\n\n<p>S3TA improves transfer performance by routing traffic through Amazon CloudFront’s globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations.</p>\n\n<p>For applications interacting with your S3 buckets through the S3 API from outside of your bucket’s region, S3TA helps avoid the variability in Internet routing and congestion. It does this by routing your uploads and downloads over the AWS global network infrastructure, so you get the benefit of AWS network optimizations.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users</strong> - Amazon S3 with CloudFront is a very powerful way of distributing static content to geographically dispersed users with low latency speeds. If you have objects that are smaller than 1GB or if the data set is less than 1GB in size, you should consider using Amazon CloudFront's PUT/POST commands for optimal performance. The given use case has data larger than 1GB and hence S3 Transfer Acceleration is a better option.</p>\n\n<p><strong>Use Amazon EC2 with Global Accelerator for faster distribution of content, while using Amazon S3 as storage service</strong> - AWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. With Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, Elastic IPs, and EC2 Instances, without making user-facing changes. As discussed, Global Accelerator is meant for a different use case and is not meant for increasing the speed of S3 uploads or downloads.</p>\n\n<p><strong>Use Amazon EC2 with ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service</strong> -\nAmazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. S3 Transfer Acceleration is a better performing option than opting for EC2 with ElastiCache, which is not meant to address the given use-case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/transfer-acceleration/\">https://aws.amazon.com/s3/transfer-acceleration/</a></p>\n",
			"answers": [
				"<p>Use Amazon S3 for hosting the web application and use Amazon CloudFront for faster distribution of content to geographically dispersed users</p>",
				"<p>Use Amazon EC2 with Global Accelerator for faster distribution of content, while using Amazon S3 as storage service</p>",
				"<p>Use Amazon EC2 with ElastiCache for faster distribution of content, while Amazon S3 can be used as a storage service</p>",
				"<p>Use Amazon S3 for hosting the web application and use S3 Transfer Acceleration to reduce the latency that geographically dispersed users might face</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world. The main feature of the application requires the upload and download of video files that can reach a maximum size of 10GB. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience.</p>\n\n<p>As a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A media startup is looking at hosting their web application on AWS Cloud. The application will be accessed by users from different geographic regions of the world. The main feature of the application requires the upload and download of video files that can reach a maximum size of 10GB. The startup wants the solution to be cost-effective and scalable with the lowest possible latency for a great user experience.\n\nAs a Solutions Architect, which of the following will you suggest as an optimal solution to meet the given requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815796,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Configure Amazon CloudWatch events that can trigger the recovery of the EC2 instance, in case the instance or the application fails</p>",
				"<p>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance can be configured with EBS volume or with instance store volumes</p>",
				"<p>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance, however, should only be configured with an EBS volume</p>",
				"<p>Configure AWS Trusted Advisor to monitor the health check of EC2 instance and provide a remedial action in case an unhealthy flag is detected</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes.</p>\n\n<p>In case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance? </p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance, however, should only be configured with an EBS volume</strong> - If your instance fails a system status check, you can use CloudWatch alarm actions to automatically recover it. The recover option is available for over 90% of deployed customer EC2 instances. The CloudWatch recovery option works only for system check failures, not for instance status check failures. Also, if you terminate your instance, then it can't be recovered.</p>\n\n<p>You can create an Amazon CloudWatch alarm that monitors an Amazon EC2 instance and automatically recovers the instance if it becomes impaired due to an underlying hardware failure or a problem that requires AWS involvement to repair. Terminated instances cannot be recovered. A recovered instance is identical to the original instance, including the instance ID, private IP addresses, Elastic IP addresses, and all instance metadata. If the impaired instance is in a placement group, the recovered instance runs in the placement group.</p>\n\n<p>The automatic recovery process attempts to recover your instance for up to three separate failures per day. Your instance may subsequently be retired if automatic recovery fails and a hardware degradation is determined to be the root cause for the original system status check failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon CloudWatch events that can trigger the recovery of the EC2 instance, in case the instance or the application fails</strong> - You cannot use CloudWatch events to directly trigger the recovery of the EC2 instance.</p>\n\n<p><strong>Configure an Amazon CloudWatch alarm that triggers the recovery of the EC2 instance, in case the instance fails. The instance can be configured with EBS volume or with instance store volumes</strong> - The recover action is supported only on instances that have EBS volumes configured on them, instance store volumes are not supported for automatic recovery by CloudWatch alarms.</p>\n\n<p><strong>Configure AWS Trusted Advisor to monitor the health check of EC2 instance and provide a remedial action in case an unhealthy flag is detected</strong> - You can use Amazon CloudWatch Events to detect and react to changes in the status of Trusted Advisor checks. This support is only available with AWS Business Support and AWS Enterprise Support. Trusted Advisor by itself does not support health checks of EC2 instances or their recovery.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html</a></p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A startup has recently moved their monolithic web application to AWS Cloud. The application runs on a single EC2 instance. Currently, the user base is small and the startup does not want to spend effort on elaborate disaster recovery strategies or Auto Scaling Group. The application can afford a maximum downtime of 10 minutes.\n\nIn case of a failure, which of these options would you suggest as a cost-effective and automatic recovery procedure for the instance?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815798,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Upstream microservices should send their output to the configured Amazon SQS queue. The downstream slower microservices can pick messages from the respective queues for processing</strong></p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p>\n\n<p>Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed. Being able to store the messages and replay them is a very important feature in decoupling the system architecture, as is needed in the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Upstream microservices should publish their output to the configured Amazon SNS topic. The downstream, slower microservices will get the notifications for working on them to completion, at their own pace</strong> - Amazon SNS follows the \"publish-subscribe\" (pub-sub) messaging paradigm, with notifications being delivered to clients using a \"push\" mechanism. This is an important difference between SNS and SQS. Whereas SQS is a polling mechanism, that gives applications the chance to poll at their own comfort, the push mechanism assumes the other applications are present. For the current requirement, we need messages to be stored till they are processed by the downstream applications. Hence, SQS is the right choice.</p>\n\n<p><strong>Upstream microservices can stream their output to Amazon Kinesis Data Streams. All the consumer microservices can fetch the data from these streams and work on them at their own pace</strong> - Amazon Kinesis Data Streams are used for streaming real-time high-volume data. Kinesis is a publish-subscribe model, used when publisher applications need to publish the same data to different consumers in parallel. SQS is the right fit for the current use case.</p>\n\n<p><strong>The upstream microservices can send their output to Amazon EventBridge, which can forward or distribute the work across different downstream microservices</strong> - This event-based service is extremely useful for connecting non-AWS SaaS (Software as a Service) services to AWS services. With Eventbridge, the downstream application would need to immediately process the events whenever they arrive, thereby making it a tightly coupled scenario. Hence, this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>A legacy application is built using a tightly-coupled monolithic architecture. With an increased number of users, the company is unable to provide a good user experience. Performance, scalability, and security have become an issue, since a small change propagates to all the connected components, making it difficult to develop, test, and maintain the application features. The company has decided to decouple the architecture and adopt AWS microservices architecture. Some of the microservices handle need to handle fast running processes whereas other microservices need to handle slower processes.</p>\n\n<p>As a Solutions Architect, which of these options would you identify as the right way of connecting these microservices</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Upstream microservices should publish their output to the configured Amazon SNS topic. The downstream, slower microservices will get the notifications for working on them to completion, at their own pace</p>",
				"<p>Upstream microservices can stream their output to Amazon Kinesis Data Streams. All the consumer microservices can fetch the data from these streams and work on them at their own pace</p>",
				"<p>The upstream microservices can send their output to Amazon EventBridge, which can forward or distribute the work across different downstream microservices</p>",
				"<p>Upstream microservices should send their output to the configured Amazon SQS queue. The downstream slower microservices can pick messages from the respective queues for processing</p>"
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A legacy application is built using a tightly-coupled monolithic architecture. With an increased number of users, the company is unable to provide a good user experience. Performance, scalability, and security have become an issue, since a small change propagates to all the connected components, making it difficult to develop, test, and maintain the application features. The company has decided to decouple the architecture and adopt AWS microservices architecture. Some of the microservices handle need to handle fast running processes whereas other microservices need to handle slower processes.\n\nAs a Solutions Architect, which of these options would you identify as the right way of connecting these microservices",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815800,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A profitable small business has been running their IT systems using the on-premises infrastructure. With growing demand, the business is finding it difficult to manage the IT infrastructure, which is not their core competency. The business plans to move to AWS Cloud, in light of their plans of extending their operations to other countries.</p>\n\n<p>As a Solutions Architect, can you suggest a cost-effective, serverless solution for their flagship application that has both static and dynamic content as part of its core data model?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Host the static content on Amazon S3 and use Lambda with DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of Lambda for distribution across diverse regions</strong> - Lambda with DynamoDB is the right answer for a serverless solution. CloudFront will help in enhancing user experience by delivering content, across different geographic locations with low latency. Amazon S3 is a cost-effective and faster way of distributing static content for web applications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries</strong> - S3 is not the right fit for hosting Dynamic content, so this option is incorrect.</p>\n\n<p><strong>Host the static content on Amazon S3 and use Amazon EC2 with RDS for generating the dynamic content. Amazon CloudFront can be configured in front of EC2 instance, to make global distribution easy</strong> - The company is looking for a serverless solution, and Amazon EC2 is not a serverless service as the EC2 instances have to be managed by AWS customers.</p>\n\n<p><strong>Host both the static and dynamic content of the web application on Amazon EC2 with RDS as the database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions</strong> - This is a possible solution, but not a cost-effective or optimal one. Since static content can be cost-effectively managed on Amazon S3 and can be accessed and distributed faster when compared to fetching the content from the EC2 server.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/\">https://aws.amazon.com/blogs/networking-and-content-delivery/deliver-your-apps-dynamic-content-using-amazon-cloudfront-getting-started-template/</a></p>\n",
			"answers": [
				"<p>Host both the static and dynamic content of the web application on Amazon S3 and use Amazon CloudFront for distribution across diverse regions/countries</p>",
				"<p>Host the static content on Amazon S3 and use Lambda with DynamoDB for the serverless web application that handles dynamic content. Amazon CloudFront will sit in front of Lambda for distribution across diverse regions</p>",
				"<p>Host the static content on Amazon S3 and use Amazon EC2 with RDS for generating the dynamic content. Amazon CloudFront can be configured in front of EC2 instance, to make global distribution easy</p>",
				"<p>Host both the static and dynamic content of the web application on Amazon EC2 with RDS as database. Amazon CloudFront should be configured to distribute the content across geographically disperse regions</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A profitable small business has been running their IT systems using the on-premises infrastructure. With growing demand, the business is finding it difficult to manage the IT infrastructure, which is not their core competency. The business plans to move to AWS Cloud, in light of their plans of extending their operations to other countries.\n\nAs a Solutions Architect, can you suggest a cost-effective, serverless solution for their flagship application that has both static and dynamic content as part of its core data model?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815802,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create a read replica and connect the report generation tool/application to it</strong> - Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.</p>\n\n<p>There are a variety of scenarios where deploying one or more read replicas for a given source DB instance may make sense. Common reasons for deploying a read replica include:</p>\n\n<ol>\n<li>Scaling beyond the compute or I/O capacity of a single DB instance for read-heavy database workloads. This excess read traffic can be directed to one or more read replicas.</li>\n<li>Serving read traffic while the source DB instance is unavailable. If your source DB Instance cannot take I/O requests (e.g. due to I/O suspension for backups or scheduled maintenance), you can direct read traffic to your read replica(s). For this use case, keep in mind that the data on the read replica may be “stale” since the source DB Instance is unavailable.</li>\n<li>Business reporting or data warehousing scenarios; you may want business reporting queries to run against a read replica, rather than your primary, production DB Instance.</li>\n<li>You may use a read replica for disaster recovery of the source DB instance, either in the same AWS Region or in another Region.</li>\n</ol>\n\n<p>Comparing Read Replicas with Multi-AZ and Multi-Region RDS deployments:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q41-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Increase the size of RDS instance</strong> - This will not help as it's mentioned that the CPU, memory, and storage are running at only 50% of the total capacity.</p>\n\n<p><strong>Migrate from General Purpose SSD to magnetic storage to enhance IOPS</strong> - This is incorrect. Amazon RDS supports magnetic storage for backward compatibility only. AWS recommends that you use General Purpose SSD or Provisioned IOPS for any storage needs.</p>\n\n<p><strong>Configure the RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. However, you cannot read from the standby database, making multi-AZ, an incorrect option for the given scenario.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
			"question": "<p>The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity.</p>\n\n<p>Can you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?</p>\n",
			"answers": [
				"<p>Increase the size of RDS instance</p>",
				"<p>Migrate from General Purpose SSD to magnetic storage to enhance IOPS</p>",
				"<p>Create a read replica and connect the report generation tool/application to it</p>",
				"<p>Configure the RDS instance to be Multi-AZ DB instance, and connect the report generation tool to the DB instance in a different AZ</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The application maintenance team at a company has noticed that the production application is very slow when the business reports are run on the RDS database. These reports fetch a large amount of data and have complex queries with multiple joins, spanning across multiple business-critical core tables. CPU, memory, and storage metrics are around 50% of the total capacity.\n\nCan you recommend an improved and cost-effective way of generating the business reports while keeping the production application unaffected?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815804,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Configure AWS Lambda with an RDS database solution, to provide a serverless architecture</p>",
				"<p>Use Amazon CloudFront with DynamoDB for greater speed and low latency access</p>",
				"<p>Use AWS Lambda with ElastiCache and Amazon RDS for serving content at high speed and low latency</p>",
				"<p>Use Amazon CloudFront with S3 as the storage solution</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon CloudFront with S3 as the storage solution</strong> -</p>\n\n<p>When you put your content in an S3 bucket in the cloud, a lot of things become much easier. First, you don’t need to plan for and allocate a specific amount of storage space because S3 buckets scale automatically. As S3 is a serverless service, you don’t need to manage or patch servers that store files yourself; you just put and get your content. Finally, even if you require a server for your application (for example, because you have a dynamic application), the server can be smaller because it doesn’t have to handle requests for static content.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users. CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront. You only pay for what is delivered to the internet from CloudFront, plus request fees.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure AWS Lambda with an RDS database solution, to provide a serverless architecture</strong> - RDS is not the right choice for the current scenario because of the overhead of a database management system, as the given use-case can be addressed by using Amazon S3 storage solution.</p>\n\n<p><strong>Use Amazon CloudFront with DynamoDB for greater speed and low latency access</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. But, DynamoDB is overkill for the given use-case and will prove to be a very costly solution.</p>\n\n<p><strong>Use AWS Lambda with ElastiCache and Amazon RDS for serving content at high speed and low latency</strong> - As discussed above, RDS is not needed for this use case where web application needs to display static pages and facilitate downloads of historic data. As a managed service, S3 is much better suited for this requirement.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n",
			"question": "<p>An online gaming application has been built on AWS Cloud. Of late, a large chunk of traffic is coming from users who download the historic leaderboard reports and their game tactics for various games. The current infrastructure and design are unable to cope up with the traffic numbers and application freezes on most of the pages.</p>\n\n<p>The company has hired you as a Solutions Architect to provide a cost-optimal and easily implementable solution that does not need provisioning of infrastructure resources. Which of the following will you recommend?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An online gaming application has been built on AWS Cloud. Of late, a large chunk of traffic is coming from users who download the historic leaderboard reports and their game tactics for various games. The current infrastructure and design are unable to cope up with the traffic numbers and application freezes on most of the pages.\n\nThe company has hired you as a Solutions Architect to provide a cost-optimal and easily implementable solution that does not need provisioning of infrastructure resources. Which of the following will you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815806,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain.</p>\n\n<p>Which of the following solutions addresses these requirements with the minimal integration effort?</p>\n",
			"answers": [
				"<p>Use Amazon FSx for Windows File Server, for shared storage space</p>",
				"<p>Use File Gateway of AWS Storage Gateway to create a hybrid storage solution</p>",
				"<p>Use Amazon FSx for Lustre, for effective shared storage and millisecond latencies</p>",
				"<p>Use Amazon Elastic File System (Amazon EFS) as a shared storage solution</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon FSx for Windows File Server, for shared storage space</strong> - Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. It offers single-AZ and multi-AZ deployment options, fully managed backups, and encryption of data at rest and in transit. You can optimize cost and performance for your workload needs with SSD and HDD storage options; and you can scale storage and change the throughput performance of your file system at any time.</p>\n\n<p>With Amazon FSx, you get highly available and durable file storage starting from $0.013 per GB-month. Data deduplication enables you to optimize costs even further by removing redundant data. You can increase your file system storage and scale throughput capacity at any time, making it easy to respond to changing business needs. There are no upfront costs or licensing fees.</p>\n\n<p>How Amazon FSx for Windows File Server works:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q43-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/fsx/windows/?nc=sn&amp;loc=1\">https://aws.amazon.com/fsx/windows/?nc=sn&amp;loc=1</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use File Gateway of AWS Storage Gateway to create a hybrid storage solution</strong> - AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration between your on-premises IT environment and the AWS storage infrastructure. Storage Gateway uses Amazon S3 to store data on AWS Cloud and from here the on-premises data can seamlessly integrate with Cloud services. It is not suited to be used as a shared storage space that multiple applications can access in parallel.</p>\n\n<p><strong>Use Amazon FSx for Lustre, for effective shared storage and millisecond latencies</strong> - Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. Many workloads such as machine learning, high performance computing (HPC), video rendering, and financial simulations depend on compute instances accessing the same set of data through high-performance shared storage. Lustre is Linux based, hence it is not the right choice since the use case is about Windows-based applications.</p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS) as a shared storage solution</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. EFS is a powerful, shared storage solution that would have been the right answer if the customer systems were Linux based. Amazon EFS is compatible with only Linux-based AMIs for Amazon EC2.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/?nc=sn&amp;loc=1\">https://aws.amazon.com/fsx/windows/?nc=sn&amp;loc=1</a></p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An IT company hosts windows based applications on its on-premises data center. The company is looking at moving the business to the AWS Cloud. The cloud solution should offer shared storage space that multiple applications can access without a need for replication. Also, the solution should integrate with the company's self-managed Active Directory domain.\n\nWhich of the following solutions addresses these requirements with the minimal integration effort?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815808,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>Configure a NAT instance in the public subnet of the VPC</p>",
				"<p>Configure a NAT Gateway in the public subnet of the VPC</p>",
				"<p>Configure the Internet Gateway of the VPC to be accessible to the private subnet resources by changing the route tables</p>",
				"<p>Configure an Egress-only internet gateway for the resources in the private subnet of the VPC</p>"
			],
			"question": "<p>A company has its application servers in the public subnet that connect to the RDS instances in the private subnet. For regular maintenance, the RDS instances need patch fixes that need to be downloaded from the internet.</p>\n\n<p>Considering that the company uses only IPv4 addressing and is looking for a fully managed service with little or no overhead, which of the following would you suggest as an optimal solution?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Configure a NAT Gateway in the public subnet of the VPC</strong> - You can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside.</p>\n\n<p>You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed after you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. If you no longer need a NAT gateway, you can delete it. Deleting a NAT gateway disassociates its Elastic IP address, but does not release the address from your account.</p>\n\n<p>VPC architecture with NAT:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q44-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an Egress-only internet gateway for the resources in the private subnet of the VPC</strong> - An Egress-only internet gateway is an Internet Gateway that supports IPv6 traffic, so this option is not correct for the given use-case.</p>\n\n<p><strong>Configure a NAT instance in the public subnet of the VPC</strong> - You can use a network address translation (NAT) instance in a public subnet in your VPC to enable instances in the private subnet to initiate outbound IPv4 traffic to the internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the internet. NAT instances are not a managed service, it has to be managed and maintained by the customer.</p>\n\n<p><strong>Configure the Internet Gateway of the VPC to be accessible to the private subnet resources, by changing the route tables</strong> - Internet Gateway cannot be used directly with a private subnet. It is not possible to set up this configuration, without a NAT instance or a NAT gateway in the public subnet.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A company has its application servers in the public subnet that connect to the RDS instances in the private subnet. For regular maintenance, the RDS instances need patch fixes that need to be downloaded from the internet.\n\nConsidering that the company uses only IPv4 addressing and is looking for a fully managed service with little or no overhead, which of the following would you suggest as an optimal solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815812,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A highly successful gaming company has consistently used the standard architecture of configuring an Application Load Balancer (ALB) in front of Amazon EC2 instances for different services and microservices. As they have expanded to different countries and additional features have been added, the architecture has become complex with too many ALBs in multiple regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations.</p>\n\n<p>The company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Launch AWS Global Accelerator and create endpoints for all the Regions. Register the ALBs of each Region to the corresponding endpoints</strong> - AWS Global Accelerator is a networking service that sends your user’s traffic through Amazon Web Service’s global network infrastructure, improving your internet user performance by up to 60%. When the internet is congested, Global Accelerator’s automatic routing optimizations will help keep your packet loss, jitter, and latency consistently low.</p>\n\n<p>With Global Accelerator, you are provided two global static customer-facing IPs to simplify traffic management. On the back end, add or remove your AWS application origins, such as Network Load Balancers, Application Load Balancers, Elastic IPs, and EC2 Instances, without making user-facing changes. To mitigate endpoint failure, Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint.</p>\n\n<p>Simplified and resilient traffic routing for multi-Region applications:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q45-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Elastic IPs for each of the ALBs in each region</strong> - An Application Load Balancer cannot be assigned an Elastic IP address (static IP address).</p>\n\n<p><strong>Set up a Network Load Balancer (NLB) with Elastic IPs. To this NLB, register the private IPs of all the ALBs as targets</strong> - An NLB can be configured to take an Elastic IP. However, with hundreds of ALBs, the NLB-ALB combination will be equally cumbersome to manage.</p>\n\n<p><strong>Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 to run behind the ASGs, for each of the regions</strong> - You cannot assign an Elastic IP to an Auto Scaling Group, since ASG just manages a collection of EC2 instances.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/using-static-ip-addresses-for-application-load-balancers/</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Configure Elastic IPs for each of the ALBs in each region</p>",
				"<p>Set up a Network Load Balancer (NLB) with Elastic IPs. To this NLB, register the private IPs of all the ALBs as targets</p>",
				"<p>Launch AWS Global Accelerator and create endpoints for all the Regions. Register the ALBs of each Region to the corresponding endpoints</p>",
				"<p>Assign an Elastic IP to an Auto Scaling Group (ASG), and set up multiple Amazon EC2 to run behind the ASGs, for each of the regions</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A highly successful gaming company has consistently used the standard architecture of configuring an Application Load Balancer (ALB) in front of Amazon EC2 instances for different services and microservices. As they have expanded to different countries and additional features have been added, the architecture has become complex with too many ALBs in multiple regions. Security updates, firewall configurations, and traffic routing logic have become complex with too many IP addresses and configurations.\n\nThe company is looking at an easy and effective way to bring down the number of IP addresses allowed by the firewall and easily manage the entire network infrastructure. Which of these options represents an appropriate solution for this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815814,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure Lambda to trigger logic for downstream processing</p>",
				"<p>Use Amazon Simple Queue Service (SQS) for data ingestion and configure Lambda to trigger logic for downstream processing</p>",
				"<p>Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture</p>",
				"<p>Use Amazon Kinesis Data Streams to ingest the data. Process this data using AWS Lambda function or run analytics using Kinesis Data Analytics</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Kinesis Data Streams to ingest the data. Process this data using AWS Lambda function or run analytics using Kinesis Data Analytics</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service with support for retry mechanism. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>KDS makes sure your streaming data is available to multiple real-time analytics applications, to Amazon S3, or AWS Lambda within 70 milliseconds of the data being collected. Kinesis data streams scale from megabytes to terabytes per hour and scale from thousands to millions of PUT records per second. You can dynamically adjust the throughput of your stream at any time based on the volume of your input data.</p>\n\n<p>How Data Streams work:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q46-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2\">https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon Simple Notification Service (Amazon SNS) for data ingestion and configure Lambda to trigger logic for downstream processing</strong> - Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. SNS is a push mechanism that does not support robust retry mechanisms, as is needed in the current use case.</p>\n\n<p><strong>Use Amazon Simple Queue Service (SQS) for data ingestion and configure Lambda to trigger logic for downstream processing</strong> - SQS is a messaging service that helps in decoupling systems and reducing the complexity of architecture. SQS can still work, but Kinesis Data streams is custom made for streaming near real-time data and hold an impressive feature set to deal with the same.</p>\n\n<p><strong>Use Amazon API Gateway with the existing REST-based interface to create a high performing architecture</strong> - Amazon API Gateway is not meant for handling near real-time streaming data.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2\">https://aws.amazon.com/kinesis/data-streams/?nc=sn&amp;loc=2&amp;dn=2</a></p>\n",
			"question": "<p>A health care startup has built their application on REST-based interface that receives near real-time data from the small devices connected to patients. Once the patient's health metrics flow in, the downstream applications process the data and generate a report of the diagnosis that is sent back to the doctor. The process worked well when the velocity of data ingestion was less. With the startup gaining traction and being used by more doctors, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead.</p>\n\n<p>As a Solutions Architect, which of the following would you recommend as a scalable alternative to the current solution?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A health care startup has built their application on REST-based interface that receives near real-time data from the small devices connected to patients. Once the patient's health metrics flow in, the downstream applications process the data and generate a report of the diagnosis that is sent back to the doctor. The process worked well when the velocity of data ingestion was less. With the startup gaining traction and being used by more doctors, the system has become slow and sometimes even unresponsive as it does not have a retry mechanism. The startup is looking at a scalable solution that has minimal implementation overhead.\n\nAs a Solutions Architect, which of the following would you recommend as a scalable alternative to the current solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815816,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 Auto Scaling chooses the policy that provides the largest capacity, so policy with the custom metric is triggered, and two new instances will be launched by the ASG</strong> - A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM. For an advanced scaling configuration, your Auto Scaling group can have more than one scaling policy. For example, you can define one or more target tracking scaling policies, one or more step scaling policies, or both. This provides greater flexibility to cover multiple scenarios.</p>\n\n<p>When there are multiple policies in force at the same time, there's a chance that each policy could instruct the Auto Scaling group to scale out (or in) at the same time. For example, it's possible that the CPUUtilization metric spikes and triggers the CloudWatch alarm at the same time that the SQS custom metric spikes and triggers the custom metric alarm.</p>\n\n<p>When these situations occur, Amazon EC2 Auto Scaling chooses the policy that provides the largest capacity for both scale-out and scale-in. Suppose, for example, that the policy for CPUUtilization launches one instance, while the policy for the SQS queue launches two instances. If the scale-out criteria for both policies are met at the same time, Amazon EC2 Auto Scaling gives precedence to the SQS queue policy. This results in the Auto Scaling group launching two instances. The approach of giving precedence to the policy that provides the largest capacity applies even when the policies use different criteria for scaling in.</p>\n\n<p>AWS recommends caution when using target tracking scaling policies with step scaling policies because conflicts between these policies can cause undesirable behavior. For example, if the step scaling policy initiates a scale-in activity before the target tracking policy is ready to scale in, the scale-in activity will not be blocked. After the scale-in activity completes, the target tracking policy could instruct the group to scale out again.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling chooses the minimum capacity from each of the policies that meet the criteria. So, one new instance will be launched by the ASG</strong></p>\n\n<p><strong>Amazon EC2 Auto Scaling chooses the latest policy after running the algorithm defined during ASG configuration. Based on this output, either of the policies will be chosen for scaling out</strong></p>\n\n<p><strong>Amazon EC2 Auto Scaling chooses the sum of the capacity of all the policies that meet the criteria. So, three new instances will be launched by the ASG</strong></p>\n\n<p>These three options contradict the explanation provided above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p>\n",
			"question": "<p>A retail company manages its IT infrastructure on AWS Cloud. A fleet of Amazon EC2 instances sits behind an Auto Scaling Group (ASG) that helps manage the fleet size to meet the demand. After analyzing the application, the team has configured two metrics that control the scale-in and scale-out policy of ASG. One is a target tracking policy that uses a custom metric to add and remove two new instances, based on the number of SQS messages in the queue. The other is a step scaling policy that uses the Amazon CloudWatch CPUUtilization metric to launch one new instance when the existing instance exceeds 90 percent utilization for a specified length of time.</p>\n\n<p>While testing, the scale-out policy criteria for both policies was met at the same time. How many new instances will be launched because of these multiple scaling policies?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Amazon EC2 Auto Scaling chooses the policy that provides the largest capacity, so policy with the custom metric is triggered, and two new instances will be launched by the ASG</p>",
				"<p>Amazon EC2 Auto Scaling chooses the minimum capacity from each of the policies that meet the criteria. So, one new instance will be launched by the ASG</p>",
				"<p>Amazon EC2 Auto Scaling chooses the latest policy after running the algorithm defined during ASG configuration. Based on this output, either of the policies will be chosen for scaling out</p>",
				"<p>Amazon EC2 Auto Scaling chooses the sum of the capacity of all the policies that meet the criteria. So, three new instances will be launched by the ASG</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A retail company manages its IT infrastructure on AWS Cloud. A fleet of Amazon EC2 instances sits behind an Auto Scaling Group (ASG) that helps manage the fleet size to meet the demand. After analyzing the application, the team has configured two metrics that control the scale-in and scale-out policy of ASG. One is a target tracking policy that uses a custom metric to add and remove two new instances, based on the number of SQS messages in the queue. The other is a step scaling policy that uses the Amazon CloudWatch CPUUtilization metric to launch one new instance when the existing instance exceeds 90 percent utilization for a specified length of time.\n\nWhile testing, the scale-out policy criteria for both policies was met at the same time. How many new instances will be launched because of these multiple scaling policies?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815818,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"question": "<p>An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the S3 outbound data costs.</p>\n\n<p>As a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?</p>\n",
			"answers": [
				"<p>To reduce S3 cost, the data can be saved on an EBS volume connected to an EC2 instance that can host the application</p>",
				"<p>Use Amazon Elastic File System (Amazon EFS), as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</p>",
				"<p>Configure Amazon CloudFront to distribute the data hosted on Amazon S3 cost-effectively</p>",
				"<p>Configure S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to S3 buckets</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Configure Amazon CloudFront to distribute the data hosted on Amazon S3, cost-effectively</strong> - Storing content with S3 provides a lot of advantages. But to help optimize your application’s performance and security while effectively managing cost, AWS recommends that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content.</p>\n\n<p>CloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale. By design, delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users.</p>\n\n<p>CloudFront serves content through a worldwide network of data centers called Edge Locations. Using edge servers to cache and serve content improves performance by providing content closer to where viewers are located. CloudFront has edge servers in locations all around the world.</p>\n\n<p>When a user requests content that you serve with CloudFront, their request is routed to a nearby Edge Location. If CloudFront has a cached copy of the requested file, CloudFront delivers it to the user, providing a fast (low-latency) response. If the file they’ve requested isn’t yet cached, CloudFront retrieves it from your origin – for example, the S3 bucket where you’ve stored your content. Then, for the next local request for the same content, it’s already cached nearby and can be served immediately.</p>\n\n<p>By caching your content in Edge Locations, CloudFront reduces the load on your S3 bucket and helps ensure a faster response for your users when they request content. Also, data transfer out for content by using CloudFront is often more cost-effective than serving files directly from S3, and there is no data transfer fee from S3 to CloudFront. You only pay for what is delivered to the internet from CloudFront, plus request fees.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>To reduce S3 cost, the data can be saved on an EBS volume connected to an EC2 instance that can host the application</strong> - EBS volumes are fast and are relatively cheap (though S3 is still a cheaper alternative). But, EBS volumes are accessible only through EC2 instances and are bound to a specific region.</p>\n\n<p><strong>Use Amazon Elastic File System (Amazon EFS), as it provides a shared, scalable, fully managed elastic NFS file system for storing AWS Cloud or on-premises data</strong> - EFS is a shareable file system that can be mounted onto EC2 instances. EFS is costlier than EBS and not a solution if the company is looking at reducing costs.</p>\n\n<p><strong>Configure S3 Batch Operations to read data in bulk at one go, to reduce the number of calls made to S3 buckets</strong> - This statement is incorrect and given only as a distractor. You can use S3 Batch Operations to perform large-scale batch operations on Amazon S3 objects, and it has nothing to do with content distribution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "An IT training company hosted its website on Amazon S3 a couple of years ago. Due to COVID-19 related travel restrictions, the training website has suddenly gained traction. With an almost 300% increase in the requests served per day, the company's AWS costs have sky-rocketed for just the S3 outbound data costs.\n\nAs a Solutions Architect, can you suggest an alternate method to reduce costs while keeping the latency low?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815820,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use S3 Intelligent-Tiering storage class to optimize the S3 storage costs</strong> - The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. There are no retrieval fees when using the S3 Intelligent-Tiering storage class, and no additional tiering fees when objects are moved between access tiers. It is the ideal storage class for long-lived data with access patterns that are unknown or unpredictable.</p>\n\n<p>S3 Storage Classes can be configured at the object level and a single bucket can contain objects stored in S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. You can upload objects directly to S3 Intelligent-Tiering, or use S3 Lifecycle policies to transfer objects from S3 Standard and S3 Standard-IA to S3 Intelligent-Tiering. You can also archive objects from S3 Intelligent-Tiering to S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Amazon Elastic File System (Amazon EFS) to provide a fast, cost-effective and sharable storage service</strong> - Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. EFS offers sharable service, unlike Amazon Elastic Block Storage (EBS) that cannot be shared by instances. EFS is costlier than storing data in Amazon S3. Also, EFS needs an Amazon EC2 instance or an AWS Direct Connect network connection. Hence, this is not the correct option.</p>\n\n<p><strong>Use S3 One Zone-Infrequent Access, to reduce the costs on S3 storage</strong> - S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. Not a right option, since data stored is business-critical and cannot be risked by using S3 One Zone-IA.</p>\n\n<p><strong>Use S3 Outposts storage class to reduce the costs on S3 storage by storing the data on-premises</strong> - This is a distractor as Amazon S3 Outposts delivers object storage to your on-premises AWS Outposts environment. It is used in conjunction with AWS Outposts and has no relevance to the current use case.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A medical devices company extensively uses Amazon S3 buckets for their critical data storage. Hundreds of buckets are used to keep the images' data segregated and faster to access. During the company's monthly meetings, the finance team submitted a report with the high costs incurred on S3 storage. Upon analysis, the IT team realized that the lifecycle policies on the S3 buckets have not been applied most optimally.</p>\n\n<p>The company is looking at an easy way to reduce storage costs on S3 while keeping the IT team's involvement to a minimum. As a Solutions Architect, can you recommend the most appropriate option from the choices below?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Configure Amazon Elastic File System (Amazon EFS) to provide a fast, cost-effective and sharable storage service</p>",
				"<p>Use S3 Intelligent-Tiering storage class to optimize the S3 storage costs</p>",
				"<p>Use S3 One Zone-Infrequent Access, to reduce the costs on S3 storage</p>",
				"<p>Use S3 Outposts storage class to reduce the costs on S3 storage by storing the data on-premises</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A medical devices company extensively uses Amazon S3 buckets for their critical data storage. Hundreds of buckets are used to keep the images' data segregated and faster to access. During the company's monthly meetings, the finance team submitted a report with the high costs incurred on S3 storage. Upon analysis, the IT team realized that the lifecycle policies on the S3 buckets have not been applied most optimally.\n\nThe company is looking at an easy way to reduce storage costs on S3 while keeping the IT team's involvement to a minimum. As a Solutions Architect, can you recommend the most appropriate option from the choices below?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815822,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A startup has created a cost-effective backup solution in another AWS Region. The application is running in warm standby mode and has Application Load Balancer (ALB) to support it from the front. The current failover process is manual and requires updating the DNS alias record to point to the secondary ALB in another Region in case of failure of the primary ALB.</p>\n\n<p>As a Solutions Architect, what will you recommend to automate the failover process?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Enable an ALB health check</p>",
				"<p>Enable an EC2 instance health check</p>",
				"<p>Configure Trusted Advisor to check on unhealthy instances</p>",
				"<p>Enable an Amazon Route 53 health check</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Enable an Amazon Route 53 health check</strong> - Determining the health of an ELB endpoint is more complex than health checking a single IP address. For example, what if your application is running fine on EC2, but the load balancer itself isn't reachable? Or if your load balancer and your EC2 instances are working correctly, but a bug in your code causes your application to crash? Or how about if the EC2 instances in one Availability Zone of a multi-AZ ELB are experiencing problems?</p>\n\n<p>Route 53 DNS Failover handles all of these failure scenarios by integrating with ELB behind the scenes. Once enabled, Route 53 automatically configures and manages health checks for individual ELB nodes. Route 53 also takes advantage of the EC2 instance health checking that ELB performs (information on configuring your ELB health checks is available here). By combining the results of health checks of your EC2 instances and your ELBs, Route 53 DNS Failover can evaluate the health of the load balancer and the health of the application running on the EC2 instances behind it. In other words, if any part of the stack goes down, Route 53 detects the failure and routes traffic away from the failed endpoint.</p>\n\n<p>Using Route 53 DNS Failover, you can run your primary application simultaneously in multiple AWS regions around the world and failover across regions. Your end-users will be routed to the closest (by latency), healthy region for your application. Route 53 automatically removes from service any region where your application is unavailable - it will pull an endpoint out of service if there is region-wide connectivity or operational issue, if your application goes down in that region, or if your ELB or EC2 instances go down in that region.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable an ALB health check</strong> - ELB health check verifies that a specified TCP port on an instance is accepting connections or a specified page has returned an error code of 200. It is not useful for the given failover scenario.</p>\n\n<p><strong>Enable an EC2 instance health check</strong> - Instance status checks monitor the software and network configuration of your instance. It is not intelligent enough to understand if the application on the instance is working correctly. Hence, this is not the right choice for the given use-case.</p>\n\n<p><strong>Configure Trusted Advisor to check on unhealthy instances</strong> - AWS Trusted Advisor examines the health check configuration for Auto Scaling groups. If Elastic Load Balancing is being used for an Auto Scaling group, the recommended configuration is to enable an Elastic Load Balancing health check. Trusted Advisor recommends certain configuration changes by comparing your system configurations to AWS Best practices. It cannot handle a failover as Route 53 can.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/\">https://aws.amazon.com/blogs/aws/amazon-route-53-elb-integration-dns-failover/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-system-instance-status-check.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A startup has created a cost-effective backup solution in another AWS Region. The application is running in warm standby mode and has Application Load Balancer (ALB) to support it from the front. The current failover process is manual and requires updating the DNS alias record to point to the secondary ALB in another Region in case of failure of the primary ALB.\n\nAs a Solutions Architect, what will you recommend to automate the failover process?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815824,
		"assessment_type": "multi-select",
		"prompt": {
			"question": "<p>An automobile company is running its flagship application on a fleet of EC2 instances behind an Auto Scaling Group (ASG). The ASG has been configured more than a year ago. A young developer has just joined the development team and wants to understand the best practices to manage and configure an ASG.</p>\n\n<p>As a Solutions Architect, which of these would you identify as the key characteristics that the developer needs to remember regarding ASG configurations? (Select three)</p>\n",
			"relatedLectureIds": "",
			"explanation": "<p>Correct options:</p>\n\n<p>Amazon EC2 Auto Scaling is a fully managed service designed to launch or terminate Amazon EC2 instances automatically to help ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.</p>\n\n<p><strong>If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be terminated and the ASG will be deleted</strong> This statement is correct.</p>\n\n<p><strong>EC2 Auto Scaling groups can span Availability Zones, but not AWS regions</strong> - EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.</p>\n\n<p><strong>Data is not automatically copied from existing instances to a new dynamically created instance</strong> - Data is not automatically copied from existing instances to new instances. You can use lifecycle hooks to copy the data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>If you configure the ASG to a certain base capacity, you cannot use a combined purchasing model to fulfill the instance requirements. You will need to choose either On-Demand instances or Reserved Instances only</strong> - When setting up an ASG with a combined purchasing model, you can specify the base capacity of the group to be fulfilled by On-Demand instances. As the ASG scales in or scales out, EC2 Auto Scaling ensures the base capacity is fulfilled using On-Demand instances and anything beyond that be fulfilled with either only Spot instances or a specified percentage mix of On-Demand or Spot instances.</p>\n\n<p><strong>Amazon EC2 Auto Scaling can automatically add a volume when the existing one is approaching capacity. This, however, is a configuration parameter and needs to be set explicitly</strong> - Amazon EC2 Auto Scaling doesn't automatically add a volume when the existing one is approaching capacity. You can use the EC2 API to add a volume to an existing instance.</p>\n\n<p><strong>You can only specify one launch configuration for an EC2 Auto Scaling group at a time. But, you can modify a launch configuration after you've created it</strong> - You can only specify one launch configuration for an EC2 Auto Scaling group at a time, and you can't modify a launch configuration after you've created it.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/ec2/autoscaling/faqs/\">https://aws.amazon.com/ec2/autoscaling/faqs/</a></p>\n",
			"answers": [
				"<p>If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be terminated and the ASG will be deleted</p>",
				"<p>Amazon EC2 Auto Scaling can automatically add a volume when the existing one is approaching capacity. This, however, is a configuration parameter and needs to be set explicitly</p>",
				"<p>You can only specify one launch configuration for an EC2 Auto Scaling group at a time. But, you can modify a launch configuration after you've created it</p>",
				"<p>EC2 Auto Scaling groups can span Availability Zones, but not AWS regions</p>",
				"<p>Data is not automatically copied from existing instances to a new dynamically created instance</p>",
				"<p>If you configure the ASG to a certain base capacity, you cannot use a combined purchasing model to fulfill the instance requirements. You will need to choose either On-Demand instances or Reserved Instances only</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a",
			"d",
			"e"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An automobile company is running its flagship application on a fleet of EC2 instances behind an Auto Scaling Group (ASG). The ASG has been configured more than a year ago. A young developer has just joined the development team and wants to understand the best practices to manage and configure an ASG.\n\nAs a Solutions Architect, which of these would you identify as the key characteristics that the developer needs to remember regarding ASG configurations? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815826,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</p>",
				"<p>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</p>",
				"<p>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</p>",
				"<p>Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. Instance store is ideal for the temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers.</p>\n\n<p>You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance.</p>\n\n<p>Some instance types use NVMe or SATA-based solid-state drives (SSD) to deliver high random I/O performance. This is a good option when you need storage with very low latency, but you don't need the data to persist when the instance terminates or you can take advantage of fault-tolerant architectures.</p>\n\n<p>S3 Standard offers high durability, availability, and performance object storage for frequently accessed data. Because it delivers low latency and high throughput, S3 Standard is appropriate for a wide variety of use cases, including cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.</p>\n\n<p>S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours. You can upload objects directly to S3 Glacier, or use S3 Lifecycle policies to transfer data between any of the S3 Storage Classes for active data (S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA) and S3 Glacier.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 standard storage for maximum performance, Amazon S3 Intelligent-Tiering for intelligent, durable storage, and Amazon S3 Glacier Deep Archive for archival storage</strong> - Amazon EC2 instance store volumes provide the best I/O performance for low latency requirement, as in the current use case. The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.</p>\n\n<p>S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice a year. It is designed for customers — particularly those in highly-regulated industries, such as the Financial Services, Healthcare, and Public Sectors — that retain data sets for 7-10 years or longer to meet regulatory compliance requirements.</p>\n\n<p><strong>Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage</strong> - Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. EBS volumes are particularly well-suited for use as the primary storage for file systems, databases, or for any applications that require fine granular updates and access to raw, unformatted, block-level storage. For high I/O performance, instance store volumes are a better option.</p>\n\n<p><strong>Amazon EC2 instance store for maximum performance, AWS Storage Gateway for on-premises durable data access and Amazon S3 Glacier Deep Archive for archival storage</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Storage Gateway will be the right answer if the customer wanted to retain the on-premises data storage and just move the applications to AWS Cloud. In the absence of such requirements, instance store is a better option for high performance and Amazon S3 for durable storage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
			"question": "<p>A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 TB of very durable storage for storing media content and almost double of it, i.e. 900 TB for archival of legacy data.</p>\n\n<p>As a Solutions Architect, which set of services will you recommend to meet these requirements?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A media company is evaluating the possibility of moving its IT infrastructure to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for processing certain files which are mostly large videos. The company also needs close to 450 TB of very durable storage for storing media content and almost double of it, i.e. 900 TB for archival of legacy data.\n\nAs a Solutions Architect, which set of services will you recommend to meet these requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815828,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Batch messages</p>",
				"<p>Decrease the Stream retention duration</p>",
				"<p>Increase the number of shards</p>",
				"<p>Use Exponential Backoff</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Batch messages</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>Kinesis Data Streams Overview:\n<img src=\"https://d1.awsstatic.com/Products/product-name/diagrams/product-page-diagram_Amazon-Kinesis-Data-Streams.074de94302fd60948e1ad070e425eeda73d350e7.png\">\nvia - <a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n\n<p>When a host needs to send many records per second (RPS) to Amazon Kinesis, simply calling the basic PutRecord API action in a loop is inadequate. To reduce overhead and increase throughput, the application must batch records and implement parallel HTTP requests. This will increase the efficiency overall and ensure you are optimally using the shards.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Exponential Backoff</strong>: While this may help in the short term, as soon as the request rate increases, you will see the <code>ProvisionedThroughputExceededException</code> exception again.</p>\n\n<p><strong>Increase the number of shards</strong> - Increasing shards could be a short term fix but will substantially increase the cost, so this option is ruled out.</p>\n\n<p><strong>Decrease the Stream retention duration</strong> - This operation may result in data loss and won't help with the exceptions, so this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/\">https://aws.amazon.com/blogs/big-data/implementing-efficient-and-reliable-producers-with-the-amazon-kinesis-producer-library/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>You are a cloud architect in Silicon Valley. Many companies in this area have mobile apps that capture and send data to Amazon Kinesis Data Streams. They have been getting a <code>ProvisionedThroughputExceededException</code> exception. You have been contacted to help and upon careful analysis, you are seeing that messages are being sent one by one while being sent at a high rate.</p>\n\n<p>Which of the following options will help with the exception while keeping costs at a minimum?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "You are a cloud architect in Silicon Valley. Many companies in this area have mobile apps that capture and send data to Amazon Kinesis Data Streams. They have been getting a ProvisionedThroughputExceededException exception. You have been contacted to help and upon careful analysis, you are seeing that messages are being sent one by one while being sent at a high rate.\n\nWhich of the following options will help with the exception while keeping costs at a minimum?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815830,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>By default, all DynamoDB tables are encrypted under AWS managed CMKs, which do not write to CloudTrail logs</p>",
				"<p>By default, all DynamoDB tables are encrypted under an AWS owned customer master key (CMK), which do not write to CloudTrail logs</p>",
				"<p>By default, all DynamoDB tables are encrypted under Customer managed CMKs, which do not write to CloudTrail logs</p>",
				"<p>By default, all DynamoDB tables are encrypted using Data keys, which do not write to CloudTrail logs</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>By default, all DynamoDB tables are encrypted under an AWS owned customer master key (CMK), which do not write to CloudTrail logs</strong> - AWS owned CMKs are a collection of CMKs that an AWS service owns and manages for use in multiple AWS accounts. Although AWS owned CMKs are not in your AWS account, an AWS service can use its AWS owned CMKs to protect the resources in your account.</p>\n\n<p>You do not need to create or manage the AWS owned CMKs. However, you cannot view, use, track, or audit them. You are not charged a monthly fee or usage fee for AWS owned CMKs and they do not count against the AWS KMS quotas for your account.</p>\n\n<p>The key rotation strategy for an AWS owned CMK is determined by the AWS service that creates and manages the CMK.</p>\n\n<p>All DynamoDB tables are encrypted. There is no option to enable or disable encryption for new or existing tables. By default, all tables are encrypted under an AWS owned customer master key (CMK) in the DynamoDB service account. However, you can select an option to encrypt some or all of your tables under a customer-managed CMK or the AWS managed CMK for DynamoDB in your account.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>By default, all DynamoDB tables are encrypted under AWS managed CMKs, which do not write to CloudTrail logs</strong></p>\n\n<p><strong>By default, all DynamoDB tables are encrypted under Customer managed CMKs, which do not write to CloudTrail logs</strong></p>\n\n<p><strong>By default, all DynamoDB tables are encrypted using Data keys, which do not write to CloudTrail logs</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys\">https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#master_keys</a></p>\n",
			"question": "<p>A mobile chat application uses DynamoDB as its database service to provide low latency chat updates. A new developer has joined the team and is reviewing the configuration settings for DynamoDB which have been tweaked for certain technical requirements. CloudTrail service has been enabled on all the resources used for the project. Yet, DynamoDB encryption details are nowhere to be found.</p>\n\n<p>Which of the following options can explain the root cause for the given issue?</p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A mobile chat application uses DynamoDB as its database service to provide low latency chat updates. A new developer has joined the team and is reviewing the configuration settings for DynamoDB which have been tweaked for certain technical requirements. CloudTrail service has been enabled on all the resources used for the project. Yet, DynamoDB encryption details are nowhere to be found.\n\nWhich of the following options can explain the root cause for the given issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815832,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations</strong></p>\n\n<p>As your application architecture grows, so does the complexity, with longer user-facing IP lists and more nuanced traffic routing logic. AWS Global Accelerator solves this by providing you with two static IPs that are anycast from our globally distributed edge locations, giving you a single entry point to your application, regardless of how many AWS Regions it’s deployed in. This allows you to add or remove origins, Availability Zones or Regions without reducing your application availability. Your traffic routing is managed manually, or in console with endpoint traffic dials and weights. If your application endpoint has a failure or availability issue, AWS Global Accelerator will automatically redirect your new connections to a healthy endpoint within seconds.</p>\n\n<p>By using AWS Global Accelerator, you can:</p>\n\n<ol>\n<li><p>Associate the static IP addresses provided by AWS Global Accelerator to regional AWS resources or endpoints, such as Network Load Balancers, Application Load Balancers, EC2 Instances, and Elastic IP addresses. The IP addresses are anycast from AWS edge locations so they provide onboarding to the AWS global network close to your users.</p></li>\n<li><p>Easily move endpoints between Availability Zones or AWS Regions without needing to update your DNS configuration or change client-facing applications.</p></li>\n<li><p>Dial traffic up or down for a specific AWS Region by configuring a traffic dial percentage for your endpoint groups. This is especially useful for testing performance and releasing updates.</p></li>\n<li><p>Control the proportion of traffic directed to each endpoint within an endpoint group by assigning weights across the endpoints.</p></li>\n</ol>\n\n<p>AWS Global Accelerator for Multi-Region applications:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q55-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/global-accelerator/\">https://aws.amazon.com/global-accelerator/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed</strong> - AWS Direct Connect can reduce latency to great extent. Direct Connect is used to connect on-premises systems to AWS Cloud for extremely low latency use cases. It cannot be used to serve users directly.</p>\n\n<p><strong>Create S3 buckets in different AWS Regions and configure CloudFront to pick the nearest edge location to the user</strong> - If most of the content is static, we can configure CloudFront to improve performance. In the current scenario, the architecture has ELBs, EC2 instances too that need to be covered in the automatic failover plan.</p>\n\n<p><em>Set up an Amazon Route 53 geoproximity routing policy to route traffic</em>* - Geoproximity routing lets Amazon Route 53 route traffic to your resources based on the geographic location of your users and your resources. Unlike Global Accelerator, managing and routing to different instances, ELBs and other AWS resources will become an operational overhead as the resource count reaches into the hundreds. With inbuilt features like Static anycast IP addresses, fault tolerance using network zones, Global performance-based routing, TCP Termination at the Edge, Global Accelerator is the right choice for multi-region, low latency use cases.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/global-accelerator/features/\">https://aws.amazon.com/global-accelerator/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity</a></p>\n",
			"question": "<p>An online gaming company has global users accessing its flagship application in different AWS Regions. Few weeks ago, an Elastic Load Balancer (ELB) had malfunctioned in a region taking down all the traffic with it. The manual intervention cost the company significant time and resulted in a huge revenue loss. Additionally, the users have also complained of poor performance when accessing the application over the internet.</p>\n\n<p>What should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Set up AWS Direct Connect as the backbone for each of the AWS Regions where the application is deployed</p>",
				"<p>Create S3 buckets in different AWS Regions and configure CloudFront to pick the nearest edge location to the user</p>",
				"<p>Set up an Amazon Route 53 geoproximity routing policy to route traffic</p>",
				"<p>Set up AWS Global Accelerator and add endpoints to cater to users in different geographic locations</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An online gaming company has global users accessing its flagship application in different AWS Regions. Few weeks ago, an Elastic Load Balancer (ELB) had malfunctioned in a region taking down all the traffic with it. The manual intervention cost the company significant time and resulted in a huge revenue loss. Additionally, the users have also complained of poor performance when accessing the application over the internet.\n\nWhat should a solutions architect recommend to reduce internet latency and add automatic failover across AWS Regions?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815834,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a Route 53 active-passive failover configuration. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</strong></p>\n\n<p>Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only healthy primary resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a Route 53 active-active failover configuration. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</strong> - This option has been added as a distractor as there is no such thing as an active-active failover configuration in Route 53.</p>\n\n<p><strong>Use Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed</strong> - If your application is hosted in multiple AWS Regions, you can improve performance for your users by serving their requests from the AWS Region that provides the lowest latency - this is Latency-based routing and is not helpful for the current use case.</p>\n\n<p><strong>Use Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page</strong> - Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of the software. This is not useful for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html#dns-failover-types-active-passive</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency</a></p>\n",
			"answers": [
				"<p>Set up a Route 53 active-passive failover configuration. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</p>",
				"<p>Set up a Route 53 active-active failover configuration. If Route 53 health check determines the ALB endpoint as unhealthy, the traffic will be diverted to a static error page, hosted on Amazon S3 bucket</p>",
				"<p>Use Route 53 Latency-based routing. Create a latency record to point to the Amazon S3 bucket that holds the error page to be displayed</p>",
				"<p>Use Route 53 Weighted routing to give minimum weight to Amazon S3 bucket that holds the error page to be displayed. In case of primary failure, the requests get routed to the error page</p>"
			],
			"question": "<p>A company's cloud architect has set up a solution that uses Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website.</p>\n\n<p>Which configuration will meet the company's requirements, while keeping the changes to a bare minimum?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A company's cloud architect has set up a solution that uses Route 53 to configure the DNS records for the primary website with the domain pointing to the Application Load Balancer (ALB). The company wants a solution where users will be directed to a static error page, configured as a backup, in case of unavailability of the primary website.\n\nWhich configuration will meet the company's requirements, while keeping the changes to a bare minimum?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815836,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create a CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</strong></p>\n\n<p>When you use AWS WAF with CloudFront, you can protect your applications running on any HTTP webserver, whether it's a webserver that's running in Amazon Elastic Compute Cloud (Amazon EC2) or a web server that you manage privately. You can also configure CloudFront to require HTTPS between CloudFront and your own webserver, as well as between viewers and CloudFront.</p>\n\n<p>AWS WAF is tightly integrated with Amazon CloudFront and the Application Load Balancer (ALB), services that AWS customers commonly use to deliver content for their websites and applications. When you use AWS WAF on Amazon CloudFront, your rules run in all AWS Edge Locations, located around the world close to your end-users. This means security doesn’t come at the expense of performance. Blocked requests are stopped before they reach your web servers. When you use AWS WAF on Application Load Balancer, your rules run in the region and can be used to protect internet-facing as well as internal load balancers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure an Application Load Balancer (ALB) to balance the workload for all the EC2 instances. Configure CloudFront to distribute from an ALB since WAF cannot be directly configured on ALBs. This configuration not only provides necessary safety but is scalable too</strong> - This statement is wrong. You can configure WAF on Application Load Balancers (ALB).</p>\n\n<p><em>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</em>* - AWS WAF can be deployed on Amazon CloudFront, the Application Load Balancer (ALB), and Amazon API Gateway. It cannot be configured directly on an EC2 instance.</p>\n\n<p><strong>AWS WAF can be directly configured on either an Application Load Balancer (ALB) or Amazon API Gateway only. Either of these two services can be configured with Amazon EC2 to build the needed secure architecture</strong> - This statement is only partially correct. WAF can also be deployed on Amazon CloudFront service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/faqs/\">https://aws.amazon.com/waf/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p>\n",
			"answers": [
				"<p>Create a CloudFront distribution for the application on Amazon EC2 instances. Deploy AWS WAF on Amazon CloudFront to provide the necessary safety measures</p>",
				"<p>Configure an Application Load Balancer (ALB) to balance the workload for all the EC2 instances. Configure CloudFront to distribute from an ALB since WAF cannot be directly configured on ALBs. This configuration not only provides necessary safety but is scalable too</p>",
				"<p>AWS WAF can be directly configured on Amazon EC2 instances for ensuring the security of the underlying application data</p>",
				"<p>AWS WAF can be directly configured on either an Application Load Balancer (ALB) or Amazon API Gateway only. Either of these two services can be configured with Amazon EC2 to build the needed secure architecture</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A financial services application deployed on the on-premises infrastructure has recently been moved to Amazon EC2 instances. The application data contains critical personal information about all its customers and needs to be protected from all types of cyberattacks. The company is considering using the AWS Web Application Firewall (WAF) to handle this requirement.</p>\n\n<p>Can you identify the correct solution leveraging the capabilities of WAF?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A financial services application deployed on the on-premises infrastructure has recently been moved to Amazon EC2 instances. The application data contains critical personal information about all its customers and needs to be protected from all types of cyberattacks. The company is considering using the AWS Web Application Firewall (WAF) to handle this requirement.\n\nCan you identify the correct solution leveraging the capabilities of WAF?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815838,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use Kinesis Data Streams to read in audio files as they are generated. Use Amazon provided machine learning (ML) algorithms to initially convert the audio files into text. These text files can be analyzed using various ML algorithms to generate reports for customer sentiment analysis</p>",
				"<p>Use Kinesis Data Streams to read in audio files into Amazon Alexa, which will convert the audio files into text. Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output</p>",
				"<p>Use Amazon Transcribe to convert audio files to text. Run analysis on these text files using Amazon Athena to understand the underlying customer sentiments</p>",
				"<p>Use Amazon Transcribe to convert audio files to text. Use Amazon Quicksight to run analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for human analysis</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use Amazon Transcribe to convert audio files to text. Run analysis on these text files using Amazon Athena to understand the underlying customer sentiments</strong> - Amazon Transcribe is an automatic speech recognition (ASR) service that makes it easy to convert audio to text. One key feature of the service is called speaker identification, which you can use to label each individual speaker when transcribing multi-speaker audio files. You can specify Amazon Transcribe to identify 2–10 speakers in the audio clip.</p>\n\n<p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. To leverage Athena, you can simply point to your data in Amazon S3, define the schema, and start querying using standard SQL. Most results are delivered within seconds.</p>\n\n<p>Analyzing multi-speaker audio files using Amazon Transcribe and Amazon Athena:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt5-q58-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Kinesis Data Streams to read in audio files as they are generated. Use Amazon provided machine learning (ML) algorithms to initially convert the audio files into text. These text files can be analyzed using various ML algorithms to generate reports for customer sentiment analysis</strong> - Amazon Kinesis can be used to stream real-time data for further analysis and storage. Kinesis Data Streams cannot read in audio files. You will still need to use AWS Transcribe for ASR services.</p>\n\n<p><strong>Use Kinesis Data Streams to read in audio files into Amazon Alexa, which will convert the audio files into text. Kinesis Data Analytics can be used to analyze these files and Amazon Quicksight can be used to visualize and display the output</strong> -  Kinesis Data Streams cannot read in audio files. Amazon Alexa cannot be used as an Automatic Speech Recognition (ASR) service, though Alexa internally uses ASR for its working.</p>\n\n<p><strong>Use Amazon Transcribe to convert audio files to text. Use Amazon Quicksight to run analysis on these text files to understand the underlying patterns. Visualize and display them onto user Dashboards for human analysis</strong> - Amazon Quicksight is for the visual representation of data through Dashboards, graphs and various other modes. It has a rich feature set that helps analyze data and the complex relationships that exist between different data features. It is, however, not a powerful analysis tool like Amazon Athena.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena\">https://aws.amazon.com/blogs/machine-learning/automating-the-analysis-of-multi-speaker-audio-files-using-amazon-transcribe-and-amazon-athena</a></p>\n\n<p><a href=\"https://aws.amazon.com/athena\">https://aws.amazon.com/athena</a></p>\n",
			"question": "<p>A call center used to hire experienced specialists to analyze the customer service calls attended by their small group of call center representatives. Each call center representative handles about 40-50 calls in a day. Over the years, as the company grew to more than a hundred employees, hiring specialists to analyze calls has not only become cumbersome but also ineffective. The company wants to move to AWS Cloud and is looking at an automated solution that can help them analyze the calls for sentiment and security analysis.</p>\n\n<p>As a Solutions Architect, which of the following solutions would you recommend to meet the given requirements?</p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A call center used to hire experienced specialists to analyze the customer service calls attended by their small group of call center representatives. Each call center representative handles about 40-50 calls in a day. Over the years, as the company grew to more than a hundred employees, hiring specialists to analyze calls has not only become cumbersome but also ineffective. The company wants to move to AWS Cloud and is looking at an automated solution that can help them analyze the calls for sentiment and security analysis.\n\nAs a Solutions Architect, which of the following solutions would you recommend to meet the given requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815840,
		"assessment_type": "multi-select",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>RDS Oracle</p>",
				"<p>RDS Maria DB</p>",
				"<p>RDS MySQL</p>",
				"<p>RDS PostGreSQL</p>",
				"<p>RDS Sequel Server</p>"
			],
			"question": "<p>The Development team at an e-commerce company is working on securing their databases.</p>\n\n<p>Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct options:</p>\n\n<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication.  With this authentication method, you don't need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Each token has a lifetime of 15 minutes. You don't need to store user credentials in the database, because authentication is managed externally using IAM.</p>\n\n<p><strong>RDS MySQL</strong> - IAM database authentication works with MySQL and PostgreSQL.</p>\n\n<p><strong>RDS PostGreSQL</strong> - IAM database authentication works with MySQL and PostgreSQL.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>RDS Oracle</strong></p>\n\n<p><strong>RDS Maria DB</strong></p>\n\n<p><strong>RDS Sequel Server</strong></p>\n\n<p>These three options contradict the details in the explanation above, so these are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html</a></p>\n"
		},
		"correct_response": [
			"c",
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "The Development team at an e-commerce company is working on securing their databases.\n\nWhich of the following AWS database engines can be configured with IAM Database Authentication? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815842,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A solutions architect is tasked with a requirement to design a low-latency solution for a static, single-page application, accessed by users through a custom domain name. The solution must be serverless, provide in-transit data encryption and needs to be cost-effective.</p>\n\n<p>Which AWS services can be combined to build a solution for the company's requirement?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 can be used to host the static website. While Amazon CloudFront can be used to distribute the content for low latency access</strong></p>\n\n<p>To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you must enable website hosting, set permissions, and create and add an index document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a custom error document.</p>\n\n<p>After you configure your bucket as a static website, you can access the bucket through the AWS Region-specific Amazon S3 website endpoints for your bucket. Website endpoints are different from the endpoints where you send REST API requests. Amazon S3 doesn't support HTTPS access for website endpoints. If you want to use HTTPS, you can use CloudFront to serve a static website hosted on Amazon S3.</p>\n\n<p>You can use Amazon CloudFront to improve the performance of your website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (called edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.</p>\n\n<p>CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Host the application on Amazon EC2 instance with instance store volume for high performance of the application to provide low latency access to users</strong> - Since the use case speaks about a serverless solution, Amazon EC2 cannot be the answer, since EC2 is not serverless.</p>\n\n<p><strong>Host the application on AWS Fargate and front it with an Elastic Load Balancer for an improved performance</strong> - AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Elastic Load Balancer can spread the incoming requests across a fleet of EC2 instances. This added complexity is not needed since we are looking at a static single-page webpage.</p>\n\n<p><strong>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application on serverless architecture</strong> - Fargate is overkill for hosting a static single-page webpage.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p>\n",
			"answers": [
				"<p>Host the application on Amazon EC2 instance with instance store volume for high performance of the application to provide low latency access to users</p>",
				"<p>Configure Amazon S3 to store the static data and use AWS Fargate for hosting the application on serverless architecture</p>",
				"<p>Amazon S3 can be used to host the static website. While Amazon CloudFront can be used to distribute the content for low latency access</p>",
				"<p>Host the application on AWS Fargate and front it with an Elastic Load Balancer for an improved performance</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A solutions architect is tasked with a requirement to design a low-latency solution for a static, single-page application, accessed by users through a custom domain name. The solution must be serverless, provide in-transit data encryption and needs to be cost-effective.\n\nWhich AWS services can be combined to build a solution for the company's requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815844,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</strong> - You configure the size of your Auto Scaling group by setting the minimum, maximum, and desired capacity. The minimum and maximum capacity are required to create an Auto Scaling group, while the desired capacity is optional. If you do not define your desired capacity upfront, it defaults to your minimum capacity.</p>\n\n<p>Amazon EC2 Auto Scaling enables you to take advantage of the safety and reliability of geographic redundancy by spanning Auto Scaling groups across multiple Availability Zones within a Region. When one Availability Zone becomes unhealthy or unavailable, Auto Scaling launches new instances in an unaffected Availability Zone. When the unhealthy Availability Zone returns to a healthy state, Auto Scaling automatically redistributes the application instances evenly across all of the designated Availability Zones. Since the application is extremely critical and needs to have a reliable architecture to support it, the EC2 instances should be maintained in at least two Availability Zones (AZs) for uninterrupted service.</p>\n\n<p>Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. This is why the minimum capacity should be 4 instances and not 2. ASG will launch 2 instances each in both the AZs and this redundancy is needed to keep the service available always.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</strong></p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone</strong></p>\n\n<p>The explanation above gives the correct rationale for minimum capacity as well as the instance distribution across AZs, so both these options are incorrect.</p>\n\n<p><strong>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the ASG should be set to 6</strong> - An Auto Scaling group can contain EC2 instances in one or more Availability Zones within the same region. However, Auto Scaling groups cannot span multiple Regions.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n",
			"answers": [
				"<p>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</p>",
				"<p>The ASG should be configured with the minimum capacity set to 2, with 1 instance each in two different Availability Zones. The maximum capacity of the ASG should be set to 6</p>",
				"<p>The ASG should be configured with the minimum capacity set to 2 and the maximum capacity set to 6 in a single Availability Zone</p>",
				"<p>The ASG should be configured with the minimum capacity set to 4, with 2 instances each in two different AWS Regions. The maximum capacity of the ASG should be set to 6</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A health-care company runs its extremely critical web service on Amazon EC2 instances behind an Auto Scaling Group (ASG). The company provides ambulances for critical patients and needs the application to be reliable without issues. The workload of the company can be managed on 2 EC2 instances and can peak up to 6 instances when workload builds up.</p>\n\n<p>As a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A health-care company runs its extremely critical web service on Amazon EC2 instances behind an Auto Scaling Group (ASG). The company provides ambulances for critical patients and needs the application to be reliable without issues. The workload of the company can be managed on 2 EC2 instances and can peak up to 6 instances when workload builds up.\n\nAs a Solutions Architect, which of the following configurations would you select as the best fit for these requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815846,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>API Gateway exposing Lambda Functionality</strong></p>\n\n<p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services.</p>\n\n<p>How API Gateway Works:\n<img src=\"https://d1.awsstatic.com/serverless/New-API-GW-Diagram.c9fc9835d2a9aa00ef90d0ddc4c6402a2536de0d.png\">\nvia - <a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>How Lambda function works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Lambda/Diagrams/product-page-diagram_Lambda-HowItWorks.68a0bcacfcf46fccf04b97f16b686ea44494303f.png\">\nvia - <a href=\"https://aws.amazon.com/lambda/\">https://aws.amazon.com/lambda/</a></p>\n\n<p>API Gateway can expose Lambda functionality through RESTful APIs. Both are serverless options offered by AWS and hence the right choice for this scenario, considering all the functionality they offer.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Fargate with Lambda at the front</strong> - Lambda cannot directly handle RESTful API requests. You can invoke a Lambda function over HTTPS by defining a custom RESTful API using Amazon API Gateway. So, Fargate with Lambda as the front-facing service is a wrong combination, though both Fargate and Lambda are serverless.</p>\n\n<p><strong>Public-facing Application Load Balancer with ECS on Amazon EC2</strong> - ECS on Amazon EC2 does not come under serverless and hence cannot be considered for this use case.</p>\n\n<p><strong>Route 53 with EC2 as backend</strong> - Amazon EC2 is not a serverless service and hence cannot be considered for this use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/serverless/\">https://aws.amazon.com/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/api-gateway/\">https://aws.amazon.com/api-gateway/</a></p>\n",
			"question": "<p>As an AWS Certified Solutions Architect Associate, you have been hired to work with the engineering team at a company to create a REST API using the serverless architecture.</p>\n\n<p>Which of the following solutions will you choose to move the company to the serverless architecture paradigm?</p>\n",
			"answers": [
				"<p>API Gateway exposing Lambda Functionality</p>",
				"<p>Fargate with Lambda at the front</p>",
				"<p>Public-facing Application Load Balancer with ECS on Amazon EC2</p>",
				"<p>Route 53 with EC2 as backend</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "As an AWS Certified Solutions Architect Associate, you have been hired to work with the engineering team at a company to create a REST API using the serverless architecture.\n\nWhich of the following solutions will you choose to move the company to the serverless architecture paradigm?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815848,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:</p>\n\n<p><strong>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</strong> - The network access control lists (ACLs) that are associated with the subnet must have rules to allow inbound and outbound traffic on port 80 (for HTTP traffic) and port 443 (for HTTPs traffic). This is a necessary condition for Internet Gateway connectivity</p>\n\n<p><strong>The route table in the instance’s subnet should have a route to an Internet Gateway</strong> - A route table contains a set of rules, called routes, that are used to determine where network traffic from your subnet or gateway is directed. The route table in the instance’s subnet should have a route defined to the Internet Gateway.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The instance's subnet is not associated with any route table</strong> - This is an incorrect statement. A subnet is implicitly associated with the main route table if it is not explicitly associated with a particular route table. So, a subnet is always associated with some route table.</p>\n\n<p><strong>The instance's subnet is associated with multiple route tables with conflicting configurations</strong> - This is an incorrect statement. A subnet can only be associated with one route table at a time.</p>\n\n<p><strong>The subnet has been configured to be Public and has no access to internet</strong> - This is an incorrect statement. Public subnets have access to the internet via Internet Gateway.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html</a></p>\n",
			"question": "<p>While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway.</p>\n\n<p>Which conditions should be met for internet connectivity to be established? (Select two)</p>\n",
			"answers": [
				"<p>The instance's subnet is not associated with any route table</p>",
				"<p>The network ACLs associated with the subnet must have rules to allow inbound and outbound traffic</p>",
				"<p>The instance's subnet is associated with multiple route tables with conflicting configurations</p>",
				"<p>The route table in the instance’s subnet should have a route to an Internet Gateway</p>",
				"<p>The subnet has been configured to be Public and has no access to the internet</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"b",
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "While troubleshooting, a cloud architect realized that the Amazon EC2 instance is unable to connect to the Internet using the Internet Gateway.\n\nWhich conditions should be met for internet connectivity to be established? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815850,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>You have just terminated an instance in the us-west-1a availability zone. The attached EBS volume is now available for attachment to other instances. An intern launches a new Linux EC2 instance in the us-west-1b availability zone and is attempting to attach the EBS volume. The intern informs you that it is not possible and needs your help.</p>\n\n<p>Which of the following explanations would you provide to them?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>EBS volumes are AZ locked</strong></p>\n\n<p>An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible. For current-generation volumes attached to current-generation instance types, you can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.</p>\n\n<p>When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss due to the failure of any single hardware component. You can attach an EBS volume to an EC2 instance in the same Availability Zone.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>EBS volumes are region locked</strong> - It's confined to an Availability Zone and not by region.</p>\n\n<p><strong>The required IAM permissions are missing</strong> - This is a possibility as well but if permissions are not an issue then you are still confined to an availability zone.</p>\n\n<p><strong>The EBS volume is encrypted</strong> - This doesn't affect the ability to attach an EBS volume.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html</a></p>\n",
			"answers": [
				"<p>The required IAM permissions are missing</p>",
				"<p>EBS volumes are region locked</p>",
				"<p>EBS volumes are AZ locked</p>",
				"<p>The EBS volume is encrypted</p>"
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "You have just terminated an instance in the us-west-1a availability zone. The attached EBS volume is now available for attachment to other instances. An intern launches a new Linux EC2 instance in the us-west-1b availability zone and is attempting to attach the EBS volume. The intern informs you that it is not possible and needs your help.\n\nWhich of the following explanations would you provide to them?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 25815852,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>The ASG will terminate the EC2 Instance</strong></p>\n\n<p>To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG will detach the EC2 instance from the group, and leave it running</strong> - The goal of the auto-scaling group is to get rid of the bad instance and replace it.</p>\n\n<p><strong>The ASG will keep the instance running and re-start the application</strong> - The ASG does not have control of your application.</p>\n\n<p><strong>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</strong> - This will not happen, the ASG cannot assume the format of your EBS drive, and User Data only runs once at instance first boot.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 10, a maximum value of 30, and the desired capacity value of 20. One of the 20 EC2 instances has been reported as unhealthy.</p>\n\n<p>Which of the following actions will take place?</p>\n",
			"answers": [
				"<p>The ASG will terminate the EC2 Instance</p>",
				"<p>The ASG will detach the EC2 instance from the group, and leave it running</p>",
				"<p>The ASG will keep the instance running and re-start the application</p>",
				"<p>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "You create an Auto Scaling group to work with an Application Load Balancer. The scaling group is configured with a minimum size value of 10, a maximum value of 30, and the desired capacity value of 20. One of the 20 EC2 instances has been reported as unhealthy.\n\nWhich of the following actions will take place?",
		"related_lectures": []
	}
]