[
	{
		"_class": "assessment",
		"id": 33101898,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one that can increase capacity by adding more computers to the system. This is in contrast to a vertically scalable system, which is constrained to running its processes on only one computer; in such systems, the only way to increase performance is to add more resources into one computer in the form of faster (or more) CPUs, memory or storage.</p>\n\n<p>Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different computers.</p>\n\n<p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p>\n\n<p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group.</p>\n\n<p>When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by your Auto Scaling group are automatically registered with the load balancer. Likewise, instances that are terminated by your Auto Scaling group are automatically deregistered from the load balancer.</p>\n\n<p>This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this is the right solution for the current use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the application uses synchronous transactions. The question says there should be no change in the application architecture.</p>\n\n<p><strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option has only been used as a distractor.</p>\n\n<p>You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster technology from Oracle that allows a single database (a set of data files) to be concurrently accessed and served by one or many database server instances.</p>\n\n<p><strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and will not work long term.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/\">https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/</a></p>\n",
			"answers": [
				"<p>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</p>",
				"<p>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</p>",
				"<p>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</p>",
				"<p>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</p>"
			],
			"question": "<p>A retail company's procurement application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The engineering team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture.</p>\n\n<p>As a solutions architect, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A retail company's procurement application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The engineering team at the company has identified certain bottlenecks in the application tier but it does not want to change the underlying application architecture.\n\nAs a solutions architect, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101900,
		"assessment_type": "multi-select",
		"prompt": {
			"answers": [
				"<p>Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</p>",
				"<p>Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</p>",
				"<p>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</p>",
				"<p>Spot Fleet allows you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</p>",
				"<p>Spot blocks are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot blocks are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</p>",
				"<p>A Spot block is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot instances can be interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</strong></p>\n\n<p>Spot instances are spare EC2 capacity that can save you up 90% off of On-Demand prices that Amazon Web Services can interrupt with a 2-minute notification. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.</p>\n\n<p><strong>Spot blocks allow you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</strong></p>\n\n<p>Spot blocks are designed not to be interrupted and will run continuously for the duration you select (1 to 6 hours), independent of the Spot market price. In rare situations, Spot blocks may be interrupted due to Amazon Web Services' capacity needs. In these cases, AWS will provide a two-minute warning before it terminates your instance and you will not be charged for the affected instance(s).</p>\n\n<p><strong>A Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</strong></p>\n\n<p>A Spot Fleet is a collection, or fleet, of Spot Instances, and optionally On-Demand Instances. The Spot Fleet attempts to launch the number of Spot Instances and On-Demand Instances to meet the target capacity that you specified in the Spot Fleet request. A Spot Fleet allows you to automatically request and manage multiple Spot instances that provide the lowest price per unit of capacity for your cluster or application, like a batch processing job, a Hadoop workflow, or an HPC grid computing job.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q2-i1.jpg\">\nvia - <a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Fleet allows you to request Amazon EC2 Spot instances for 1 to 6 hours at a time to avoid being interrupted</strong></p>\n\n<p><strong>Spot blocks are spare EC2 capacity that can save you up 90% off of On-Demand prices. Spot blocks are usually interrupted by Amazon EC2 for capacity requirements with a 2-minute notification</strong></p>\n\n<p><strong>A Spot block is a set of Spot Instances and optionally On-Demand Instances that are launched to meet your target capacity</strong></p>\n\n<p>These three options contradict the explanation provided above, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://www.amazonaws.cn/en/ec2/spot-instances/faqs/\">https://www.amazonaws.cn/en/ec2/spot-instances/faqs/</a></p>\n\n<p><a href=\"https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html\">https://docs.amazonaws.cn/en_us/AWSEC2/latest/UserGuide/how-spot-fleet-works.html</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>A software engineering intern at a company is documenting the features offered by EC2 Spot instances, Spot blocks, and Spot Fleets.</p>\n\n<p>Can you help the intern by selecting the correct options that identify the key characteristics of these three types of Spot entities? (Select three)</p>\n"
		},
		"correct_response": [
			"a",
			"b",
			"c"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A software engineering intern at a company is documenting the features offered by EC2 Spot instances, Spot blocks, and Spot Fleets.\n\nCan you help the intern by selecting the correct options that identify the key characteristics of these three types of Spot entities? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101902,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</strong> - Amazon Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, data stores, and analytics services. It can capture, transform, and deliver streaming data to Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints, and service providers like Datadog, New Relic, MongoDB, and Splunk.</p>\n\n<p>Amazon Kinesis Data Firehose is the easiest way to capture, transform, and load streaming data into Redshift for near real-time analytics. It is also an auto-scaling solution as there is no need to provision any shards like Kinesis Data Streams.</p>\n\n<p>Redshift allows you to run complex analytic queries against petabytes of structured data, using sophisticated query optimization, columnar storage on high-performance local disks, and massively parallel query execution. Most results come back in seconds.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Capture data in an EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</strong> - Instance store is a temporary storage available on Amazon EC2 instances. Data in the instance store is lost if the instance is rebooted. Hence, this cannot be the option for the current use case.</p>\n\n<p><strong>Capture data in an EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</strong> - EBS volumes cannot be used to store high volume data. EBS can be used to store cache data if a database is hosted on an EC2 instance. However, EBS cannot be used in place of a database. ElastiCache is a caching service. It is not relevant to the given use case.</p>\n\n<p><strong>Capture data in Amazon Kinesis Data Streams. Use Kinesis Data Analytics to query and analyze this streaming data in real-time</strong> - For Kinesis Data Streams, you have to manually allocate the shards for scaling the data ingestion process. Kinesis Data Streams (KDS) and Kinesis Data Analytics are for real-time processing of data and cannot provide long-term storage of data unlike a database or a data warehouse. So, this option is not right for the current use case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/redshift/features/\">https://aws.amazon.com/redshift/features/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\">https://aws.amazon.com/kinesis/data-firehose/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-analytics/faqs/\">https://aws.amazon.com/kinesis/data-analytics/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/kinesis/data-streams/faqs/\">https://aws.amazon.com/kinesis/data-streams/faqs/</a></p>\n",
			"question": "<p>A Big Data analytics company is using a fleet of Amazon EC2 instances to ingest Internet-of-Things (IoT) data from various data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is restarted, the in-flight data is lost. The analytics team at the company wants to query ingested data in near-real-time.</p>\n\n<p>Which of the following solutions provides near-real-time data querying that is scalable with minimal data loss?</p>\n",
			"answers": [
				"<p>Capture data in an EC2 instance store and then publish this data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data</p>",
				"<p>Capture data in an EBS volume and then publish this data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data</p>",
				"<p>Capture data in Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data</p>",
				"<p>Capture data in Amazon Kinesis Data Streams. Use Kinesis Data Analytics to query and analyze this streaming data in real-time</p>"
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A Big Data analytics company is using a fleet of Amazon EC2 instances to ingest Internet-of-Things (IoT) data from various data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is restarted, the in-flight data is lost. The analytics team at the company wants to query ingested data in near-real-time.\n\nWhich of the following solutions provides near-real-time data querying that is scalable with minimal data loss?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101904,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:</p>\n\n<p><strong>AWS WAF</strong></p>\n\n<p><strong>AWS Shield Advanced</strong></p>\n\n<p><strong>VPC Security Groups</strong></p>\n\n<p>AWS Firewall Manager is a security management service which allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure.</p>\n\n<p>Using AWS Firewall Manager, you can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q4-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/firewall-manager/faqs/\">https://aws.amazon.com/firewall-manager/faqs/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon GuardDuty</strong> - Amazon GuardDuty offers threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. GuardDuty analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs.</p>\n\n<p>How GaurdDuty Works:\n<img src=\"https://d1.awsstatic.com/product-marketing/Amazon%20GuardDuty/product-page-diagram-Amazon-GuardDuty_how-it-works.a4daf7e3aaf3532623a3797dd3af606a85fc2e7b.png\"></p>\n\n<p><strong>Amazon Inspector</strong> - Amazon Inspector is an automated security assessment service that helps you test the network accessibility of your Amazon EC2 instances and the security state of your applications running on the instances.</p>\n\n<p><strong>Network Access Control Lists (NACLs)</strong> - A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.</p>\n\n<p>These three options are not in the list of AWS resources supported by AWS Firewall Manager, so these options are incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/firewall-manager/faqs/\">https://aws.amazon.com/firewall-manager/faqs/</a></p>\n\n<p><a href=\"https://aws.amazon.com/guardduty/\">https://aws.amazon.com/guardduty/</a></p>\n\n<p><a href=\"https://aws.amazon.com/inspector/\">https://aws.amazon.com/inspector/</a></p>\n",
			"question": "<p>The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations.</p>\n\n<p>Which of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>AWS WAF</p>",
				"<p>Amazon GuardDuty</p>",
				"<p>Amazon Inspector</p>",
				"<p>AWS Shield Advanced</p>",
				"<p>Network Access Control Lists (NACLs)</p>",
				"<p>VPC Security Groups</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a",
			"d",
			"f"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "The engineering team at a multi-national company uses AWS Firewall Manager to centrally configure and manage firewall rules across its accounts and applications using AWS Organizations.\n\nWhich of the following AWS resources can the AWS Firewall Manager configure rules on? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101906,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Set up an object-created event notification within the S3 bucket to invoke Amazon Kinesis Data Streams to process the uploaded files</p>",
				"<p>Set up AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the uploaded files</p>",
				"<p>Set up an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the uploaded files</p>",
				"<p>Trigger an SNS topic via an S3 event notification when any file is uploaded to Amazon S3. Invoke an AWS Lambda function to process the files</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the uploaded files</strong></p>\n\n<p>You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n\n<p>Currently, Amazon S3 can publish notifications for the following events: New object-created events, Object removal events, Restore object events, Reduced Redundancy Storage (RRS) object lost events, Replication events.</p>\n\n<p>For this use case, we use the <code>New object created</code> event notification to invoke a Lambda function.</p>\n\n<p>Lambda can run custom code in response to Amazon S3 bucket events. When Amazon S3 detects an event of a specific type (for example, an object-created event), it can publish the event to AWS Lambda and invoke your Lambda function.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an object-created event notification within the S3 bucket to invoke Amazon Kinesis Data Streams to process the uploaded files</strong> - Amazon Kinesis Data Streams cannot be directly invoked from the S3 event notifications. Amazon S3 supports the following destinations where it can publish events: Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS) queue, AWS Lambda.</p>\n\n<p><strong>Set up AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the uploaded files</strong> - AppSync enables you to manage and update mobile app data in real-time between devices and the cloud and allows apps to interact with the data on the mobile device when it is offline. Even if you enable AWS CloudTrail trails to log S3 API calls, you cannot integrate AppSync to provide a solution for the given use case.</p>\n\n<p><strong>Trigger an SNS topic via an S3 event notification when any file is uploaded to Amazon S3. Invoke an AWS Lambda function to process the files</strong> - As discussed above, SNS, SQS, and AWS Lambda are the only supported destinations for publishing events from S3. So, configuring the Lambda function directly to an S3 event makes more sense than using an SNS topic that will eventually trigger the Lambda function to process the request.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html</a></p>\n",
			"question": "<p>A file sharing application is used to upload files to an Amazon S3 bucket. Once uploaded, the files are processed to extract metadata, which takes less than 3 seconds. The volume and frequency of the uploads vary from a few files each hour to hundreds of concurrent uploads. The company has hired you as an AWS Certified Solutions Architect Associate to design a cost-effective architecture that will meet these requirements.</p>\n\n<p>Which of the following will you recommend?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A file sharing application is used to upload files to an Amazon S3 bucket. Once uploaded, the files are processed to extract metadata, which takes less than 3 seconds. The volume and frequency of the uploads vary from a few files each hour to hundreds of concurrent uploads. The company has hired you as an AWS Certified Solutions Architect Associate to design a cost-effective architecture that will meet these requirements.\n\nWhich of the following will you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101908,
		"assessment_type": "multi-select",
		"prompt": {
			"question": "<p>The DevOps team at an e-commerce company has deployed a fleet of EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within the us-east-1 region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the EC2 instances under the ASG. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones to become unbalanced. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check.</p>\n\n<p>Can you identify the correct outcomes for these events? (Select two)</p>\n",
			"answers": [
				"<p>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application</p>",
				"<p>Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance</p>",
				"<p>Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it</p>",
				"<p>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched</p>",
				"<p>Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct options:</p>\n\n<p><strong>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application</strong></p>\n\n<p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.\nActions such as changing the Availability Zones for your group or explicitly terminating or detaching instances can lead to the Auto Scaling group becoming unbalanced between Availability Zones. Amazon EC2 Auto Scaling compensates by rebalancing the Availability Zones.</p>\n\n<p>When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones, so that rebalancing does not compromise the performance or availability of your application. Therefore, this option is correct.</p>\n\n<p>Availability Zone Rebalancing Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q6-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance</strong></p>\n\n<p>However, the scaling activity of Auto Scaling works in a different sequence compared to the rebalancing activity. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity for launching a new instance to replace the unhealthy instance. Later, EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it</strong> - This option contradicts the correct sequence of events outlined earlier for scaling activity created by EC2 Auto Scaling. Actually, Auto Scaling first terminates the unhealthy instance and then launches a new instance. Hence this is incorrect.</p>\n\n<p><strong>As the Availability Zones got unbalanced, Amazon EC2 Auto Scaling will compensate by rebalancing the Availability Zones. When rebalancing, Amazon EC2 Auto Scaling terminates old instances before launching new instances, so that rebalancing does not cause extra instances to be launched</strong> - This option contradicts the correct sequence of events outlined earlier for rebalancing activity. When rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones. Hence this is incorrect.</p>\n\n<p><strong>Amazon EC2 Auto Scaling creates a new scaling activity to terminate the unhealthy instance and launch the new instance simultaneously</strong> - This is a made-up option as both the terminate and launch activities can't happen simultaneously. This option has been added as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a",
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The DevOps team at an e-commerce company has deployed a fleet of EC2 instances under an Auto Scaling group (ASG). The instances under the ASG span two Availability Zones (AZ) within the us-east-1 region. All the incoming requests are handled by an Application Load Balancer (ALB) that routes the requests to the EC2 instances under the ASG. As part of a test run, two instances (instance 1 and 2, belonging to AZ A) were manually terminated by the DevOps team causing the Availability Zones to become unbalanced. Later that day, another instance (belonging to AZ B) was detected as unhealthy by the Application Load Balancer's health check.\n\nCan you identify the correct outcomes for these events? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101910,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region</p>",
				"<p>Use the ELB to distribute traffic for four EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region</p>",
				"<p>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region</p>",
				"<p>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region</p>"
			],
			"question": "<p>A startup uses a fleet of EC2 servers to manage its CRM application. These EC2 servers are behind an Elastic Load Balancer (ELB). Which of the following configurations are NOT allowed for the Elastic Load Balancer?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. Two of these instances are deployed in Availability Zone A of us-east-1 region and the other two instances are deployed in Availability Zone B of us-west-1 region</strong></p>\n\n<p>Elastic Load Balancer automatically distributes incoming traffic across multiple targets – Amazon EC2 instances, containers, IP addresses, and Lambda functions – in multiple Availability Zones and ensures only healthy targets receive traffic.\nELB cannot distribute incoming traffic for targets deployed in different regions. This configuration is NOT allowed for the Elastic Load Balancer and therefore this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed across two Availability Zones of us-east-1 region</strong></p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone A of us-east-1 region</strong></p>\n\n<p><strong>Use the ELB to distribute traffic for four EC2 instances. All the four instances are deployed in Availability Zone B of us-west-1 region</strong></p>\n\n<p>These three options are valid configurations for the ELB to distribute traffic (either within an Availability Zone or between two Availability Zones).</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticloadbalancing/\">https://aws.amazon.com/elasticloadbalancing/</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A startup uses a fleet of EC2 servers to manage its CRM application. These EC2 servers are behind an Elastic Load Balancer (ELB). Which of the following configurations are NOT allowed for the Elastic Load Balancer?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101912,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend.</p>\n\n<p>Which of the following is the correct outcome during the maintenance window?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete</strong></p>\n\n<p>Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching, and backups.</p>\n\n<p>Upgrades to the database engine level require downtime. Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your DB instance.</p>\n\n<p>RDS DB Engine Maintenance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q8-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the standby DB instance to be upgraded which is then followed by the upgrade of the primary DB instance. This does not cause any downtime for the duration of the upgrade</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p><strong>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the primary DB instance to be upgraded which is then followed by the upgrade of the standby DB instance. This does not cause any downtime for the duration of the upgrade</strong> - For RDS database engine level upgrade, primary and standby DB instances are upgraded at the same time and it causes downtime until the upgrade is complete, hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/\">https://aws.amazon.com/premiumsupport/knowledge-center/rds-required-maintenance/</a></p>\n",
			"answers": [
				"<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. However, this does not cause any downtime until the upgrade is complete</p>",
				"<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the standby DB instance to be upgraded which is then followed by the upgrade of the primary DB instance. This does not cause any downtime for the duration of the upgrade</p>",
				"<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers the primary DB instance to be upgraded which is then followed by the upgrade of the standby DB instance. This does not cause any downtime for the duration of the upgrade</p>",
				"<p>Any database engine level upgrade for an RDS DB instance with Multi-AZ deployment triggers both the primary and standby DB instances to be upgraded at the same time. This causes downtime until the upgrade is complete</p>"
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The DevOps team at a major financial services company uses Multi-Availability Zone (Multi-AZ) deployment for its MySQL RDS database in order to automate its database replication and augment data durability. The DevOps team has scheduled a maintenance window for a database engine level upgrade for the coming weekend.\n\nWhich of the following is the correct outcome during the maintenance window?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101914,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Store the images using the S3 Intelligent-Tiering storage class</strong></p>\n\n<p>The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access.</p>\n\n<p>For a small monthly monitoring and automation fee per object, Amazon S3 monitors access patterns of the objects in S3 Intelligent-Tiering and moves the ones that have not been accessed for 30 consecutive days to the infrequent access tier. If an object in the infrequent access tier is accessed, it is automatically moved back to the frequent access tier. Therefore using the S3 Intelligent-Tiering storage class is the correct solution for the given problem statement.</p>\n\n<p>S3 Storage Classes Overview:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q9-i1.jpg\"></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Store the images using the S3 Standard-IA storage class</strong></p>\n\n<p>S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days. As some of the objects are frequently accessed, the per GB retrieval fee for S3 Standard-IA can cause the costs to shoot up, hence this option is incorrect.</p>\n\n<p><strong>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 One Zone-IA and the frequently accessed objects are migrated to S3 Standard class</strong></p>\n\n<p><strong>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 Standard-IA and the frequently accessed objects are migrated to S3 Standard class</strong></p>\n\n<p>Creating a data monitoring application on an EC2 instance for managing the desired S3 storage class entails significant development cost as well as infrastructure maintenance effort. The S3 Intelligent-Tiering storage class does the job in a cost-effective way. Therefore both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p>\n",
			"answers": [
				"<p>Store the images using the S3 Intelligent-Tiering storage class</p>",
				"<p>Store the images using the S3 Standard-IA storage class</p>",
				"<p>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 One Zone-IA and the frequently accessed objects are migrated to S3 Standard class</p>",
				"<p>Create a data monitoring application on an EC2 instance in the same region as the bucket storing the images. The application is triggered daily via CloudWatch and it changes the storage class of infrequently accessed objects to S3 Standard-IA and the frequently accessed objects are migrated to S3 Standard class</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>The engineering team at a social media company has noticed that while some of the images stored in S3 are frequently accessed, others sit idle for a considerable span of time.</p>\n\n<p>As a solutions architect, what is your recommendation to build the MOST cost-effective solution?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "The engineering team at a social media company has noticed that while some of the images stored in S3 are frequently accessed, others sit idle for a considerable span of time.\n\nAs a solutions architect, what is your recommendation to build the MOST cost-effective solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101916,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>The CTO of an online home rental marketplace wants to re-engineer the caching layer of the current architecture for its relational database. The CTO wants the caching layer to have replication and archival support built into the architecture.</p>\n\n<p>Which of the following AWS service offers the capabilities required for the re-engineering of the caching layer?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>ElastiCache for Redis</strong></p>\n\n<p>Amazon ElastiCache for Redis is a blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, media streaming, queues, real-time analytics, and session store. ElastiCache for Redis supports replication and archival snapshots right out of the box. Hence this is the correct option.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison sheet for Redis vs Memcached features:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q10-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>ElastiCache for Memcached</strong> - Amazon ElastiCache for Memcached is a Memcached-compatible in-memory key-value store service that can be used as a cache or a data store. Amazon ElastiCache for Memcached is a great choice for implementing an in-memory cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database. Session stores are easy to create with Amazon ElastiCache for Memcached. ElastiCache for Memcached does not support replication and archival snapshots, so this option is ruled out.</p>\n\n<p><strong>DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications. DAX cannot be used as a caching layer for a relational database.</p>\n\n<p><strong>DocumentDB</strong> - Amazon DocumentDB is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. DocumentDB cannot be used as a caching layer for a relational database.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis/\">https://aws.amazon.com/elasticache/redis/</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>DynamoDB Accelerator (DAX)</p>",
				"<p>ElastiCache for Memcached</p>",
				"<p>ElastiCache for Redis</p>",
				"<p>DocumentDB</p>"
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The CTO of an online home rental marketplace wants to re-engineer the caching layer of the current architecture for its relational database. The CTO wants the caching layer to have replication and archival support built into the architecture.\n\nWhich of the following AWS service offers the capabilities required for the re-engineering of the caching layer?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101918,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:\n<strong>Run the custom scripts as user data scripts on the Amazon EC2 instances</strong> - When you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.</p>\n\n<p>By default, user data scripts and cloud-init directives run only during the boot cycle when you first launch an instance. Hence, no extra configuration is needed, apart from including the custom scripts in user data scripts.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process</strong> - You can update your configuration to ensure that your user data scripts and cloud-init directives run every time you restart your instance. By default, the scripts are run, only once during the boot process of the instance.</p>\n\n<p><strong>Run the custom scripts as instance metadata scripts on the Amazon EC2 instances</strong> - Instance metadata is data about your instance that you can use to configure or manage the running instance. Metadata cannot be used to run custom scripts.</p>\n\n<p><strong>Use AWS CLI to run the user data scripts only once while launching the instance</strong> - This statement is incorrect and used only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon EC2 instances that host their application.</p>\n\n<p>Which of the following represents the best way of configuring a solution for this requirement with minimal effort?</p>\n",
			"answers": [
				"<p>Update Amazon EC2 instance configuration to ensure that the custom scripts, added as user data scripts, are run only during the boot process</p>",
				"<p>Run the custom scripts as user data scripts on the Amazon EC2 instances</p>",
				"<p>Run the custom scripts as instance metadata scripts on the Amazon EC2 instances</p>",
				"<p>Use AWS CLI to run the user data scripts only once while launching the instance</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A systems administration team has a requirement to run certain custom scripts only once during the launch of the Amazon EC2 instances that host their application.\n\nWhich of the following represents the best way of configuring a solution for this requirement with minimal effort?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101922,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.</p>\n\n<p>Which of the following actions meets the given requirements?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</strong> - Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines.</p>\n\n<p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with <em>/</em> permissions to the user.</p>\n\n<p>SCPs don't affect users or roles in the management account. They affect only the member accounts in your organization.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</strong> - Configuring each developer account individually is not a viable solution to start with. In addition, any configuration changes can be undone by the user once they are logged into their individual accounts as root users.</p>\n\n<p><strong>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</strong> - The root user can modify this IAM policy itself, so this option is not correct.</p>\n\n<p><strong>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</strong> - A service-linked role is a unique type of IAM role that is linked directly to an AWS service. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. The linked service also defines how you create, modify, and delete a service-linked role.</p>\n\n<p>The linked service defines the permissions of its service-linked roles, and unless defined otherwise, only that service can assume the roles. The defined permissions include the trust policy and the permissions policy, and that permissions policy cannot be attached to any other entity such as the ARN in the master account.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Configure a new trail in CloudTrail from within the developer accounts with the organization trails option enabled</p>",
				"<p>Set up a service control policy (SCP) that prohibits changes to CloudTrail, and attach it to the developer accounts</p>",
				"<p>Set up an IAM policy that prohibits changes to CloudTrail and attach it to the root user</p>",
				"<p>Set up a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the master account</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A security consultant is designing a solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Since the individual developers will have AWS account root user-level access to their own accounts, the consultant wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified.\n\nWhich of the following actions meets the given requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101924,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</strong> - After a successful write of a new object, or an overwrite or delete of an existing object, any subsequent read request immediately receives the latest version of the object. S3 also provides strong consistency for list operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes reflected.</p>\n\n<p>For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.</p>\n\n<p>This improvement is great for data lakes, but other types of applications will also benefit. Because S3 now has strong consistency, migration of on-premises workloads and storage to AWS should now be easier than ever before.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</strong></p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</strong></p>\n\n<p>As mentioned in the explanation above, Amazon S3 is strongly consistent for all GET, PUT and LIST operations, so these two options are incorrect</p>\n\n<p><strong>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</strong> - For all existing and new objects, and in all regions, all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are now strongly consistent. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/s3/consistency/\">https://aws.amazon.com/s3/consistency/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/\">https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/</a></p>\n",
			"question": "<p>A company runs an analytics workload with heavy reads and writes through the workload lifecycle. The development team at the company is interested in using Amazon S3 as the data lake to support this workload. The team has hired you as a solutions architect to advise them on the S3 data consistency model.</p>\n\n<p>Which of the following statements would you identify as correct?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Amazon S3 is strongly consistent for all GET operations and eventually consistent for PUT and LIST operations</p>",
				"<p>Amazon S3 is strongly consistent for all GET and PUT operations and eventually consistent for LIST operations</p>",
				"<p>Amazon S3 is strongly consistent for all GET, PUT and LIST operations</p>",
				"<p>Amazon S3 is strongly consistent for all GET, PUT and LIST operations and eventually consistent for operations that need metadata information</p>"
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A company runs an analytics workload with heavy reads and writes through the workload lifecycle. The development team at the company is interested in using Amazon S3 as the data lake to support this workload. The team has hired you as a solutions architect to advise them on the S3 data consistency model.\n\nWhich of the following statements would you identify as correct?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101926,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon GuardDuty</strong> - Amazon GuardDuty continuously monitors for malicious or unauthorized behavior to help protect your AWS resources, including your AWS accounts and access keys. GuardDuty identifies any unusual or unauthorized activity, like cryptocurrency mining or infrastructure deployments in a region that has never been used. Powered by threat intelligence and machine learning, GuardDuty is continuously evolving to help you protect your AWS environment.</p>\n\n<p>The cryptocurrency finding expands the service’s ability to detect Amazon EC2 instances querying IP addresses associated with the cryptocurrency-related activity. The finding type is: CryptoCurrency:EC2/BitcoinTool.B, CryptoCurrency:EC2/BitcoinTool.B!DNS.</p>\n\n<p>This finding informs you that the listed EC2 instance in your AWS environment is querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin mining and is highly sought after by threat actors.</p>\n\n<p>If you use the EC2 instance to mine or manage cryptocurrency, or this instance is otherwise involved in blockchain activity, this finding could represent expected activity for your environment. If this is the case in your AWS environment, AWS recommends that you set up a suppression rule for this finding.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Web Application Firewall (AWS WAF)</strong> - AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting.</p>\n\n<p><strong>AWS Shield Advanced</strong> - For higher levels of protection against attacks targeting your applications running on Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 resources, you can subscribe to AWS Shield Advanced. In addition to the network and transport layer protections that come with Standard, AWS Shield Advanced provides additional detection and mitigation against large and sophisticated DDoS attacks, near real-time visibility into attacks, and integration with AWS WAF, a web application firewall. AWS Shield Advanced also gives you 24x7 access to the AWS DDoS Response Team (DRT) and protection against DDoS-related spikes in your Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 charges.</p>\n\n<p><strong>AWS Firewall Manager</strong> - AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. As new applications are created, Firewall Manager makes it easy to bring new applications and resources into compliance by enforcing a common set of security rules. Now you have a single service to build firewall rules, create security policies, and enforce them in a consistent, hierarchical manner across your entire infrastructure, from a central administrator account.</p>\n\n<p>None of these three services can detect unauthorized cryptocurrency mining activity on EC2 instances, so these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns\">https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-ec2.html#cryptocurrency-ec2-bitcointoolbdns</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>AWS Web Application Firewall (AWS WAF)</p>",
				"<p>AWS Shield Advanced</p>",
				"<p>AWS Firewall Manager</p>",
				"<p>Amazon GuardDuty</p>"
			],
			"question": "<p>During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The EC2 instance does not host any authorized application related to cryptocurrency mining.</p>\n\n<p>Which AWS service can be used to protect the EC2 instances from such unauthorized behavior in the future?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "During a review, a security team has flagged concerns over an Amazon EC2 instance querying IP addresses used for cryptocurrency mining. The EC2 instance does not host any authorized application related to cryptocurrency mining.\n\nWhich AWS service can be used to protect the EC2 instances from such unauthorized behavior in the future?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101928,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A company is developing a document management application on AWS. The application runs on EC2 instances in multiple Availability Zones. The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use EBS to store the documents but the team is willing to consider other options to meet the availability requirement.</p>\n\n<p>As a solutions architect, which of the following will you recommend?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 as the document store</strong></p>\n\n<p>Instances that use Amazon EBS for the root device automatically have an Amazon EBS volume attached. When you launch an Amazon EBS-backed instance, AWS creates an Amazon EBS volume for each Amazon EBS snapshot referenced by the AMI you use. An Amazon EBS-backed instance can be stopped and later restarted without affecting data stored in the attached volumes.</p>\n\n<p>Amazon S3 provides access to reliable, fast, and inexpensive data storage infrastructure. It is designed to make web-scale computing easier by enabling you to store and retrieve any amount of data, at any time, from within Amazon EC2 or anywhere on the web. S3 is highly available and can be configured to work as a document store for the given use case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 Glacier as the document store</strong> - As the documents need to be returned immediately when requested, S3 Glacier is not the right fit, since there is a lag of several minutes/hours when you want to read data from Glacier.</p>\n\n<p><strong>Create snapshots for the EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones</strong> - You can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots. Snapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved. Hence, using EBS volumes as a primary storage solution is ineffective, and creating recurring snapshots is a management nightmare for the current use case.</p>\n\n<p><strong>Provision at least three Provisioned IOPS EBS volumes for the EC2 instances and then mount these volumes to the EC2 instances in a RAID 5 configuration</strong> - RAID 5 and RAID 6 are not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the IOPS available to your volumes. Depending on the configuration of your RAID array, these RAID modes provide 20-30% fewer usable IOPS than a RAID 0 configuration. So this option is incorrect.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 Glacier as the document store</p>",
				"<p>Create snapshots for the EBS volumes regularly and then build new volumes using those snapshots in additional Availability Zones</p>",
				"<p>Provision at least three Provisioned IOPS EBS volumes for the EC2 instances and then mount these volumes to the EC2 instances in a RAID 5 configuration</p>",
				"<p>Set up Amazon EBS as the EC2 instance root volume and then configure the application to use S3 as the document store</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A company is developing a document management application on AWS. The application runs on EC2 instances in multiple Availability Zones. The company requires the document store to be highly available and the documents need to be returned immediately when requested. The engineering team has configured the application to use EBS to store the documents but the team is willing to consider other options to meet the availability requirement.\n\nAs a solutions architect, which of the following will you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101930,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>Amazon SQS dead-letter queues</p>",
				"<p>Amazon SQS FIFO queues</p>",
				"<p>Amazon SQS temporary queues</p>",
				"<p>Amazon SQS delay queues</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern.</p>\n\n<p>Which of the following SQS queue types is the best fit to meet this requirement?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon SQS temporary queues</strong> - Temporary queues help you save development time and deployment costs when using common message patterns such as request-response. You can use the Temporary Queue Client to create high-throughput, cost-effective, application-managed temporary queues.</p>\n\n<p>The client maps multiple temporary queues—application-managed queues created on demand for a particular process—onto a single Amazon SQS queue automatically. This allows your application to make fewer API calls and have a higher throughput when the traffic to each temporary queue is low. When a temporary queue is no longer in use, the client cleans up the temporary queue automatically, even if some processes that use the client aren't shut down cleanly.</p>\n\n<p>The following are the benefits of temporary queues:</p>\n\n<ol>\n<li><p>They serve as lightweight communication channels for specific threads or processes.</p></li>\n<li><p>They can be created and deleted without incurring additional costs.</p></li>\n<li><p>They are API-compatible with static (normal) Amazon SQS queues. This means that existing code that sends and receives messages can send messages to and receive messages from virtual queues.</p></li>\n</ol>\n\n<p>To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. This client makes it easy to create and delete many temporary messaging destinations without inflating your AWS bill. The key concept behind the client is the virtual queue. Virtual queues let you multiplex many low-traffic queues onto a single SQS queue. Creating a virtual queue only instantiates a local buffer to hold messages for consumers as they arrive; there is no API call to SQS and no costs associated with creating a virtual queue.</p>\n\n<p>End-to-end process for sending messages through virtual queues:</p>\n\n<p><img src=\"https://d2908q01vomqb2.cloudfront.net/1b6453892473a467d07372d45eb05abc2031647a/2019/07/26/Selection_015.png\">\nvia - <a href=\"https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\">https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon SQS dead-letter queues</strong> - Amazon SQS supports dead-letter queues, which other queues (source queues) can target for messages that can't be processed (consumed) successfully. Dead-letter queues are useful for debugging your application or messaging system because they let you isolate problematic messages to determine why their processing doesn't succeed. Amazon SQS does not create the dead-letter queue automatically. You must first create the queue before using it as a dead-letter queue.</p>\n\n<p><strong>Amazon SQS FIFO queues</strong> - FIFO (First-In-First-Out) queues are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated. FIFO queues also provide exactly-once processing but have a limited number of transactions per second (TPS).</p>\n\n<p><strong>Amazon SQS delay queues</strong> - Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-temporary-queues.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/\">https://aws.amazon.com/blogs/compute/simple-two-way-messaging-using-the-amazon-sqs-temporary-queue-client/</a></p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A development team is looking for a solution that saves development time and deployment costs for an application that uses a high-throughput request-response message pattern.\n\nWhich of the following SQS queue types is the best fit to meet this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101932,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Aurora Serverless</p>",
				"<p>Amazon ElastiCache</p>",
				"<p>DynamoDB with Provisioned Capacity and Auto Scaling</p>",
				"<p>DynamoDB with On-Demand Capacity</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Aurora Serverless</strong></p>\n\n<p>Amazon Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora (MySQL-compatible and PostgreSQL-compatible editions), where the database will automatically start up, shut down, and scale capacity up or down based on your application's needs. It enables you to run your database in the cloud without managing any database instances. It's a simple, cost-effective option for infrequent, intermittent, or unpredictable workloads. The database design for an OLTP application fits the relational model, therefore you can infer an OLTP system as a Relational Database.</p>\n\n<p>Aurora Serverless is the perfect way to create a database that can scale down to 0 servers, and scale up to many servers, as an OLTP database. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB with Provisioned Capacity and Auto Scaling</strong></p>\n\n<p><strong>DynamoDB with On-Demand Capacity</strong></p>\n\n<p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-master, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications.</p>\n\n<p>DynamoDB is a NoSQL database and doesn't do relational queries, therefore it's a choice we have to eliminate, even though the two modes proposed here help us cope with an unpredictable amount of usage. So both these options are incorrect.</p>\n\n<p><strong>Amazon ElastiCache</strong> - Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Elasticache is used as a caching layer in front of relational databases. ElastiCache is a NoSQL database and doesn't facilitate relational queries, so this option is ruled out.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/aurora/serverless/\">https://aws.amazon.com/rds/aurora/serverless/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/\">https://aws.amazon.com/rds/</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application, which will have unpredictable spikes of usage, that they do not know in advance.</p>\n\n<p>Which database would you recommend using?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The engineering team at an IT company is deploying an Online Transactional Processing (OLTP) application, which will have unpredictable spikes of usage, that they do not know in advance.\n\nWhich database would you recommend using?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101934,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Run on a Spot Instance with Spot Block</strong></p>\n\n<p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price.</p>\n\n<p>Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously for the duration you select. This makes them ideal for jobs that take a finite time to complete, such as batch processing, encoding and rendering, modeling and analysis, and continuous integration.</p>\n\n<p>Running our load on a Spot Instance with Spot Block sounds like the perfect use case, as we can block the spot instance for 1 hour, run the script there, and then the instance will be terminated.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Run on an Application Load Balancer</strong> - Application Load Balancer operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>Application Load Balancer helps distribute load for HTTP(S) requests. This option has been added as a distractor.</p>\n\n<p><strong>Run on EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p>EMR is to run Big Data load that is meant to be run on Hadoop, this is also a distractor.</p>\n\n<p><strong>Run on Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>Lambda would be the perfect fit if our script could run in less than 15 minutes, as this is the maximum timeout for Lambda.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/\">https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/</a></p>\n",
			"answers": [
				"<p>Run on a Spot Instance with Spot Block</p>",
				"<p>Run on EMR</p>",
				"<p>Run on Lambda</p>",
				"<p>Run on an Application Load Balancer</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>The development team at a company manages a flexible nightly process which runs for 1 hour using Python. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS.</p>\n\n<p>Which of the following options do you recommend as the MOST cost-effective solution?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "The development team at a company manages a flexible nightly process which runs for 1 hour using Python. The process currently runs on the on-premises infrastructure and it needs to be migrated to AWS.\n\nWhich of the following options do you recommend as the MOST cost-effective solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101936,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create a Read Replica in the same AZ as the Master database and point the analytics workload there</strong></p>\n\n<p>Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. For the MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database engines, Amazon RDS creates a second DB instance using a snapshot of the source DB instance. It then uses the engines' native asynchronous replication to update the read replica whenever there is a change to the source DB instance. Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.</p>\n\n<p>Creating a Read Replica is the answer. As we want to minimize the costs, we need to launch the Read Replica in the same AZ, because we have to pay for inter-AZ data transfer, whereas the transfer of data within a single AZ is free.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please review this comparison vis-a-vis Multi-AZ vs Read Replica for RDS:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q19-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</strong> - Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.</p>\n\n<p>Enabling Multi-AZ helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure. So this option is not correct.</p>\n\n<p><strong>Migrate the analytics application to AWS Lambda</strong>- AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>Running the application on AWS Lambda will not help, as it will still run against the main database and slow down our e-commerce application.</p>\n\n<p><strong>Create a Read Replica in another AZ as the Master database and point the analytics workload there</strong> - This is incorrect because we have to pay for inter-AZ data transfer for the Read Replica, whereas the transfer of data within a single AZ is free.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p>\n\n<p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Create a Read Replica in the same AZ as the Master database and point the analytics workload there</p>",
				"<p>Migrate the analytics application to AWS Lambda</p>",
				"<p>Enable Multi-AZ for the RDS database and run the analytics workload on the standby database</p>",
				"<p>Create a Read Replica in another AZ as the Master database and point the analytics workload there</p>"
			],
			"question": "<p>Your e-commerce application is using an RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales.</p>\n\n<p>Which of the following is the MOST cost-optimal solution to fix this issue?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "Your e-commerce application is using an RDS PostgreSQL database and an analytics workload also runs on the same database. When the analytics workload is run, your e-commerce application slows down which further affects your sales.\n\nWhich of the following is the MOST cost-optimal solution to fix this issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101938,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in RDS</strong></p>\n\n<p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.</p>\n\n<p>Using the Range HTTP header in a GET Object request, you can fetch a byte-range from an object, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. This helps you achieve higher aggregate throughput versus a single whole-object request. Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted.</p>\n\n<p>A byte-range request is a perfect way to get the beginning of a file and ensuring we remain efficient during our scan of our S3 bucket. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use the RDS Import feature to load the data from S3 to PostgreSQL, and run a SQL query to build the index</strong> - You cannot import data from S3 into RDS, so this option is incorrect.</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in RDS</strong> - If you build an application that loads all the files from S3, that would work, but you would read 50TB of data and that may be very expensive and slow. So this option is incorrect.</p>\n\n<p><strong>Create an application that will traverse the S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in RDS</strong> - S3 Select is a new Amazon S3 capability designed to pull out only the data you need from an object, which can dramatically improve the performance and reduce the cost of applications that need to access data in S3. You cannot use Byte Range Fetch parameter with S3 Select to traverse the S3 bucket and get the first bytes of a file. So this option is incorrect.</p>\n\n<p>Exam Alert:</p>\n\n<p>Please note that with Amazon S3 Select, you can scan a subset of an object by specifying a range of bytes to query using the ScanRange parameter. This capability lets you parallelize scanning the whole object by splitting the work into separate Amazon S3 Select requests for a series of non-overlapping scan ranges. Use the Amazon S3 Select ScanRange parameter and Start at (Byte) and End at (Byte).</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q20-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html</a></p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range\">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance-guidelines.html#optimizing-performance-guidelines-get-range</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>You are looking to build an index of your files in S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50TB of data.</p>\n\n<p>How can you build this index efficiently?</p>\n",
			"answers": [
				"<p>Create an application that will traverse the S3 bucket, issue a Byte Range Fetch for the first 250 bytes, and store that information in RDS</p>",
				"<p>Use the RDS Import feature to load the data from S3 to PostgreSQL, and run a SQL query to build the index</p>",
				"<p>Create an application that will traverse the S3 bucket, read all the files one by one, extract the first 250 bytes, and store that information in RDS</p>",
				"<p>Create an application that will traverse the S3 bucket, then use S3 Select Byte Range Fetch parameter to get the first 250 bytes, and store that information in RDS</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "You are looking to build an index of your files in S3, using Amazon RDS PostgreSQL. To build this index, it is necessary to read the first 250 bytes of each object in S3, which contains some metadata about the content of the file itself. There are over 100,000 files in your S3 bucket, amounting to 50TB of data.\n\nHow can you build this index efficiently?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101940,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines.</p>\n\n<p>Which of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Dedicated Instances</p>",
				"<p>Spot Instances</p>",
				"<p>Dedicated Hosts</p>",
				"<p>On-Demand Instances</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Dedicated Instances</strong> - Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer. Dedicated Instances that belong to different AWS accounts are physically isolated at a hardware level, even if those accounts are linked to a single-payer account. However, Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>\n\n<p>A Dedicated Host is also a physical server that's dedicated for your use. With a Dedicated Host, you have visibility and control over how instances are placed on the server.</p>\n\n<p>Differences between Dedicated Hosts and Dedicated Instances:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-overview.html#dedicated-hosts-dedicated-instances</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Spot Instances</strong> -  A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.  Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price. Any instance present with unused capacity will be allocated. Even though this is cost-effective, it does not fulfill the single-tenant hardware requirement of the client and hence is not the correct option.</p>\n\n<p><strong>Dedicated Hosts</strong> - An Amazon EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. Dedicated Hosts allow you to use your existing software licenses on EC2 instances. With a Dedicated Host, you have visibility and control over how instances are placed on the server. This option is costlier than the Dedicated Instance and hence is not the right choice for the current requirement.</p>\n\n<p><strong>On-Demand Instances</strong> - With On-Demand Instances, you pay for the compute capacity by the second with no long-term commitments. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Hardware isolation is not possible and on-demand has one of the costliest instance charges and hence is not the correct answer for current requirements.</p>\n\n<p>High Level Overview of EC2 Instance Purchase Options:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q21-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/ec2/pricing/\">https://aws.amazon.com/ec2/pricing/</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html</a></p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A healthcare company wants to run its applications on single-tenant hardware to meet compliance guidelines.\n\nWhich of the following is the MOST cost-effective way of isolating the Amazon EC2 instances to a single tenant?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101942,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Opt for two separate Direct Connect connections terminating on separate devices in more than one Direct Connect location</strong> - Maximum resilience is achieved by separate connections terminating on separate devices in more than one location. This configuration offers customers maximum resilience to failure. As shown in the figure above, such a topology provides resilience to device failure, connectivity failure, and complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect locations.</p>\n\n<p>Maximum Resiliency for Critical Workloads:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Opt for one Direct Connect connection at each of the multiple Direct Connect locations</strong> - For critical production workloads that require high resiliency, it is recommended to have one connection at multiple locations. As shown in the figure below, such a topology ensures resilience to connectivity failure due to a fiber cut or a device failure as well as a complete location failure. You can use Direct Connect Gateway to access any AWS Region (except AWS Regions in China) from any AWS Direct Connect location.</p>\n\n<p>High Resiliency for Critical Workloads:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i2.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n\n<p><strong>Opt for at least two Direct Connect connections terminating on different devices at a single Direct Connect location</strong> - For non-critical production workloads and development workloads that do not require high resiliency, it is recommended to have at least two connections terminating on different devices at a single location. As shown in the figure above, such a topology helps in the case of the device failure at a location but does not help in the event of a total location failure.</p>\n\n<p>Non Critical Production Workloads or Development Workloads:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q22-i3.jpg\">\nvia - <a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n\n<p><strong>Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency</strong> - It is important to understand that AWS Managed VPN supports up to 1.25 Gbps throughput per VPN tunnel and does not support Equal Cost Multi-Path (ECMP) for egress data path in the case of multiple AWS Managed VPN tunnels terminating on the same VGW. Thus, AWS does not recommend customers use AWS Managed VPN as a backup for AWS Direct Connect connections with speeds greater than 1 Gbps.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/directconnect/resiliency-recommendation/\">https://aws.amazon.com/directconnect/resiliency-recommendation/</a></p>\n",
			"answers": [
				"<p>Opt for one Direct Connect connection at each of the multiple Direct Connect locations</p>",
				"<p>Opt for two separate Direct Connect connections terminating on separate devices in more than one Direct Connect location</p>",
				"<p>Use AWS Managed VPN as a backup for AWS Direct Connect connections to ensure maximum resiliency</p>",
				"<p>Opt for at least two Direct Connect connections terminating on different devices at a single Direct Connect location</p>"
			],
			"question": "<p>To support critical production workloads that require maximum resiliency, a company wants to configure network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct Connect connections with speeds greater than 1 Gbps.</p>\n\n<p>As a solutions architect, which of the following will you suggest as the best architecture for this requirement?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "To support critical production workloads that require maximum resiliency, a company wants to configure network connections between its Amazon VPC and the on-premises infrastructure. The company needs AWS Direct Connect connections with speeds greater than 1 Gbps.\n\nAs a solutions architect, which of the following will you suggest as the best architecture for this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101944,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set</strong> - Amazon S3 encrypts your data at the object level as it writes to disks in AWS data centers, and decrypts it for you when you access it. You can encrypt objects by using client-side encryption or server-side encryption. Client-side encryption occurs when an object is encrypted before you upload it to S3, and the keys are not managed by AWS. With server-side encryption, Amazon manages the keys in one of three ways:</p>\n\n<ol>\n<li>Server-side encryption with customer-provided encryption keys (SSE-C).</li>\n<li>SSE-S3.</li>\n<li>SSE-KMS.</li>\n</ol>\n\n<p>Server-side encryption is about data encryption at rest—that is, S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects.</p>\n\n<p>To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS.</p>\n\n<p>In order to enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMS–managed keys.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private</strong> - The x-amz-acl header is used to specify an ACL in the PutObject request. Access permissions are defined using this header.</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true</strong> - By default, Amazon S3 allows both HTTP and HTTPS requests. aws:SecureTransport key is used to check if the request is sent through HTTP or HTTPS. When this key is true, it means that the request is sent through HTTPS.</p>\n\n<p><strong>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set</strong> - As discussed above, the s3:x-amz-acl header is used to set permissions on the specified S3 bucket and has nothing to do with encryption.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\">https://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html</a></p>\n",
			"answers": [
				"<p>Configure the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set</p>",
				"<p>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private</p>",
				"<p>Configure the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true</p>",
				"<p>Configure the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set</p>"
			],
			"question": "<p>A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?</p>\n\n<p>Which of the following options represents the correct solution?</p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A development team wants to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?\n\nWhich of the following options represents the correct solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101948,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Provisioned IOPS SSD EBS volumes</strong> - Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances that are in the same Availability Zone. You can attach multiple Multi-Attach enabled volumes to an instance or set of instances. Each instance to which the volume is attached has full read and write permission to the shared volume. Multi-Attach makes it easier for you to achieve higher application availability in clustered Linux applications that manage concurrent write operations.</p>\n\n<p>Multi-Attach is supported exclusively on Provisioned IOPS SSD volumes.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>General-purpose SSD-based EBS volumes</strong> - These SSD-backed EBS volumes provide a balance of price and performance. AWS recommends these volumes for most workloads. These volume types are not supported for Multi-Attach functionality.</p>\n\n<p><strong>Throughput Optimized HDD EBS volumes</strong> - These HDD-backed volumes provide a low-cost HDD designed for frequently accessed, throughput-intensive workloads. These volume types are not supported for Multi-Attach functionality.</p>\n\n<p><strong>Cold HDD EBS volumes</strong> - These HDD-backed volumes provide a lowest-cost HDD design for less frequently accessed workloads. These volume types are not supported for Multi-Attach functionality.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html</a></p>\n",
			"answers": [
				"<p>Provisioned IOPS SSD EBS volumes</p>",
				"<p>General-purpose SSD-based EBS volumes</p>",
				"<p>Throughput Optimized HDD EBS volumes</p>",
				"<p>Cold HDD EBS volumes</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>An application is hosted on multiple Amazon EC2 instances in the same Availability Zone. The engineering team wants to set up shared data access for these EC2 instances using EBS Multi-Attach volumes.</p>\n\n<p>Which EBS volume type is the correct choice for these EC2 instances?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An application is hosted on multiple Amazon EC2 instances in the same Availability Zone. The engineering team wants to set up shared data access for these EC2 instances using EBS Multi-Attach volumes.\n\nWhich EBS volume type is the correct choice for these EC2 instances?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101950,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection.</p>\n\n<p>What is the MOST cost-effective way to establish such a connection?</p>\n",
			"answers": [
				"<p>Set up a bastion host on Amazon EC2</p>",
				"<p>Set up AWS Direct Connect</p>",
				"<p>Set up an AWS Site-to-Site VPN connection</p>",
				"<p>Set up an Internet Gateway between the on-premises data center and AWS cloud</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an AWS Site-to-Site VPN connection</strong> - By default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network. You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN (Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection. A VPN connection refers to the connection between your VPC and your own on-premises network.</p>\n\n<p>A Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side.</p>\n\n<p>A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection.</p>\n\n<p>How virtual private gateway works:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p>\n\n<p>A transit gateway is a transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway.</p>\n\n<p>How transit gateway works:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt6-q25-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a bastion host on Amazon EC2</strong> - A bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. The bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC. Other EC2 instances can be in a subnet that is not publicly accessible, and they are set up with a security group that allows SSH access from the security group attached to the underlying EC2 instance running the bastion host. A bastion host cannot be used to set up a connection between its on-premises data center and AWS Cloud.</p>\n\n<p><strong>Set up AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to utilize AWS cloud services. AWS Direct Connect enables customers to have low latency, secure and private connections to AWS for workloads that require higher speed or lower latency than the internet. A Dedicated Connection is made through a 1 Gbps, 10 Gbps, or 100 Gbps Ethernet port dedicated to a single customer. AWS Direct Connect takes about a month to provision the connection, so this option is ruled out for the given use case.</p>\n\n<p><strong>Set up an Internet Gateway between the on-premises data center and AWS cloud</strong> - An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An Internet Gateway cannot be used to set up a connection between its on-premises data center and AWS Cloud.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A retail company needs a secure connection between its on-premises data center and AWS Cloud. This connection does not need high bandwidth and will handle a small amount of traffic. The company wants a quick turnaround time to set up the connection.\n\nWhat is the MOST cost-effective way to establish such a connection?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101954,
		"assessment_type": "multi-select",
		"prompt": {
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Move the data to S3 Standard IA after 30 days</strong> - S3 Standard-IA is for data that is accessed less frequently but requires rapid access when needed. S3 Standard-IA offers high durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB retrieval fee. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. The minimum storage duration charge is 30 days.</p>\n\n<p><strong>Analyze the cold data with Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p>Moving the data to S3 glacier will prevent us from being able to query it.  Therefore, we should migrate the data to S3 Standard IA and use Athena to analyze the cold data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Migrate the Redshift underlying storage to S3 IA</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p>Redshift's internal storage does not have \"tiers\" of storage classes like Amazon S3, so this option is also ruled out.</p>\n\n<p><strong>Create a smaller Redshift Cluster with the cold data</strong> - Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will only increase with time as we keep on creating data. Therefore this option is ruled out.</p>\n\n<p><strong>Move the data to S3 Glacier after 30 days</strong> - Amazon S3 Glacier and S3 Glacier Deep Archive are secure, durable, and extremely low-cost Amazon S3 cloud storage classes for data archiving and long-term backup. They are designed to deliver 99.999999999% durability, and provide comprehensive security and compliance capabilities that can help meet even the most stringent regulatory requirements.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/athena/\">https://aws.amazon.com/athena/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html</a></p>\n",
			"question": "<p>Your company has created a data warehouse using Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Redshift.</p>\n\n<p>What do you recommend? (Select two)</p>\n",
			"answers": [
				"<p>Migrate the Redshift underlying storage to S3 IA</p>",
				"<p>Create a smaller Redshift Cluster with the cold data</p>",
				"<p>Move the data to S3 Glacier after 30 days</p>",
				"<p>Move the data to S3 Standard IA after 30 days</p>",
				"<p>Analyze the cold data with Athena</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"d",
			"e"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "Your company has created a data warehouse using Redshift that is used to analyze data from Amazon S3. From the usage pattern, you have detected that after 30 days, the data is rarely queried in Redshift and it's not \"hot data\" anymore. You would like to preserve the SQL querying capability on your data and get the queries started immediately. Also, you want to adopt a pricing model that allows you to save the maximum amount of cost on Redshift.\n\nWhat do you recommend? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101958,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>An e-commerce website is migrating towards a microservices-based approach for their website and plans to expose their website from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, mycorp.com/products, and mycorp.com/orders. The website would like to use ECS on the backend to manage these microservices and possibly host the same container of the application multiple times on the same EC2 instance.</p>\n\n<p>Which feature can help you achieve this with minimal effort?</p>\n",
			"answers": [
				"<p>Classic Load Balancer + dynamic port mapping</p>",
				"<p>Application Load Balancer + dynamic port mapping</p>",
				"<p>Network Load Balancer + dynamic port mapping</p>",
				"<p>Application Load Balancer + Reverse Proxy running as a Docker daemon on each ECS host</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Application Load Balancer + dynamic port mapping</strong></p>\n\n<p>Application Load Balancer can automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.</p>\n\n<p>Dynamic port mapping with an Application Load Balancer makes it easier to run multiple tasks on the same Amazon ECS service on an Amazon ECS cluster.</p>\n\n<p>Incorrect option:</p>\n\n<p><strong>Application Load Balancer + Reverse Proxy running as a Docker daemon on each ECS host</strong> - Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the ALB has a feature that provides a direct dynamic port mapping feature and integration with the ECS service so we will leverage that.</p>\n\n<p><strong>Classic Load Balancer + dynamic port mapping</strong> - Classic Load Balancer provides basic load balancing across multiple Amazon EC2 instances and operates at both the request level and connection level. Classic Load Balancer is intended for applications that were built within the EC2-Classic network.</p>\n\n<p>With the Classic Load Balancer, you must statically map port numbers on a container instance. The Classic Load Balancer does not allow you to run multiple copies of a task on the same instance because of the ports conflict. An Application Load Balancer uses dynamic port mapping so that you can run multiple tasks from a single service on the same container instance.</p>\n\n<p><strong>Network Load Balancer + dynamic port mapping</strong> - Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/\">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An e-commerce website is migrating towards a microservices-based approach for their website and plans to expose their website from the same load balancer, linked to different target groups with different URLs: checkout.mycorp.com, www.mycorp.com, mycorp.com/products, and mycorp.com/orders. The website would like to use ECS on the backend to manage these microservices and possibly host the same container of the application multiple times on the same EC2 instance.\n\nWhich feature can help you achieve this with minimal effort?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101960,
		"assessment_type": "multi-select",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Amazon Redshift</p>",
				"<p>AWS Glue</p>",
				"<p>Amazon EMR</p>",
				"<p>Amazon Athena</p>",
				"<p>AWS Batch</p>"
			],
			"question": "<p>The engineering team at a company wants to create a daily big data analysis job leveraging Spark for analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client basis. The big data analysis job needs to read the data from Amazon S3 and output it back to S3.</p>\n\n<p>Which technology do you recommend to run the Big Data analysis job? (Select two)</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon EMR</strong> - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. With EMR you can run Petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. EMR is used for launching Hadoop / Spark clusters. For short-running jobs, you can spin up and spin down clusters and pay per second for the instances used. For long-running workloads, you can create highly available clusters that automatically scale to meet demand. Amazon EMR uses Hadoop, an open-source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances.</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. AWS Glue job is meant to be used for batch ETL data processing. AWS Glue ETL jobs can use Amazon S3, data stores in a VPC, or on-premises JDBC data stores as a source. AWS Glue jobs extract data, transform it, and load the resulting data back to S3, data stores in a VPC, or on-premises JDBC data stores as a target.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Redshift</strong> - Amazon Redshift is a fully-managed petabyte-scale cloud-based data warehouse product designed for large scale data set storage and analysis. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. An Amazon Redshift cluster consists of nodes. Each cluster has a leader node and one or more compute nodes. The leader node receives queries from client applications, parses the queries, and develops query execution plans. The leader node then coordinates the parallel execution of these plans with the compute nodes and aggregates the intermediate results from these nodes. It then finally returns the results to the client applications.</p>\n\n<p><strong>Amazon Athena</strong> - Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to set up or manage, and customers pay only for the queries they run. You can use Athena to process logs, perform ad-hoc analysis, and run interactive queries.</p>\n\n<p><strong>AWS Batch</strong> AWS Batch can be used to plan, schedule, and execute your batch computing workloads on Amazon EC2 Instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource requirements of the batch jobs submitted.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/emr/\">https://aws.amazon.com/emr/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/\">https://aws.amazon.com/blogs/big-data/how-to-access-and-analyze-on-premises-data-stores-using-aws-glue/</a></p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b",
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The engineering team at a company wants to create a daily big data analysis job leveraging Spark for analyzing online/offline sales and customer loyalty data to create customized reports on a client-by-client basis. The big data analysis job needs to read the data from Amazon S3 and output it back to S3.\n\nWhich technology do you recommend to run the Big Data analysis job? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101962,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.</p>\n\n<p>How can you provide these users access in the least possible time, with minimal changes?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create a group, attach the policy to the group and place the users in the group</strong> - An IAM group is a collection of IAM users. You can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. For example, you could have a group called Admins and give that group the types of permissions that administrators typically need. Any user in that group automatically has the permissions that are assigned to the group. If a new user joins your organization and should have administrator privileges, you can assign the appropriate permissions by adding the user to that group.</p>\n\n<p>Here creating a group, assigning users to that group and attaching policies to that group is the best way.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the S3 bucket policy</strong> - Updating the S3 bucket policy could work but would not scale, as the size of the S3 bucket policy is limited (Bucket policies are limited to 20 KB in size).</p>\n\n<p><strong>Create a policy and assign it manually to the 50 users</strong> -</p>\n\n<p>An IAM user is an entity that you create in AWS. The IAM user represents the person or service who uses the IAM user to interact with AWS. Primary use for IAM users is to give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. A user in AWS consists of a name, a password to sign in to the AWS Management Console, and up to two access keys that can be used with the API or CLI.</p>\n\n<p>A policy is an object in AWS that, when associated with an identity or resource, defines their permissions. AWS evaluates these policies when an IAM principal (user or role) makes a request. Permissions in the policies determine whether the request is allowed or denied.</p>\n\n<p>Identity-based policies – Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.</p>\n\n<p>Resource-based policies – Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.</p>\n\n<p>Creating a policy and assigning it manually to users would work but would be hard to scale and manage.</p>\n\n<p><strong>Create an MFA user with read / write access and link 50 IAM with MFA</strong> - MFA adds extra security because it requires users to provide unique authentication from an AWS supported MFA mechanism in addition to their regular sign-in credentials when they access AWS websites or services. MFA cannot help in terms of granting read/write access to only 50 of the IAM users.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Update the S3 bucket policy</p>",
				"<p>Create a group, attach the policy to the group and place the users in the group</p>",
				"<p>Create a policy and assign it manually to the 50 users</p>",
				"<p>Create an MFA user with read / write access and link 50 IAM with MFA</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A team has around 200 users, each of these having an IAM user account in AWS. Currently, they all have read access to an Amazon S3 bucket. The team wants 50 among them to have write and read access to the buckets.\n\nHow can you provide these users access in the least possible time, with minimal changes?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101964,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Using SQS as a middleware will help us sustain the write throughput during write peaks and therefore this option is the best fit for the given use-case.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>DynamoDB DAX</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring developers to manage cache invalidation, data population, or cluster management.</p>\n\n<p>DAX is used for caching reads, not to help with writes. So this option is ruled out.</p>\n\n<p><strong>Kinesis Data Streams</strong> - Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.  KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The throughput of an Amazon Kinesis data stream is designed to scale without limits via increasing the number of shards within a data stream.</p>\n\n<p>Kinesis cannot scale infinitely and we may have the same throughput issues. Therefore this option is not the right fit for the given use-case.</p>\n\n<p><strong>Amazon Simple Notification Service (SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. SNS won't keep our data if it cannot be delivered, so this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
			"answers": [
				"<p>DynamoDB DAX</p>",
				"<p>Kinesis Data Streams</p>",
				"<p>Amazon Simple Notification Service (SNS)</p>",
				"<p>Amazon Simple Queue Service (SQS)</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>A company has noticed several provisioned throughput exceptions on its DynamoDB database due to major spikes in the writes to the database. The development team wants to decouple the application layer from the database layer and dedicate a worker process to writing the data to DynamoDB.</p>\n\n<p>Which middleware do you recommend on using that can scale infinitely and meet these requirements?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A company has noticed several provisioned throughput exceptions on its DynamoDB database due to major spikes in the writes to the database. The development team wants to decouple the application layer from the database layer and dedicate a worker process to writing the data to DynamoDB.\n\nWhich middleware do you recommend on using that can scale infinitely and meet these requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101966,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3</strong></p>\n\n<p>A file gateway provides a simple solution for presenting one or more Amazon S3 buckets and their objects as a mountable NFS or SMB file share to one or more clients on-premises.</p>\n\n<p>The file gateway is deployed as a virtual machine in VMware ESXi or Microsoft Hyper-V environments on-premises, or in an Amazon Elastic Compute Cloud (Amazon EC2) instance in AWS. File gateway can also be deployed in data center and remote office locations on a Storage Gateway hardware appliance. When deployed, file gateway provides a seamless connection between on-premises NFS (v3.0 or v4.1) or SMB (v1 or v2) clients—typically applications—and Amazon S3 buckets hosted in a given AWS Region. The file gateway employs a local read/write cache to provide low-latency access to data for file share clients in the same local area network (LAN) as the file gateway.</p>\n\n<p>A bucket share consists of a file share hosted from a file gateway across a single Amazon S3 bucket. The file gateway virtual machine appliance currently supports up to 10 bucket shares.</p>\n\n<p>File Gateway Architecture:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt6-q31-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\">https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3</strong> - The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. Data on the volumes is stored in Amazon S3 and you can take point in time copies of volumes that are stored in AWS as Amazon EBS snapshots. Volume Gateway is for block storage and not for file storage, so it is not the right option.</p>\n\n<p><strong>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between the company's on-premises storage and the company's S3 bucket</strong></p>\n\n<p><strong>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's S3 bucket</strong></p>\n\n<p>AWS recommends that you should use AWS DataSync to migrate existing data to Amazon S3, and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. Therefore, both these options are incorrect, as they use DataSync for ongoing replication.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html\">https://docs.aws.amazon.com/whitepapers/latest/file-gateway-hybrid-cloud-storage-architectures/file-gateway-architecture.html</a></p>\n",
			"question": "<p>A company uses a legacy on-premises reporting application that operates on gigabytes of .json files and represents years of data. The legacy application cannot handle the growing size of .json files. New .json files are added daily from various data sources to a central on-premises storage location. The company wants to continue to support the legacy application. The company has hired you as a solutions architect to build a solution that maintains two synchronized copies of all the .json files on-premises and in Amazon S3.</p>\n\n<p>Which of the following solutions would you suggest to address the given requirement?</p>\n",
			"answers": [
				"<p>Set up an on-premises file gateway. Configure data sources to write the .json files to the file gateway. Point the legacy analytics application to the file gateway. The file gateway should replicate the .json files to Amazon S3</p>",
				"<p>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between the company's on-premises storage and the company's S3 bucket</p>",
				"<p>Set up an on-premises volume gateway. Configure data sources to write the .json files to the volume gateway. Point the legacy analytics application to the volume gateway. The volume gateway should replicate data to Amazon S3</p>",
				"<p>Set up AWS DataSync on-premises. Configure DataSync to continuously replicate the .json files between on-premises and Amazon Elastic File System (Amazon EFS). Enable replication from Amazon EFS to the company's S3 bucket</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A company uses a legacy on-premises reporting application that operates on gigabytes of .json files and represents years of data. The legacy application cannot handle the growing size of .json files. New .json files are added daily from various data sources to a central on-premises storage location. The company wants to continue to support the legacy application. The company has hired you as a solutions architect to build a solution that maintains two synchronized copies of all the .json files on-premises and in Amazon S3.\n\nWhich of the following solutions would you suggest to address the given requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101968,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create two SQS standard queues: one for pro and one for lite. Set up EC2 instances to prioritize polling for the pro queue over the lite queue</strong></p>\n\n<p>AWS recommends using separate queues to provide prioritization of work. Therefore, for the given use case, you need to create an SQS standard queue for processing pro users' photos and another SQS standard queue for processing lite users' photos. Then you can configure EC2 instances to prioritize polling for the pro queue over the lite queue.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q32-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create two SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</strong></p>\n\n<p><strong>Create two SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</strong></p>\n\n<p>Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long-polling doesn’t return a response until a message arrives in the message queue, or the long poll times out. Since long polling or short polling cannot impact the priority of processing for the two queues, so both these options are incorrect.</p>\n\n<p><strong>Create one SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up EC2 instances to prioritize visibility settings so pro photos are processed first</strong> - To prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds. The maximum is 12 hours. Setting visibility timeout to zero can result in the same pro photo being processed by more than one consumer. This does not help in prioritizing the processing of pro photos over the lite photos.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/images/sqs-visibility-timeout-diagram.png\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Create two SQS standard queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</p>",
				"<p>Create two SQS standard queues: one for pro and one for lite. Set up EC2 instances to prioritize polling for the pro queue over the lite queue</p>",
				"<p>Create two SQS FIFO queues: one for pro and one for lite. Set the lite queue to use short polling and the pro queue to use long polling</p>",
				"<p>Create one SQS standard queue. Set the visibility timeout of the pro photos to zero. Set up EC2 instances to prioritize visibility settings so pro photos are processed first</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to S3 and the job information is sent to Amazon SQS.</p>\n\n<p>As a solutions architect, which of the following solutions would you recommend?</p>\n"
		},
		"correct_response": [
			"b"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A social media application lets users upload photos and perform image editing operations. The application offers two classes of service: pro and lite. The product team wants the photos submitted by pro users to be processed before those submitted by lite users. Photos are uploaded to S3 and the job information is sent to Amazon SQS.\n\nAs a solutions architect, which of the following solutions would you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101970,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>An engineering team wants to orchestrate multiple Amazon ECS task types running on EC2 instances that are part of the ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 MB and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 TB.</p>\n\n<p>As a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?</p>\n",
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p>Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale. It also enables massively parallel access from compute instances, including Amazon EC2, Amazon ECS, and AWS Lambda, to your data.</p>\n\n<p><strong>Use Amazon EFS with Provisioned Throughput mode</strong> - Provisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode. For example, say you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands. Your file system can now get the high levels of throughput your applications require without having to pad your file system.</p>\n\n<p>If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease. Additionally, you can change between Provisioned Throughput mode and the default Bursting Throughput mode as long as it’s been more than 24 hours since the last throughput mode change.</p>\n\n<p><img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt6-q33-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use Amazon EFS with Bursting Throughput mode</strong> - With Bursting Throughput mode, a file system's throughput scales as the amount of data stored in the standard storage class grows. File-based workloads are typically spiky, driving high levels of throughput for short periods of time, and low levels of throughput the rest of the time. To accommodate this, Amazon EFS is designed to burst to high throughput levels for periods of time. By default, AWS recommends that you run your application in the Bursting Throughput mode. But, if you're planning to migrate large amounts of data into your file system, consider switching to Provisioned Throughput mode.</p>\n\n<p>The use-case mentions that the solution should be optimized for high-frequency reading and writing even when the old outputs are archived, therefore Provisioned Throughput mode is a better fit as it guarantees high levels of throughput your applications require without having to pad your file system.</p>\n\n<p><strong>Use an Amazon EBS volume mounted to the ECS cluster instances</strong> - EFS has a higher throughput than EBS. In addition, EBS can be attached to multiple EC2 instances when the underlying EBS type is io1/io2 and the instance is of Nitro type. The use-case does not provide any such details, so this option is ruled out.</p>\n\n<p><strong>Use a DynamoDB table that is accessible by all ECS cluster instances</strong> - DynamoDB is not a fit for this scenario as each task output is 20 MB but the storage limit for each item in a DynamoDB table is 400 KB. You could write custom code to split the task output data into multiple items but it is not an optimal solution compared to using EFS in Provisioned Throughput mode.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use Amazon EFS with Provisioned Throughput mode</p>",
				"<p>Use Amazon EFS with Bursting Throughput mode</p>",
				"<p>Use a DynamoDB table that is accessible by all ECS cluster instances</p>",
				"<p>Use an Amazon EBS volume mounted to the ECS cluster instances</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An engineering team wants to orchestrate multiple Amazon ECS task types running on EC2 instances that are part of the ECS cluster. The output and state data for all tasks need to be stored. The amount of data output by each task is approximately 20 MB and there could be hundreds of tasks running at a time. As old outputs are archived, the storage size is not expected to exceed 1 TB.\n\nAs a solutions architect, which of the following would you recommend as an optimized solution for high-frequency reading and writing?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101972,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in an S3 bucket and read them from within the application code directly</p>",
				"<p>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly</p>",
				"<p>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Add the EC2 instance to the trust relationship policy document so that the instance can assume the role</p>",
				"<p>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Configure an instance profile to assign this IAM role to the EC2 instance</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Configure an instance profile to assign this IAM role to the EC2 instance</strong></p>\n\n<p>A service role is an IAM role that a service assumes to perform actions on your behalf. Service roles provide access only within your account and cannot be used to grant access to services in other accounts. An IAM administrator can create, modify, and delete a service role from within IAM. When you create the service role, you define the <code>trusted entity</code> in the definition.</p>\n\n<p>If you are going to use the role with Amazon EC2 or another AWS service that uses Amazon EC2, you must store the role in an instance profile. An instance profile is a container for a role that can be attached to an Amazon EC2 instance when launched. An instance profile can contain only one role, and that limit cannot be increased. If you create the role using the AWS Management Console, the instance profile is created for you with the same name as the role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in an S3 bucket and read them from within the application code directly</strong></p>\n\n<p><strong>Set up an IAM user with the appropriate permissions to allow access to the DynamoDB table. Store the access credentials in the local storage and read them from within the application code directly</strong></p>\n\n<p>You should never store the IAM access credentials for a user in S3 or local storage or a database. It's a security bad practice. It is always recommended to use IAM roles to configure access to other AWS resources from EC2 instances. Therefore both these options are incorrect.</p>\n\n<p><strong>Set up an IAM service role with the appropriate permissions to allow access to the DynamoDB table. Add the EC2 instance to the trust relationship policy document so that the instance can assume the role</strong> - There is no need for this option because when you create an IAM service role for EC2, the role automatically has EC2 identified as a trusted entity. Therefore this option is not correct.</p>\n\n<p>Configuring a Service Role:\n<img src=\"https://media.datacumulus.com/aws-saa-pt/assets/pt6-q34-i1.jpg\"></p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html</a></p>\n",
			"question": "<p>An application running on an EC2 instance needs to access a DynamoDB table in the same AWS account.</p>\n\n<p>Which of the following solutions should a solutions architect configure for the necessary permissions?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "An application running on an EC2 instance needs to access a DynamoDB table in the same AWS account.\n\nWhich of the following solutions should a solutions architect configure for the necessary permissions?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101974,
		"assessment_type": "multi-select",
		"prompt": {
			"answers": [
				"<p>Set up AWS Database Migration Service (AWS DMS) to ingest the data</p>",
				"<p>Set up AWS Lambda with AWS Step Functions to process the data</p>",
				"<p>Set up Amazon Kinesis Data Streams to ingest the data</p>",
				"<p>Provision EC2 instances in an Auto Scaling group to process the data</p>",
				"<p>Set up AWS Fargate with Amazon ECS to process the data</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Set up Amazon Kinesis Data Streams to ingest the data</strong></p>\n\n<p><strong>Set up AWS Fargate with Amazon ECS to process the data</strong></p>\n\n<p>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p>\n\n<p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications. Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p>\n\n<p>For the given use case, we can use Kinesis Data Streams as the ingestion layer and the containerized ECS application on AWS Fargate as the processing layer. Both these components are serverless and can scale to offer the desired performance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up AWS Database Migration Service (AWS DMS) to ingest the data</strong> - AWS Database Migration Service helps you migrate databases to AWS quickly and securely. DMS cannot be used for real-time data ingestion. Hence, this option is incorrect.</p>\n\n<p><strong>Set up AWS Lambda with AWS Step Functions to process the data</strong> - The maximum timeout value for any AWS Lambda function is 15 minutes. When the specified timeout is reached, AWS Lambda terminates the execution of your Lambda function. Since the use case talks about a job that runs for 30 minutes, Lambda is not an option here.</p>\n\n<p><strong>Provision EC2 instances in an Auto Scaling group to process the data</strong> - The given requirement is for a serverless solution to process the data. Hence, provisioning an EC2 instance is clearly not the right solution.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/\">https://aws.amazon.com/blogs/big-data/building-a-scalable-streaming-data-processor-with-amazon-kinesis-data-streams-on-aws-fargate/</a></p>\n",
			"question": "<p>A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.</p>\n\n<p>Which combination of steps should the solutions architect take? (Select two)</p>\n"
		},
		"correct_response": [
			"c",
			"e"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A company's real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance.\n\nWhich combination of steps should the solutions architect take? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101976,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Amazon S3 Glacier</p>",
				"<p>Amazon S3 One Zone-IA</p>",
				"<p>Amazon S3 Standard-Infrequent Access</p>",
				"<p>Amazon S3 Intelligent Tiering</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon S3 Glacier</strong></p>\n\n<p>Amazon S3 Glacier is a secure, durable, and low-cost storage class for data archiving.\nAmazon S3 Glacier automatically encrypts data at rest using Advanced Encryption Standard (AES) 256-bit symmetric keys and supports secure transfer of your data over Secure Sockets Layer (SSL).</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon S3 Standard-Infrequent Access</strong> - Amazon S3 Standard-Infrequent Access is for data that is accessed less frequently, but requires rapid access when needed. This combination of low cost and high performance makes S3 Standard-IA ideal for long-term storage, backups, and as a data store for disaster recovery files. It does not support encryption by default for both data at rest as well as in-transit.</p>\n\n<p><strong>Amazon S3 Intelligent Tiering</strong> - The S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead. It works by storing objects in two access tiers: one tier that is optimized for frequent access and another lower-cost tier that is optimized for infrequent access. It does not support encryption by default for both data at rest as well as in-transit.</p>\n\n<p><strong>Amazon S3 One Zone-IA</strong> - S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ. It does not support encryption by default for both data at rest as well as in-transit.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/glacier/features/\">https://aws.amazon.com/glacier/features/</a></p>\n",
			"question": "<p>Which of the following S3 storage classes supports encryption by default for both data at rest as well as in-transit?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "Which of the following S3 storage classes supports encryption by default for both data at rest as well as in-transit?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101978,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Use RAID 0 when I/O performance is more important than fault tolerance</p>",
				"<p>Use RAID 1 when I/O performance is more important than fault tolerance</p>",
				"<p>Both RAID 0 and RAID 1 provide equally good I/O performance</p>",
				"<p>Amazon EBS does not support the standard RAID configurations</p>"
			],
			"question": "<p>A data analytics company is running a proprietary database on an EC2 instance using EBS volumes. The database is heavily I/O bound. As a solutions architect, which of the following RAID configurations would you recommend improving the I/O performance?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use RAID 0 when I/O performance is more important than fault tolerance</strong></p>\n\n<p>With Amazon EBS, you can use any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level.</p>\n\n<p>RAID configuration options for I/O performance v/s fault tolerance:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q37-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use RAID 1 when I/O performance is more important than fault tolerance</strong> - This is incorrect because you should use RAID 1 when fault tolerance is more important than I/O performance.</p>\n\n<p><strong>Both RAID 0 and RAID 1 provide equally good I/O performance</strong> -  This is incorrect because RAID 0 provides better I/O performance.</p>\n\n<p><strong>Amazon EBS does not support the standard RAID configurations</strong> - This is incorrect because EBS supports the standard RAID configurations.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A data analytics company is running a proprietary database on an EC2 instance using EBS volumes. The database is heavily I/O bound. As a solutions architect, which of the following RAID configurations would you recommend improving the I/O performance?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101980,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>AWS Snowball</p>",
				"<p>AWS Storage Gateway</p>",
				"<p>AWS Direct Connect</p>",
				"<p>AWS Snowmobile</p>"
			],
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>AWS Snowmobile</strong></p>\n\n<p>AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck. Snowmobile makes it easy to move massive volumes of data to the cloud, including video libraries, image repositories, or even a complete data center migration. Transferring data with Snowmobile is more secure, fast, and cost-effective.  AWS recommends using Snowmobile to migrate large datasets of 10PB or more in a single location. For datasets less than 10PB or distributed in multiple locations, you should use Snowball.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Snowball</strong> - The AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and client's onsite data storage location at faster-than-internet speeds. Snowball provides powerful interfaces that you can use to create jobs, track data, and track the status of your jobs through to completion. AWS recommends snowball only if you want to transfer greater than 10 TB of data between your on-premises data centers and Amazon S3.</p>\n\n<p><strong>AWS Storage Gateway</strong> - AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage.  Used for key hybrid storage solutions that include moving tape backups to the cloud, reducing on-premises storage with cloud-backed file shares, providing low latency access to data in AWS for on-premises applications, as well as various migration, archiving, processing, and disaster recovery use cases. This is not an optimal solution since the studio's data centers are in remote locations where internet speed may not optimal, thereby increasing both cost and time for migrating 20TB of data.</p>\n\n<p><strong>AWS Direct Connect</strong> - AWS Direct Connect is a network service that provides an alternative to using the Internet to connect a customer’s on-premises sites to AWS. Data is transmitted through a private network connection between AWS and a customer’s datacenter or corporate network. Direct Connect connection takes significant cost as well as time to provision. This is not the correct solution since the studio wants the data transfer to be done in the shortest possible time.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/snowmobile/\">https://aws.amazon.com/snowmobile/</a></p>\n",
			"question": "<p>A Hollywood production studio is looking at transferring their existing digital media assets of around 20PB to AWS Cloud in the shortest possible timeframe.</p>\n\n<p>Which of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A Hollywood production studio is looking at transferring their existing digital media assets of around 20PB to AWS Cloud in the shortest possible timeframe.\n\nWhich of the following is an optimal solution for this requirement, given that the studio's data centers are located at a remote location?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101982,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>EFS Infrequent Access</strong></p>\n\n<p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed, elastic, NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS Infrequent Access (EFS IA) is a storage class that provides price/performance that is cost-optimized for files not accessed every day, with storage prices up to 92% lower compared to Amazon EFS Standard. The EFS IA storage class costs only $0.025/GB-month. To get started with EFS IA, simply enable EFS Lifecycle Management for your file system by selecting a lifecycle policy that matches your needs.</p>\n\n<p>How EFS Infrequent Access Works:\n<img src=\"https://d1.awsstatic.com/EFS/product-page-diagram-Amazon-EFS-Infrequent-Access-How-It-Works.83f88e30a40c27f38abae1ff157712a336dd1320.png\">\nvia - <a href=\"https://aws.amazon.com/efs/features/infrequent-access/\">https://aws.amazon.com/efs/features/infrequent-access/</a></p>\n\n<p>Incorrect options</p>\n\n<p><strong>EFS Standard</strong> - EFS Infrequent Access is more cost-effective than EFS Standard for the given use-case, therefore this option is incorrect.</p>\n\n<p><strong>S3 Standard</strong></p>\n\n<p><strong>S3 Standard-IA</strong></p>\n\n<p>Both these options are object-based storage, whereas the given use-case requires a POSIX compliant file storage solution. Hence these two options are incorrect.</p>\n\n<p>Reference:\nhttps://aws.amazon.com/efs/features/infrequent-access/</p>\n",
			"answers": [
				"<p>EFS Infrequent Access</p>",
				"<p>EFS Standard</p>",
				"<p>S3 Standard</p>",
				"<p>S3 Standard-IA</p>"
			],
			"question": "<p>A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "A big data analytics company is looking to archive the on-premises data into a POSIX compliant file storage system on AWS Cloud. The archived data would be accessed for just about a week in a year.\n\nAs a solutions architect, which of the following AWS services would you recommend as the MOST cost-optimal solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101984,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A Big Data consulting company runs large distributed and replicated workloads on the on-premises data center. The company now wants to move these workloads to Amazon EC2 instances by using the placement groups feature and it wants to minimize correlated hardware failures.</p>\n\n<p>Which of the following represents the correct placement group configuration for the given requirement?</p>\n",
			"answers": [
				"<p>Partition placement groups</p>",
				"<p>Cluster placement groups</p>",
				"<p>Spread placement groups</p>",
				"<p>Multi-AZ placement groups</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Partition placement groups</strong> - Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application.</p>\n\n<p>The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions—Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.</p>\n\n<p>Partition placement groups can be used to deploy large distributed and replicated workloads, such as HDFS, HBase, and Cassandra, across distinct racks. When you launch instances into a partition placement group, Amazon EC2 tries to distribute the instances evenly across the number of partitions that you specify. You can also launch instances into a specific partition to have more control over where the instances are placed.</p>\n\n<p>A partition placement group can have partitions in multiple Availability Zones in the same Region. A partition placement group can have a maximum of seven partitions per Availability Zone. The number of instances that can be launched into a partition placement group is limited only by the limits of your account.</p>\n\n<p>Partition placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i1.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Cluster placement groups</strong> - A cluster placement group is a logical grouping of instances within a single Availability Zone. A cluster placement group can span peered VPCs in the same Region. Instances in the same cluster placement group enjoy a higher per-flow throughput limit for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. As the instances are packed close together inside an Availability Zone, this option is not correct for the given use case.</p>\n\n<p>Cluster placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i2.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a></p>\n\n<p><strong>Spread placement groups</strong> - A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time. As the use-case talks about running large distributed and replicated workloads, so it needs more instances, therefore this option is not the right fit for the given use-case.</p>\n\n<p>A spread placement group can span multiple Availability Zones in the same Region. You can have a maximum of seven running instances per Availability Zone per group.</p>\n\n<p>The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks.</p>\n\n<p>Spread placement groups:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q40-i3.jpg\">\nvia - <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-partition</a></p>\n\n<p><strong>Multi-AZ placement groups</strong> - This is a made-up option, given as a distractor. You should note that the Partition and Spread placement groups can span across multiple Availability Zones in the same Region.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A Big Data consulting company runs large distributed and replicated workloads on the on-premises data center. The company now wants to move these workloads to Amazon EC2 instances by using the placement groups feature and it wants to minimize correlated hardware failures.\n\nWhich of the following represents the correct placement group configuration for the given requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101986,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>SSE-S3</p>",
				"<p>SSE-C</p>",
				"<p>Client Side Encryption</p>",
				"<p>SSE-KMS</p>"
			],
			"question": "<p>A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.</p>\n\n<p>Which of the following options should you use?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>SSE-S3</strong></p>\n\n<p>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.</p>\n\n<p><strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.</p>\n\n<p><strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage encryption keys yourself.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A financial services company stores confidential data on an Amazon Simple Storage Service (S3) bucket. The compliance guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.\n\nWhich of the following options should you use?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101988,
		"assessment_type": "multi-select",
		"prompt": {
			"answers": [
				"<p>You can't detach an instance store volume from one instance and attach it to a different instance</p>",
				"<p>Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation</p>",
				"<p>You can specify instance store volumes for an instance when you launch or restart it</p>",
				"<p>An instance store is a network storage type</p>",
				"<p>If you create an AMI from an instance, the data on its instance store volumes isn't preserved</p>"
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>You can't detach an instance store volume from one instance and attach it to a different instance</strong> - You can specify instance store volumes for an instance only when you launch it. You can't detach an instance store volume from one instance and attach it to a different instance. The data in an instance store persists only during the lifetime of its associated instance. If an instance reboots (intentionally or unintentionally), data in the instance store persists.</p>\n\n<p><strong>If you create an AMI from an instance, the data on its instance store volumes isn't preserved</strong> - If you create an AMI from an instance, the data on its instance store volumes isn't preserved and isn't present on the instance store volumes of the instances that you launch from the AMI.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Instance store is reset when you stop or terminate an instance. Instance store data is preserved during hibernation</strong> - When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset. Therefore, this option is incorrect.</p>\n\n<p><strong>You can specify instance store volumes for an instance when you launch or restart it</strong> - You can specify instance store volumes for an instance only when you launch it.</p>\n\n<p><strong>An instance store is a network storage type</strong> - An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p>\n",
			"question": "<p>The engineering team at a startup is evaluating the most optimal block storage volume type for the EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements.</p>\n\n<p>Which of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a",
			"e"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The engineering team at a startup is evaluating the most optimal block storage volume type for the EC2 instances hosting its flagship application. The storage volume should support very low latency but it does not need to persist the data when the instance terminates. As a solutions architect, you have proposed using Instance Store volumes to meet these requirements.\n\nWhich of the following would you identify as the key characteristics of the Instance Store volumes? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101990,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A company is deploying a web application and it wants to ensure that only the web tier of the application is publicly accessible. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several EC2 instances in an Auto Scaling group. The team also wants TLS termination to be offloaded from the EC2 instances.</p>\n\n<p>Which solution should a solutions architect implement to address these requirements?</p>\n",
			"answers": [
				"<p>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</p>",
				"<p>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</p>",
				"<p>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</p>",
				"<p>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</strong></p>\n\n<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. This increases the availability of your application. You add one or more listeners to your load balancer.</p>\n\n<p>With Network Load Balancer (NLB), you can offload the decryption/encryption of TLS traffic from your application servers to the Network Load Balancer, which helps you optimize the performance of your backend application servers while keeping your workloads secure. Additionally, Network Load Balancers preserve the source IP of the clients to the back-end applications, while terminating TLS on the load balancer.</p>\n\n<p>An Auto Scaling Group (ASG) contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies. Both maintaining the number of instances in an Auto Scaling group and automatic scaling are the core functionality of the Amazon EC2 Auto Scaling service.</p>\n\n<p>The NLB has to be accessible over the internet and hence has to be in a public subnet and will act as a single point-of-contact for all incoming traffic. NLB will forward the incoming traffic to the EC2 instances managed by the ASG in the private subnet.</p>\n\n<p>Exam Alert:</p>\n\n<p>You should note that the Application Load Balancer also supports TLS offloading. The Classic Load Balancer supports SSL offloading.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Set up a Network Load Balancer in the public subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</strong> - ASG with its target EC2 instances should be in the private subnet to avoid access to EC2 instances over the public internet. Hence, this option is incorrect.</p>\n\n<p><strong>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the public subnet and associate it with the Network Load Balancer</strong></p>\n\n<p><strong>Set up a Network Load Balancer in the private subnet. Create an Auto Scaling group in the private subnet and associate it with the Network Load Balancer</strong></p>\n\n<p>NLB should be in the public subnet as it represents the internet-facing component of the web tier. Therefore, both these options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/\">https://aws.amazon.com/blogs/aws/new-tls-termination-for-network-load-balancers/</a></p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A company is deploying a web application and it wants to ensure that only the web tier of the application is publicly accessible. To accomplish this, the engineering team has designed the VPC with a public subnet and a private subnet. The application will be hosted on several EC2 instances in an Auto Scaling group. The team also wants TLS termination to be offloaded from the EC2 instances.\n\nWhich solution should a solutions architect implement to address these requirements?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101992,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory.</p>\n\n<p>Which AWS Directory Service is the best fit for this requirement?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"answers": [
				"<p>AD Connector</p>",
				"<p>Simple AD</p>",
				"<p>AWS Transit Gateway</p>",
				"<p>AWS Managed Microsoft AD</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Managed Microsoft AD</strong> - AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, is powered by Windows Server 2012 R2. When you select and launch this directory type, it is created as a highly available pair of domain controllers connected to your virtual private cloud (VPC).</p>\n\n<p>With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, including Microsoft SharePoint and custom .NET and SQL Server-based applications. You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO).</p>\n\n<p>AWS Managed Microsoft AD is your best choice if you need actual Active Directory features to support AWS applications or Windows workloads, including Amazon Relational Database Service for Microsoft SQL Server. It's also best if you want a standalone AD in the AWS Cloud that supports Office 365 or you need an LDAP directory to support your Linux applications.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AD Connector</strong> - AD Connector is a directory gateway with which you can redirect directory requests to your on-premises Microsoft Active Directory without caching any information in the cloud. AD Connector is your best choice when you want to use your existing on-premises directory with compatible AWS services.</p>\n\n<p><strong>Simple AD</strong> - Simple AD is a standalone directory in the cloud, where you create and manage user identities and manage access to applications. Simple AD provides a subset of the features offered by AWS Managed Microsoft AD. However, note that Simple AD does not support features such as multi-factor authentication (MFA), trust relationships with other domains, Active Directory Administrative Center, PowerShell support, Active Directory recycle bin, group managed service accounts, and schema extensions for POSIX and Microsoft applications.</p>\n\n<p><strong>AWS Transit Gateway</strong> - AWS Transit Gateway connects VPCs and on-premises networks through a central hub. Transit Gateway is not an Active Directory service.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_simple_ad.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html</a></p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A company needs an Active Directory service to run directory-aware workloads in the AWS Cloud and it should also support configuring a trust relationship with any existing on-premises Microsoft Active Directory.\n\nWhich AWS Directory Service is the best fit for this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101994,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Create a corporate Amazon S3 bucket and move all media files</p>",
				"<p>Set up EFS and move all media files</p>",
				"<p>Set up Amazon FSx for Windows File Server and move all the media files</p>",
				"<p>Provision EC2 with Windows OS, attach multiple EBS volumes, and move all media files</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Set up Amazon FSx for Windows File Server and move all the media files</strong></p>\n\n<p>Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry-standard Server Message Block (SMB) protocol. It is built on Windows Server, delivering a wide range of administrative features such as user quotas, end-user file restore, and Microsoft Active Directory (AD) integration. To support a wide spectrum of workloads, Amazon FSx provides high levels of throughput and IOPS and consistent sub-millisecond latencies.</p>\n\n<p>Amazon FSx file storage is accessible from Windows, Linux, and macOS compute instances and devices running on AWS or on-premises. Thousands of compute instances and devices can access a file system concurrently. Amazon FSx for Windows File Server supports Microsoft Active Directory (AD) integration so the same user permissions and access credentials can be used to access the files on FSx Windows File Server.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a corporate Amazon S3 bucket and move all media files</strong> - S3 is object-based storage and it does not support file storage. Hence S3 is not the correct option.</p>\n\n<p><strong>Set up EFS and move all media files</strong> - Amazon EFS provides scalable file storage for use with Amazon EC2. You can use an EFS file system as a common data source for workloads and applications running on multiple instances. EFS is not compatible with the Windows platform, so this option is ruled out.</p>\n\n<p><strong>Provision EC2 with Windows OS, attach multiple EBS volumes, and move all media files</strong> - Multi-attach EBS volumes are supported only for Nitro EC2 instances which are Linux-based. So this option is ruled out.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/fsx/windows/\">https://aws.amazon.com/fsx/windows/</a></p>\n",
			"relatedLectureIds": "",
			"question": "<p>A company has media files that need to be shared internally. Users are first authenticated using Active Directory and then they access files on a Microsoft Windows platform. The engineering manager wants to keep the same user permissions but wants the company to migrate the storage layer to AWS Cloud as the company is reaching its storage capacity limit on the on-premises infrastructure.</p>\n\n<p>What should a solutions architect recommend to meet this requirement?</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A company has media files that need to be shared internally. Users are first authenticated using Active Directory and then they access files on a Microsoft Windows platform. The engineering manager wants to keep the same user permissions but wants the company to migrate the storage layer to AWS Cloud as the company is reaching its storage capacity limit on the on-premises infrastructure.\n\nWhat should a solutions architect recommend to meet this requirement?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101996,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>The ASG will terminate the EC2 Instance</p>",
				"<p>The ASG will detach the EC2 instance from the group, and leave it running</p>",
				"<p>The ASG will keep the instance running and re-start the application</p>",
				"<p>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>The ASG will terminate the EC2 Instance</strong></p>\n\n<p>To maintain the same number of instances, Amazon EC2 Auto Scaling performs a periodic health check on running instances within an Auto Scaling group. When it finds that an instance is unhealthy, it terminates that instance and launches a new one. Amazon EC2 Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The ASG will detach the EC2 instance from the group, and leave it running</strong> - The goal of the auto-scaling group is to get rid of the bad instance and replace it.</p>\n\n<p><strong>The ASG will keep the instance running and re-start the application</strong> - The ASG does not have control of your application.</p>\n\n<p><strong>The ASG will format the root EBS drive on the EC2 instance and run the User Data again</strong> - This will not happen. The ASG cannot format the EBS drive. The User Data only runs once at the first boot of the instance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance\">https://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-terminate-instance</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-maintain-instance-levels.html#replace-unhealthy-instance</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>An Auto Scaling group (ASG) has been created to work with an Application Load Balancer (ALB). The scaling group is configured with a minimum size value of 10, a maximum value of 30, and the desired capacity value of 20. One of the 20 EC2 instances has been reported as unhealthy.</p>\n\n<p>Which of the following actions will take place?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "An Auto Scaling group (ASG) has been created to work with an Application Load Balancer (ALB). The scaling group is configured with a minimum size value of 10, a maximum value of 30, and the desired capacity value of 20. One of the 20 EC2 instances has been reported as unhealthy.\n\nWhich of the following actions will take place?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33101998,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Amazon ElastiCache for Redis</p>",
				"<p>Amazon DynamoDB Accelerator (DAX)</p>",
				"<p>AWS Global Accelerator</p>",
				"<p>Amazon ElastiCache for Memcached</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon ElastiCache for Memcached</strong> - Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store and cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.</p>\n\n<p>Memcached is an open-source, distributed, in-memory key-value store that can retrieve data in milliseconds. Caching site information with Memcached can help you improve the performance and scalability of your site while controlling cost.</p>\n\n<p>Choose Memcached if the following apply to you:</p>\n\n<p>You need the simplest model possible.</p>\n\n<p>You need to run large nodes with multiple cores or threads (support for multi-threading).</p>\n\n<p>You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases.</p>\n\n<p>You need to cache objects.</p>\n\n<p><img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q47-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon ElastiCache for Redis</strong> - Redis, which stands for Remote Dictionary Server, is a fast, open-source, in-memory key-value data store for use as a database, cache, message broker, and queue. Redis now delivers sub-millisecond response times enabling millions of requests per second for real-time applications in Gaming, Ad-Tech, Financial Services, Healthcare, and IoT. Redis is a popular choice for caching, session management, gaming, leaderboards, real-time analytics, geospatial, ride-hailing, chat/messaging, media streaming, and pub/sub apps.</p>\n\n<p>Redis does not support multi-threading, so this option is not the right fit for the given use case.</p>\n\n<p><strong>Amazon DynamoDB Accelerator (DAX)</strong> - Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB. DAX does not support relational databases.</p>\n\n<p><strong>AWS Global Accelerator</strong> - AWS Global Accelerator is a networking service that helps you improve the availability and performance of the applications that you offer to your global users. This option has been added as a distractor, it has nothing to do with database caching.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/caching/aws-caching/\">https://aws.amazon.com/caching/aws-caching/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached/\">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>\n",
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading.</p>\n\n<p>As a solutions architect, which of the following services would you recommend for the given use case?</p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "An e-commerce application uses a relational database that runs several queries that perform joins on multiple tables. The development team has found that these queries are slow and expensive, therefore these are a good candidate for caching. The application needs to use a caching service that supports multi-threading.\n\nAs a solutions architect, which of the following services would you recommend for the given use case?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102000,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>AWS Data Pipeline</p>",
				"<p>Amazon EMR</p>",
				"<p>AWS Glue</p>",
				"<p>AWS Database Migration Service (DMS)</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>AWS Glue</strong> - AWS Glue provides a managed ETL service that runs on a serverless Apache Spark environment. This allows you to focus on your ETL job and not worry about configuring and managing the underlying compute resources. AWS Glue takes a data-first approach and allows you to focus on the data properties and data manipulation to transform the data to a form where you can derive business insights. It provides an integrated data catalog that makes metadata available for ETL as well as querying via Amazon Athena and Amazon Redshift Spectrum.</p>\n\n<p>Create a unified catalog to find data across multiple data stores using Glue:\n<img src=\"https://assets-pt.media.datacumulus.com/aws-saa-pt/assets/pt6-q48-i1.jpg\">\nvia - <a href=\"https://aws.amazon.com/glue/\">https://aws.amazon.com/glue/</a></p>\n\n<p>AWS Glue automates much of the effort required for data integration. AWS Glue crawls your data sources, identifies data formats, and suggests schemas to store your data. It automatically generates the code to run your data transformations and loading processes. You can use AWS Glue to easily run and manage thousands of ETL jobs or to combine and replicate data across multiple data stores using SQL.</p>\n\n<p>AWS Glue runs in a serverless environment. There is no infrastructure to manage, and AWS Glue provisions, configures, and scales the resources required to run your data integration jobs. You pay only for the resources your jobs use while running.</p>\n\n<p>AWS Glue is the right fit since the company is looking at a managed ETL service without having the overhead of configuring, maintaining, or managing any servers.</p>\n\n<p><img src=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/images/pattern-img/1f854a3e-44d4-4d70-9cd2-d61f852e3231/images/f26e2ee3-74be-49f1-8290-cd81e4ef9465.png\">\nvia - <a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>\n\n<p>Incorrect options:</p>\n\n<p><strong>AWS Data Pipeline</strong> - AWS Data Pipeline provides a managed orchestration service that gives you greater flexibility in terms of the execution environment, access and control over the compute resources that run your code, as well as the code itself that does data processing. AWS Data Pipeline launches compute resources in your account allowing you direct access to the Amazon EC2 instances or Amazon EMR clusters. As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.</p>\n\n<p><strong>Amazon EMR</strong> - EMR is a web service to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). As this option provides access to the underlying EC2 instances so it's not a serverless solution. Therefore this option is incorrect for the given use case.</p>\n\n<p><strong>AWS Database Migration Service (DMS)</strong> - AWS Database Migration Service (DMS) helps you migrate databases to AWS easily and securely. For use cases that require a database migration from on-premises to AWS or database replication between on-premises sources and sources on AWS, AWS recommends you use AWS DMS. Once your data is in AWS, you can use AWS Glue to move, combine, replicate, and transform data from your data source into another database or data warehouse, such as Amazon Redshift. As the use-case talks about data migration and transformation between AWS services, so AWS Glue is a better fit than DMS.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/glue/faqs/\">https://aws.amazon.com/glue/faqs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/load-data-from-amazon-s3-to-amazon-redshift-using-aws-glue.html</a></p>\n",
			"question": "<p>A Big Data company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and transforms data from its S3 based data lake to a Redshift cluster. The team wants to manage this daily job in a serverless environment.</p>\n\n<p>Which AWS service is the best fit to manage this process without the need to configure or manage the underlying compute resources?</p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A Big Data company wants to optimize its daily Extract-Transform-Load (ETL) process that migrates and transforms data from its S3 based data lake to a Redshift cluster. The team wants to manage this daily job in a serverless environment.\n\nWhich AWS service is the best fit to manage this process without the need to configure or manage the underlying compute resources?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102002,
		"assessment_type": "multi-select",
		"prompt": {
			"relatedLectureIds": "",
			"question": "<p>The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on RDS Multi-AZ capabilities.</p>\n\n<p>Which of the following would you identify as correct for RDS Multi-AZ? (Select two)</p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby</strong></p>\n\n<p>Running a DB instance as a Multi-AZ deployment can further reduce the impact of a maintenance event because Amazon RDS applies operating system updates by following these steps:</p>\n\n<p>Perform maintenance on the standby.</p>\n\n<p>Promote the standby to primary.</p>\n\n<p>Perform maintenance on the old primary, which becomes the new standby.</p>\n\n<p>When you modify the database engine for your DB instance in a Multi-AZ deployment, then Amazon RDS upgrades both the primary and secondary DB instances at the same time. In this case, the database engine for the entire Multi-AZ deployment is shut down during the upgrade.</p>\n\n<p><strong>Amazon RDS automatically initiates a failover to the standby, in case the primary database fails for any reason</strong> - You also benefit from enhanced database availability when running your DB instance as a Multi-AZ deployment. If an Availability Zone failure or DB instance failure occurs, your availability impact is limited to the time automatic failover takes to complete.</p>\n\n<p>Another implied benefit of running your DB instance as a Multi-AZ deployment is that DB instance failover is automatic and requires no administration. In an Amazon RDS context, this means you are not required to monitor DB instance events and initiate manual DB instance recovery in the event of an Availability Zone failure or DB instance failure.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend to planned maintenance. For example, with automated backups, I/O activity is no longer suspended on your primary during your preferred backup window, since backups are taken from the standby.</p>\n\n<p><strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments are designed to provide enhanced database availability and durability, rather than read scaling benefits. As such, the feature uses synchronous replication between primary and standby. AWS implementation makes sure the primary and the standby are constantly in sync, but precludes using the standby for read or write operations.</p>\n\n<p><strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous “standby” replica in a different Availability Zone. Updates to your DB Instance are synchronously replicated across the Availability Zone to the standby in order to keep both in sync and protect your latest database updates against DB instance failure.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/rds/faqs/\">https://aws.amazon.com/rds/faqs/</a></p>\n",
			"answers": [
				"<p>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</p>",
				"<p>RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes the new standby</p>",
				"<p>Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason</p>",
				"<p>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</p>",
				"<p>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</p>"
			]
		},
		"correct_response": [
			"b",
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The engineering team at a retail company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application. The team has hired you as an AWS Certified Solutions Architect Associate to advise on RDS Multi-AZ capabilities.\n\nWhich of the following would you identify as correct for RDS Multi-AZ? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102004,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A development team has noticed that one of the EC2 instances has been incorrectly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.</p>\n\n<p>As a Solution's Architect, can you suggest a way to disable this flag while the instance is still running?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p>When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the DeleteOnTermination attribute is set to True for the root volume and is set to False for all other volume types.</p>\n\n<p><strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set <code>DeleteOnTermination</code> to False using the command line.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can set the <code>DeleteOnTermination</code> attribute to False when you launch a new instance. It is not possible to update this attribute of a running instance from the AWS console.</p>\n\n<p><strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console, command-line interface, or API. To prevent your instance from being accidentally terminated using Amazon EC2, you can enable termination protection for the instance. The <code>DisableApiTermination</code> attribute controls whether the instance can be terminated using the console, CLI, or API. This option cannot be used to control the delete status for the EBS volume when the instance terminates.</p>\n\n<p><strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given only as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\">https://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Set the <code>DeleteOnTermination</code> attribute to False using the command line</p>",
				"<p>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</p>",
				"<p>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</p>",
				"<p>Set the <code>DisableApiTermination</code> attribute of the instance using the API</p>"
			]
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A development team has noticed that one of the EC2 instances has been incorrectly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.\n\nAs a Solution's Architect, can you suggest a way to disable this flag while the instance is still running?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102006,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>SSE-S3</p>",
				"<p>SSE-KMS</p>",
				"<p>SSE-C</p>",
				"<p>Client Side Encryption</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"question": "<p>A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using AWS S3 and would like you, the solution architect, to advise them on the encryption scheme to adopt.</p>\n\n<p>What do you recommend?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Client Side Encryption</strong></p>\n\n<p>Client-side encryption is the act of encrypting data before sending it to Amazon S3. To enable client-side encryption, you have the following options:</p>\n\n<ol>\n<li><p>Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).</p></li>\n<li><p>Use a master key you store within your application.</p></li>\n</ol>\n\n<p>Because the company has its proprietary encryption algorithm, you have to leverage client-side encryption.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>SSE-KMS</strong> - AWS Key Management Service (AWS KMS) is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom.</p>\n\n<p><strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key.</p>\n\n<p><strong>SSE-C</strong> - With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption when you access your objects.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html</a></p>\n"
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A company helps its customers legally sign highly confidential contracts. To meet the strong industry requirements, the company must ensure that the signed contracts are encrypted using the company's proprietary algorithm. The company is now migrating to AWS Cloud using AWS S3 and would like you, the solution architect, to advise them on the encryption scheme to adopt.\n\nWhat do you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102008,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p>AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.</p>\n\n<p>With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services or call it directly from any web or mobile app. AWS Lambda functions can be configured to run up to 15 minutes per execution. You can set the timeout to any value between 1 second and 15 minutes.</p>\n\n<p><strong>The AWS Lambda function is timing out</strong> - AWS Lambda functions time out after 15 minutes, and are not usually meant for long-running jobs.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>The AWS Lambda function is running out of memory</strong> - Memory errors will not result in the abrupt termination of the function with no error message.</p>\n\n<p><strong>The AWS Lambda function chosen runtime is wrong</strong> - Lambda function execution will fail if there is an issue with runtime. So, this is not the issue in the current case.</p>\n\n<p><strong>The AWS Lambda function is missing IAM permissions</strong> - Without enough permissions, Lambda would not have been able to start its execution at all. So, permissions are not an issue here.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/lambda/faqs/\">https://aws.amazon.com/lambda/faqs/</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>The AWS Lambda function is running out of memory</p>",
				"<p>The AWS Lambda function chosen runtime is wrong</p>",
				"<p>The AWS Lambda function is timing out</p>",
				"<p>The AWS Lambda function is missing IAM permissions</p>"
			],
			"question": "<p>You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical trends, a similar job runs for 30 minutes on average. The Lambda function pulls data from Amazon S3, processes it, and then writes the results back to S3. When you deployed your AWS Lambda function, you noticed an issue where the Lambda function abruptly failed after 15 minutes of execution.</p>\n\n<p>As a solutions architect, which of the following would you identify as the root cause of the issue?</p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "You are using AWS Lambda to implement a batch job for a big data analytics workflow. Based on historical trends, a similar job runs for 30 minutes on average. The Lambda function pulls data from Amazon S3, processes it, and then writes the results back to S3. When you deployed your AWS Lambda function, you noticed an issue where the Lambda function abruptly failed after 15 minutes of execution.\n\nAs a solutions architect, which of the following would you identify as the root cause of the issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102010,
		"assessment_type": "multi-select",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"question": "<p>Your company is building a video streaming service accessible to users who have paid an ongoing subscription. The subscription data is stored in DynamoDB. You would like to expose the users to a serverless architecture allowing them to request the video files that sit on Amazon S3 and are distributed by CloudFront and protected by an origin access identity (OAI).</p>\n\n<p>Which of the following options can be combined to build a solution? (Select two)</p>\n",
			"answers": [
				"<p>Use DynamoDB triggers to generate the URL</p>",
				"<p>Use AWS Lambda to generate the URL</p>",
				"<p>Generate an S3 pre-signed URL</p>",
				"<p>Generate a CloudFront signed URL</p>",
				"<p>Use API Gateway to generate the URL</p>"
			],
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use AWS Lambda to generate the URL</strong></p>\n\n<p><strong>Generate a CloudFront signed URL</strong></p>\n\n<p>To restrict access to content that you serve from Amazon S3 buckets, follow these steps:</p>\n\n<p>Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.\nConfigure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the S3 bucket to access a file there.</p>\n\n<p>After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.\nIn general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your content remains protected.</p>\n\n<p>To generate this URL we must code, and Lambda is the perfect tool for running that code on the fly.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Use DynamoDB triggers to generate the URL</strong></p>\n\n<p><strong>Use API Gateway to generate the URL</strong></p>\n\n<p>DynamoDB triggers or API Gateway as services cannot be used to generate these pre-signed URLs.</p>\n\n<p><strong>Generate an S3 pre-signed URL</strong> - Generating S3 pre-signed URLs would bypass CloudFront, therefore we should use CloudFront signed URLs.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b",
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "Your company is building a video streaming service accessible to users who have paid an ongoing subscription. The subscription data is stored in DynamoDB. You would like to expose the users to a serverless architecture allowing them to request the video files that sit on Amazon S3 and are distributed by CloudFront and protected by an origin access identity (OAI).\n\nWhich of the following options can be combined to build a solution? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102012,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"question": "<p>As a Solutions Architect, you would like to completely secure the communications between your CloudFront distribution and your S3 bucket which contains the static files for your website. Users should only be able to access the S3 bucket through CloudFront and not directly.</p>\n\n<p>What do you recommend?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create an origin access identity (OAI) and update the S3 Bucket Policy</strong></p>\n\n<p>To restrict access to content that you serve from Amazon S3 buckets, you need to follow the following steps:</p>\n\n<ol>\n<li>Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.</li>\n<li>Configure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the S3 bucket to access a file there.</li>\n</ol>\n\n<p>After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.</p>\n\n<p>In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you restrict access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct Amazon S3 URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your content remains protected.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Update the S3 bucket security groups to only allow traffic from the CloudFront security group</strong> - S3 buckets don't have security groups, hence this is an incorrect option.</p>\n\n<p><strong>Make the S3 bucket public</strong> - If the S3 bucket is made public, it can be accessed by anyone directly. This is not the requirement.</p>\n\n<p><strong>Create a bucket policy to only authorize the IAM role attached to the CloudFront distribution</strong> -  You cannot attach IAM roles to the CloudFront distribution. Here you need to use an OAI.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Create a bucket policy to only authorize the IAM role attached to the CloudFront distribution</p>",
				"<p>Update the S3 bucket security groups to only allow traffic from the CloudFront security group</p>",
				"<p>Make the S3 bucket public</p>",
				"<p>Create an origin access identity (OAI) and update the S3 Bucket Policy</p>"
			]
		},
		"correct_response": [
			"d"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "As a Solutions Architect, you would like to completely secure the communications between your CloudFront distribution and your S3 bucket which contains the static files for your website. Users should only be able to access the S3 bucket through CloudFront and not directly.\n\nWhat do you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102014,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Stop the CloudWatch agent to improve performance</p>",
				"<p>Convert the gp2 volume to an io1</p>",
				"<p>Increase the IOPS on the gp2 volume</p>",
				"<p>Convert the EC2 instance to an i3.4xlarge</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct option:</p>\n\n<p>Amazon EBS provides the following volume types, which differ in performance characteristics and price so that you can tailor your storage performance and cost to the needs of your applications. The volumes types fall into two categories:</p>\n\n<p>SSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS</p>\n\n<p>HDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS</p>\n\n<p><strong>Convert the gp2 volume to an io1</strong> - Provisioned IOPS SSD (io1) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Unlike gp2, which uses a bucket and credit model to calculate performance, an io1 volume allows you to specify a consistent IOPS rate when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time.</p>\n\n<p>The only solution is to convert the volume into an io1 volume. This will allow us to keep the same disk size while independently increasing the IOPS for that volume.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Stop the CloudWatch agent to improve performance</strong> - The CloudWatch agent does not have any impact on the performance of the instance.</p>\n\n<p><strong>Increase the IOPS on the gp2 volume</strong> - General Purpose SSD (gp2) volumes offer cost-effective storage that is ideal for a broad range of workloads. These volumes deliver single-digit millisecond latencies and the ability to burst to 3,000 IOPS for extended periods. Between a minimum of 100 IOPS (at 33.33 GiB and below) and a maximum of 16,000 IOPS (at 5,334 GiB and above), baseline performance scales linearly at 3 IOPS per GiB of volume size. AWS designs gp2 volumes to deliver their provisioned performance 99% of the time. A gp2 volume can range in size from 1 GiB to 16 TiB.</p>\n\n<p>IOPS cannot be directly increased on a gp2 volume without increasing its size, which is not possible due to the question's constraints.</p>\n\n<p><strong>Convert the EC2 instance to an i3.4xlarge</strong> -  Converting the EC2 instance to i3.4xlarge won't improve the EBS drive's performance.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_gp2</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops</a></p>\n",
			"question": "<p>As a Solutions Architect, you have set up a database on a single EC2 instance that has an EBS volume of type gp2. You currently have 300GB of space on the gp2 device. The EC2 instance is of type m5.large. The database performance has recently been poor and upon looking at CloudWatch, you realize the IOPS on the EBS volume is maxing out. The disk size of the database must not change because of a licensing issue.</p>\n\n<p>How do you troubleshoot this issue?</p>\n",
			"relatedLectureIds": ""
		},
		"correct_response": [
			"b"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "As a Solutions Architect, you have set up a database on a single EC2 instance that has an EBS volume of type gp2. You currently have 300GB of space on the gp2 device. The EC2 instance is of type m5.large. The database performance has recently been poor and upon looking at CloudWatch, you realize the IOPS on the EBS volume is maxing out. The disk size of the database must not change because of a licensing issue.\n\nHow do you troubleshoot this issue?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102018,
		"assessment_type": "multiple-choice",
		"prompt": {
			"answers": [
				"<p>Create a public Network Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</p>",
				"<p>Create a public Application Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</p>",
				"<p>Create a VPC Endpoint for a fleet of EC2 instances that are bastion hosts managed by an ASG</p>",
				"<p>Create an Elastic IP and assign it to all EC2 instances that are bastion hosts managed by an ASG</p>"
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create a public Network Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</strong></p>\n\n<p>Network Load Balancer is best suited for use-cases involving low latency and high throughput workloads that involve scaling to millions of requests per second. Network Load Balancer operates at the connection level (Layer 4), routing connections to targets - Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.</p>\n\n<p>Including bastion hosts in your VPC environment enables you to securely connect to your Linux instances without exposing your environment to the Internet. After you set up your bastion hosts, you can access the other instances in your VPC through Secure Shell (SSH) connections on Linux. Bastion hosts are also configured with security groups to provide fine-grained ingress control.</p>\n\n<p>You need to remember that Bastion Hosts are using the SSH protocol, which is a TCP based protocol on port 22. They must be publicly accessible.</p>\n\n<p>Here, the correct answer is to use a Network Load Balancer, which supports TCP traffic, and will automatically allow you to connect to the EC2 instance in the backend.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create an Elastic IP and assign it to all EC2 instances that are bastion hosts managed by an ASG</strong> - An Elastic IP can only be attached to one EC2 instance at a time, so it won't provide you a highly available setup on its own. Note that if we had two Elastic IPs and two Bastion Hosts, this would work.</p>\n\n<p><strong>Create a VPC Endpoint for a fleet of EC2 instances that are bastion hosts managed by an ASG</strong> - A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>VPC Endpoints are not used on top of EC2 instances. They're a way to access AWS services privately within your VPC (without using the public internet). This is a distractor.</p>\n\n<p><strong>Create a public Application Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG</strong> - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>An ALB only supports HTTP traffic, which is layer 7, while the SSH protocol is based on TCP and is layer 4. So, the Application Load Balancer doesn't work.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\">https://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>The systems administrator at a company wants to set up a highly available architecture for a bastion host solution.</p>\n\n<p>As a solutions architect, which of the following options would you recommend as the solution?</p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "The systems administrator at a company wants to set up a highly available architecture for a bastion host solution.\n\nAs a solutions architect, which of the following options would you recommend as the solution?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102020,
		"assessment_type": "multi-select",
		"prompt": {
			"question": "<p>You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an AZ.</p>\n\n<p>Which of the following options should be combined to form the MOST cost-efficient solution? (Select three)</p>\n",
			"answers": [
				"<p>Create an auto-scaling group that spans across 2 AZ, which min=1, max=1, desired=1</p>",
				"<p>Assign an EC2 Instance Role to perform the necessary API calls</p>",
				"<p>Create an Elastic IP and use the EC2 user-data script to attach it</p>",
				"<p>Create a Spot Fleet request</p>",
				"<p>Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group</p>",
				"<p>Create an auto-scaling group that spans across 2 AZ, which min=1, max=2, desired=2</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Create an auto-scaling group that spans across 2 AZ, which min=1, max=1, desired=1</strong></p>\n\n<p>Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.</p>\n\n<p>So we have an ASG with desired=1, across two AZ, so that if an instance goes down, it is automatically recreated in another AZ. So this option is correct.</p>\n\n<p><strong>Create an Elastic IP and use the EC2 user-data script to attach it</strong></p>\n\n<p>Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses, and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>An Elastic IP address is a static IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.</p>\n\n<p>Now, between the ALB and the Elastic IP. If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one EC2 instance. Instead, to minimize costs, we must use an Elastic IP.</p>\n\n<p><strong>Assign an EC2 Instance Role to perform the necessary API calls</strong></p>\n\n<p>For that Elastic IP to be attached to our EC2 instance, we must use an EC2 user data script, and our EC2 instance must have the correct IAM permissions to perform the API call, so we need an EC2 instance role.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a Spot Fleet request</strong> - A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price.</p>\n\n<p>The Spot Fleet selects the Spot Instance pools that meet your needs and launches Spot Instances to meet the target capacity for the fleet. By default, Spot Fleets are set to maintain target capacity by launching replacement instances after Spot Instances in the fleet are terminated.</p>\n\n<p>Spot Fleets requests would not fit our purpose as we are looking at a critical application. Spot instances can be terminated. So this option is incorrect.</p>\n\n<p><strong>Create an auto-scaling group that spans across 2 AZ, which min=1, max=2, desired=2</strong> - An ASG with desired=2 would create two instances, and this won't work for us as our monolith application is not made to work with two instances as per the given use-case.</p>\n\n<p><strong>Create an Application Load Balancer and a target group with the instance(s) of the Auto Scaling Group</strong> - If we use an ALB, things will still work, but we will have to pay for the provisioned ALB which sends traffic to only one EC2 instance. So this option is not correct.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html</a></p>\n"
		},
		"correct_response": [
			"a",
			"b",
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "You are deploying a critical monolith application that must be deployed on a single web server, as it hasn't been created to work in distributed mode. Still, you want to make sure your setup can automatically recover from the failure of an AZ.\n\nWhich of the following options should be combined to form the MOST cost-efficient solution? (Select three)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102022,
		"assessment_type": "multiple-choice",
		"prompt": {
			"question": "<p>A photo-sharing company is storing user profile pictures in an S3 bucket and an image analysis application is deployed on four EC2 instances. A solutions architect would like to trigger an image analysis procedure only on one of the four EC2 instances for each photo uploaded.</p>\n\n<p>What do you recommend?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create an S3 Event Notification that sends a message to an SQS queue. Make the EC2 instances read from the SQS queue</strong></p>\n\n<p>The Amazon S3 event notification feature enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.</p>\n\n<p>Amazon S3 supports the following destinations where it can publish events:</p>\n\n<p>Amazon Simple Notification Service (Amazon SNS) topic</p>\n\n<p>Amazon Simple Queue Service (Amazon SQS) queue</p>\n\n<p>AWS Lambda</p>\n\n<p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.</p>\n\n<p>Here we have to use S3 Event Notifications (which can send a message to either Lambda, SNS, or SQS) to send a message to the SQS queue. By using SQS, we know only one EC2 instance among the four will pick up a message and process it.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Subscribe the EC2 instances to the S3 Inventory stream</strong> - S3 Inventory is a distractor. If you're curious - Amazon S3 inventory helps you manage your storage by creating lists of the objects in an S3 bucket on a defined schedule.</p>\n\n<p><strong>Create a CloudWatch Event that reacts to objects uploads in S3 and invokes one of the EC2 instances</strong> - CloudWatch Events cannot invoke applications on EC2 instances, so we have to rule out that answer.</p>\n\n<p><strong>Create an S3 Event Notification that sends a message to an SNS topic. Subscribe the EC2 instances to the SNS topic</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p>\n\n<p>Using SNS would send a message to each EC2 instance via the SNS topic, therefore making all of them work for each upload. This is not the intended behavior.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Create an S3 Event Notification that sends a message to an SQS queue. Make the EC2 instances read from the SQS queue</p>",
				"<p>Create an S3 Event Notification that sends a message to an SNS topic. Subscribe the EC2 instances to the SNS topic</p>",
				"<p>Create a CloudWatch Event that reacts to objects uploads in S3 and invokes one of the EC2 instances</p>",
				"<p>Subscribe the EC2 instances to the S3 Inventory stream</p>"
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A photo-sharing company is storing user profile pictures in an S3 bucket and an image analysis application is deployed on four EC2 instances. A solutions architect would like to trigger an image analysis procedure only on one of the four EC2 instances for each photo uploaded.\n\nWhat do you recommend?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102024,
		"assessment_type": "multiple-choice",
		"prompt": {
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use the EC2 instances private IP for the replication</strong></p>\n\n<p>The source of the cost is that traffic between two EC2 instances is going over the public internet, thus incurring high costs. Here, the correct answer is to use Private IP, so that the network remains private, for a minimal cost.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Assign Elastic IPs to the EC2 instances and use them for the replication</strong> - Using Elastic IPs will not solve the problem as the traffic will still be going over the public internet.</p>\n\n<p><strong>Create a Private Link between the two EC2 instances</strong> - AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of data to the public Internet. AWS PrivateLink provides private connectivity between VPCs, AWS services, and on-premises applications, securely on the Amazon network.</p>\n\n<p>Private Link is a distractor in this question. Private Link is leveraged to create a private connection between an application that is fronted by an NLB in an account, and an Elastic Network Interface (ENI) in another account, without the need of VPC peering and allowing the connections between the two to remain within the AWS network.</p>\n\n<p><strong>Use an Elastic Fabric Adapter</strong> - The Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, and reservoir simulation, at scale on AWS. This option is not relevant to the given use-case.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/privatelink/\">https://aws.amazon.com/privatelink/</a></p>\n\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n",
			"question": "<p>You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two EC2 instances in two Availability Zones. The database must be publicly available so you have deployed the EC2 instances in public subnets. The replication protocol currently uses the EC2 public IP addresses.</p>\n\n<p>What can you do to decrease the replication cost?</p>\n",
			"answers": [
				"<p>Use the EC2 instances private IP for the replication</p>",
				"<p>Assign Elastic IPs to the EC2 instances and use them for the replication</p>",
				"<p>Create a Private Link between the two EC2 instances</p>",
				"<p>Use an Elastic Fabric Adapter</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": ""
		},
		"correct_response": [
			"a"
		],
		"section": "Design Cost-Optimized Architectures",
		"question_plain": "You have deployed a database technology that has a synchronous replication mode to survive disasters in data centers. The database is therefore deployed on two EC2 instances in two Availability Zones. The database must be publicly available so you have deployed the EC2 instances in public subnets. The replication protocol currently uses the EC2 public IP addresses.\n\nWhat can you do to decrease the replication cost?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102026,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"answers": [
				"<p>Use a Web Application Firewall and setup a rate-based rule</p>",
				"<p>Use AWS Shield Advanced and setup a rate-based rule</p>",
				"<p>Define a Network ACL (NACL) on your Application Load Balancer</p>",
				"<p>Configure Sticky Sessions on the Application Load Balancer</p>"
			],
			"question": "<p>Your application is deployed on EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.</p>\n\n<p>How can you efficiently prevent attackers from overwhelming your application?</p>\n",
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Use a Web Application Firewall and setup a rate-based rule</strong></p>\n\n<p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p>\n\n<p>The correct answer is to use WAF (which has integration on top of your ALB) and define a rate-based rule.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Configure Sticky Sessions on the Application Load Balancer</strong> - Application Load Balancer (ALB) operates at the request level (layer 7), routing traffic to targets – EC2 instances, containers, IP addresses and Lambda functions based on the content of the request. Ideal for advanced load balancing of HTTP and HTTPS traffic, Application Load Balancer provides advanced request routing targeted at delivery of modern application architectures, including microservices and container-based applications.</p>\n\n<p>Sticky Sessions on your ALB is a distractor here. Sticky sessions are a mechanism to route requests from the same client to the same target. Application Load Balancer supports sticky sessions using load balancer generated cookies. If you enable sticky sessions, the same target receives the request and can use the cookie to recover the session context.</p>\n\n<p><strong>Define a Network ACL (NACL) on your Application Load Balancer</strong> - An NACL does not work, as this only helps to block specific IPs. On top of things, NACLs are defined at the subnet level, not Application Load Balancers.</p>\n\n<p><strong>Use AWS Shield Advanced and setup a rate-based rule</strong> - AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.</p>\n\n<p>AWS Shield Advanced provides enhanced resource-specific detection and employs advanced mitigation and routing techniques for sophisticated or larger attacks.</p>\n\n<p>AWS Shield Advanced will give you DDoS protection overall, and you cannot set up rate-based rules in Shield.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html\">https://docs.aws.amazon.com/waf/latest/developerguide/waf-rule-statement-type-rate-based.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#sticky-sessions</a></p>\n"
		},
		"correct_response": [
			"a"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "Your application is deployed on EC2 instances fronted by an Application Load Balancer. Recently, your infrastructure has come under attack. Attackers perform over 100 requests per second, while your normal users only make about 5 requests per second.\n\nHow can you efficiently prevent attackers from overwhelming your application?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102028,
		"assessment_type": "multiple-choice",
		"prompt": {
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"relatedLectureIds": "",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Amazon MQ</strong></p>\n\n<p>Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Message brokers allow different software systems–often using different programming languages, and on different platforms–to communicate and exchange information. If an organization is using messaging with existing applications and wants to move the messaging service to the cloud quickly and easily, AWS recommends Amazon MQ for such a use case. So this is the correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Amazon Simple Notification Service (Amazon SNS)</strong> - Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. SNS does not provide support for migration from RabbitMQ as its a fully managed pub/sub messaging service. Hence this option is incorrect.</p>\n\n<p><strong>Amazon SQS Standard</strong> - Amazon SQS Standard offers a reliable, highly scalable hosted queue for storing messages as they travel between computers. Amazon SQS lets you easily move data between distributed application components and helps you build applications in which messages are processed independently (with message-level ack/fail semantics), such as automated workflows. SQS Standard does not provide support for migration from RabbitMQ. Hence this option is incorrect.</p>\n\n<p><strong>Amazon SQS FIFO (First-In-First-Out)</strong> - Amazon SQS FIFO (First-In-First-Out) has all the capabilities of the standard queue. They are used when the order of operations and events is critical, or where duplicates can't be tolerated. SQS FIFO does not provide support for migration from RabbitMQ. Hence this option is incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://aws.amazon.com/amazon-mq/\">https://aws.amazon.com/amazon-mq/</a></p>\n\n<p><a href=\"https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/\">https://aws.amazon.com/blogs/compute/migrating-from-rabbitmq-to-amazon-mq/</a></p>\n",
			"answers": [
				"<p>Amazon Simple Notification Service (Amazon SNS)</p>",
				"<p>Amazon SQS Standard</p>",
				"<p>Amazon MQ</p>",
				"<p>Amazon SQS FIFO (First-In-First-Out)</p>"
			],
			"question": "<p>A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS.</p>\n\n<p>As a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?</p>\n"
		},
		"correct_response": [
			"c"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A company is experiencing stability issues with their cluster of self-managed RabbitMQ message brokers and the company now wants to explore an alternate solution on AWS.\n\nAs a solutions architect, which of the following AWS services would you recommend that can provide support for quick and easy migration from RabbitMQ?",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102030,
		"assessment_type": "multi-select",
		"prompt": {
			"question": "<p>A digital media streaming company wants to use AWS Cloudfront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)</p>\n",
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Use CloudFront signed URLs</strong></p>\n\n<p>Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee.</p>\n\n<p>To securely serve this private content by using CloudFront, you can do the following:</p>\n\n<p>Require that your users access your private content by using special CloudFront signed URLs or signed cookies.</p>\n\n<p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. So this is a correct option.</p>\n\n<p><strong>Use CloudFront signed cookies</strong></p>\n\n<p>CloudFront signed cookies allow you to control who can access your content when you don't want to change your current URLs or when you want to provide access to multiple restricted files, for example, all of the files in the subscribers' area of a website. So this is also a correct option.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Require HTTPS for communication between CloudFront and your custom origin</strong></p>\n\n<p><strong>Require HTTPS for communication between CloudFront and your S3 origin</strong></p>\n\n<p>Requiring HTTPS for communication between CloudFront and your custom origin (or S3 origin) only enables secure access to the underlying content. You cannot use HTTPS to restrict access to your private content. So both these options are incorrect.</p>\n\n<p><strong>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</strong> - This option is just added as a distractor. You cannot use HTTPS to restrict access to your private content.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html</a></p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Use CloudFront signed URLs</p>",
				"<p>Require HTTPS for communication between CloudFront and your custom origin</p>",
				"<p>Require HTTPS for communication between CloudFront and your S3 origin</p>",
				"<p>Forward HTTPS requests to the origin server by using the ECDSA or RSA ciphers</p>",
				"<p>Use CloudFront signed cookies</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"a",
			"e"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "A digital media streaming company wants to use AWS Cloudfront to distribute its content only to its service subscribers. As a solutions architect, which of the following solutions would you suggest to deliver restricted content to the bona fide end users? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102032,
		"assessment_type": "multi-select",
		"prompt": {
			"relatedLectureIds": "",
			"answers": [
				"<p>enableVpcSupport</p>",
				"<p>enableVpcHostnames</p>",
				"<p>enableDnsHostnames</p>",
				"<p>enableDnsDomain</p>",
				"<p>enableDnsSupport</p>"
			],
			"question": "<p>The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Route 53 to accomplish this.</p>\n\n<p>Which of the following settings of the VPC need to be enabled? (Select two)</p>\n",
			"explanation": "<p>Correct options:</p>\n\n<p><strong>enableDnsHostnames</strong></p>\n\n<p><strong>enableDnsSupport</strong></p>\n\n<p>A private hosted zone is a container for records for a domain that you host in one or more Amazon virtual private clouds (VPCs). You create a hosted zone for a domain (such as example.com), and then you create records to tell Amazon Route 53 how you want traffic to be routed for that domain within and among your VPCs.</p>\n\n<p>For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true:</p>\n\n<p>enableDnsHostnames</p>\n\n<p>enableDnsSupport</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>enableVpcSupport</strong></p>\n\n<p><strong>enableVpcHostnames</strong></p>\n\n<p><strong>enableDnsDomain</strong></p>\n\n<p>The options enableVpcSupport, enableVpcHostnames and enableDnsDomain have been added as distractors.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-creating.html</a></p>\n",
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			]
		},
		"correct_response": [
			"c",
			"e"
		],
		"section": "Design Resilient Architectures",
		"question_plain": "The engineering team at an e-commerce company wants to set up a custom domain for internal usage such as internaldomainexample.com. The team wants to use the private hosted zones feature of Route 53 to accomplish this.\n\nWhich of the following settings of the VPC need to be enabled? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102034,
		"assessment_type": "multi-select",
		"prompt": {
			"question": "<p>A company manages a High Performance Computing (HPC) application that needs to be deployed on EC2 instances. The application requires high levels of inter-node communications and high network traffic between the instances.</p>\n\n<p>As a solutions architect, which of the following options would you recommend to the engineering team at the company? (Select two)</p>\n",
			"relatedLectureIds": "",
			"answers": [
				"<p>Deploy EC2 instances in a spread placement group</p>",
				"<p>Deploy EC2 instances in a partition placement group</p>",
				"<p>Deploy EC2 instances with Elastic Fabric Adapter</p>",
				"<p>Deploy EC2 instances in a cluster placement group</p>",
				"<p>Deploy EC2 instances behind a Network Load Balancer</p>"
			],
			"feedbacks": [
				"",
				"",
				"",
				"",
				""
			],
			"explanation": "<p>Correct options:</p>\n\n<p><strong>Deploy EC2 instances with Elastic Fabric Adapter</strong></p>\n\n<p>Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. Therefore this option is correct.</p>\n\n<p><strong>Deploy EC2 instances in a cluster placement group</strong></p>\n\n<p>Cluster placement groups pack instances close together inside an Availability Zone. They are recommended when the majority of the network traffic is between the instances in the group. These are also recommended for applications that benefit from low network latency, high network throughput, or both. Therefore this option is one of the correct answers.</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Deploy EC2 instances in a spread placement group</strong></p>\n\n<p>A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The instances are placed across distinct underlying hardware to reduce correlated failures. You can have a maximum of seven running instances per Availability Zone per group. Since the spread placement group can span across multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect.</p>\n\n<p><strong>Deploy EC2 instances in a partition placement group</strong></p>\n\n<p>A partition placement group spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. A partition placement group can have a maximum of seven partitions per Availability Zone. Since the partition placement group can have partitions in multiple Availability Zones in the same Region, it cannot support high levels of inter-node communications and high network traffic. So this option is incorrect.</p>\n\n<p><strong>Deploy EC2 instances behind a Network Load Balancer</strong></p>\n\n<p>A load balancer serves as the single point of contact for clients. The load balancer distributes incoming traffic across multiple targets, such as Amazon EC2 instances. A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. Network Load Balancer cannot facilitate high network traffic between instances. Network Load Balancer cannot support high levels of inter-node communication between EC2 instances. This option just serves as a distractor.</p>\n\n<p>References:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p>\n\n<p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p>\n\n<p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p>\n"
		},
		"correct_response": [
			"c",
			"d"
		],
		"section": "Design High-Performing Architectures",
		"question_plain": "A company manages a High Performance Computing (HPC) application that needs to be deployed on EC2 instances. The application requires high levels of inter-node communications and high network traffic between the instances.\n\nAs a solutions architect, which of the following options would you recommend to the engineering team at the company? (Select two)",
		"related_lectures": []
	},
	{
		"_class": "assessment",
		"id": 33102036,
		"assessment_type": "multiple-choice",
		"prompt": {
			"relatedLectureIds": "",
			"feedbacks": [
				"",
				"",
				"",
				""
			],
			"question": "<p>A company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.</p>\n\n<p>How would you go about providing private access to these AWS resources which are not part of this custom VPC?</p>\n",
			"explanation": "<p>Correct option:</p>\n\n<p><strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong></p>\n\n<p>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without imposing availability risks or bandwidth constraints on your network traffic.</p>\n\n<p>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. Traffic between your VPC and the other service does not leave the Amazon network.</p>\n\n<p>There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.</p>\n\n<p>A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:</p>\n\n<p>Amazon S3</p>\n\n<p>DynamoDB</p>\n\n<p>Incorrect options:</p>\n\n<p><strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</strong></p>\n\n<p><strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an interface endpoint for S3 and then connect to the S3 service using the private IP address</strong></p>\n\n<p><strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong></p>\n\n<p>Neither S3 nor DynamoDB supports interface endpoints, so these three options are incorrect.</p>\n\n<p>Reference:</p>\n\n<p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html</a></p>\n",
			"answers": [
				"<p>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service using the private IP address</p>",
				"<p>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</p>",
				"<p>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an interface endpoint for S3 and then connect to the S3 service using the private IP address</p>",
				"<p>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</p>"
			]
		},
		"correct_response": [
			"b"
		],
		"section": "Design Secure Applications and Architectures",
		"question_plain": "A company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.\n\nHow would you go about providing private access to these AWS resources which are not part of this custom VPC?",
		"related_lectures": []
	}
]